{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 無限ホライゾンの目的関数について\n",
                "\n",
                "* [Markov Decision Processes: Discrete Stochastic Dynamic Programming](https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316887)の５章と８章\n",
                "\n",
                "無限ホライゾンでは，大きく３種類の目的関数を考えることができます．\n",
                "\n",
                "1. 期待総報酬：$v^\\pi(s) \\equiv \\lim _{N \\rightarrow \\infty} E_s^\\pi\\left\\{\\sum_{t=1}^N r\\left(X_t, Y_t\\right)\\right\\}=\\lim _{N \\rightarrow \\infty} v_{N+1}^\\pi(s)$\n",
                "    * これは$+\\infty$もしくは$-\\infty$になることがあります．\n",
                "2. 期待割引総報酬：$v_\\lambda^\\pi(s) \\equiv \\lim _{N \\rightarrow \\infty} E_s^\\pi\\left\\{\\sum_{t=1}^N \\lambda^{t-1} r\\left(X_t, Y_t\\right)\\right\\}$\n",
                "    * これは$\\sup \\sup |r(s, a)|=M<\\infty$ $s \\in S \\quad a \\in A_s$である限り，極限が存在します．\n",
                "3. 期待平均報酬（ゲインとも呼ばれます）：$g^\\pi(s) \\equiv \\lim _{N \\rightarrow \\infty} \\frac{1}{N} E_s^\\pi\\left\\{\\sum_{t=1}^N r\\left(X_t, Y_t\\right)\\right\\}=\\lim _{N \\rightarrow \\infty} \\frac{1}{N} v_{N+1}^\\pi(s) = \\lim_{N \\to \\infty} \\frac{1}{N} \\sum^N_{n=1}P^{n-1}_{\\pi} r_{d_n}$\n",
                "\n",
                "これは極限が存在しない場合があります．しかし，次のlimsupとliminfは必ず存在します．\n",
                "* $g^\\pi(s) \\equiv \\liminf _{N \\rightarrow \\infty} \\frac{1}{N} v_{N+1}^\\pi(s), \\quad g_{+}^\\pi(s) \\equiv \\limsup _{N \\rightarrow \\infty} \\frac{1}{N} v_{N+1}^\\pi(s)$\n",
                "\n",
                "また，次の補題が成立します：\n",
                "* $S$が可算とする．$d^\\infty \\in \\Pi^{\\mathrm{SR}}$とし，$P_d$の極限行列$P_d^*$が確率的とする．このとき，ゲインの極限が存在し，\n",
                "$g^{d^\\infty}(s) = \\lim_{N \\to \\infty} \\frac{1}{N}v^{d^\\infty}_{N+1}(s)=P^*_d r_d(s)$\n",
                "が成立する．\n",
                "* また，$S$が有限ならば，極限が存在する（これは有限状態のマルコフ連鎖ならば$P_d^*$が確率的になることを使います．[RL_AverageReward_Basic_Puterman_memo.ipynb](RL_AverageReward_Basic_Puterman_memo.ipynb)にありますが，Cesaro-limitが有限行列ならば必ず存在します．）\n",
                "\n",
                "---\n",
                "\n",
                "**補足**\n",
                "* UnichainなMDPでは，任意の方策について，ゲインが状態に非依存であることに注意しましょう．つまり，$\\rho^\\pi(x)=\\rho^\\pi(y)=\\rho^\\pi$です．\n",
                "    * recurrentな状態は有限回しか訪問されないので，平均を取ると影響が消えます\n",
                "    * recurrentな状態は永遠に訪問されるので，状態間で平均報酬が変わりません．\n",
                "\n",
                "---\n",
                "\n",
                "期待平均報酬が存在しない例を見てみましょう．\n",
                "$S=\\{1, 2, \\dots\\}$とし，各$s \\in \\mathcal{S}$に対して，$p\\left(s+1 \\mid s, a_s\\right)=1, p\\left(j \\mid s, a_s\\right)=0$ for $j \\neq s+1$および$r\\left(s, a_s\\right)=(-1)^{s+1} s$になるような$a_s$が一つだけ存在するとします．\n",
                "\n",
                "![no-limit](figs/no-limit-MDP.png)\n",
                "\n",
                "このとき，方策は一つしか存在しません．それを$\\pi$としましょう．また，$r_N(s)=0$としましょう．また，$v_1^\\pi(1)=0, v_2^\\pi(1)=1$とします．このとき，任意の$N > 1$で\n",
                "$$\n",
                "v_N^\\pi(1)=\\left\\{\\begin{array}{ll}\n",
                "k & N=2 k \\\\\n",
                "-k & N=2 k+1,\n",
                "\\end{array} k=1,2, \\ldots\\right.\n",
                "$$\n",
                "なので，\n",
                "$$\n",
                "\\lim \\sup _{N \\rightarrow \\infty} v_N^\\pi(1)=+\\infty \\quad \\text { and } \\quad \\liminf _{N \\rightarrow \\infty} v_N^\\pi(1)=-\\infty\n",
                "$$\n",
                "であるため，$s=1$における期待総報酬は存在しません．また，\n",
                "$$\n",
                "\\begin{aligned}\n",
                "&g_{+}^\\pi(1)=\\limsup _{N \\rightarrow \\infty} N^{-1} v_{N+1}^\\pi(1)=\\frac{1}{2}\\\\\n",
                "&g_{-}^\\pi(1)=\\liminf _{N \\rightarrow \\infty} N^{-1} v_{N+1}^\\pi(1)=-\\frac{1}{2}\n",
                "\\end{aligned}\n",
                "$$\n",
                "なので，$s=1$におけるゲインも存在しません．\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 1. 期待総報酬問題について\n",
                "\n",
                "この問題では$v^\\pi(s)=\\lim _{N \\rightarrow \\infty} v_N^\\pi(s)$を達成する$\\pi$について扱います．\n",
                "しかし，多くの場合，この$v^\\pi(s)$は発散することがあります．\n",
                "そこで，次の問題だけ考えることでこれを回避します：\n",
                "1. 任意の方策で$\\lim _{N \\rightarrow \\infty} v_N^\\pi(s)$が存在する問題\n",
                "2. 極限が存在し，かつそれが有限であり，また，任意の方策に対して$v^{\\pi^*}(s) \\geq \\limsup _{N \\rightarrow \\infty} v_N^\\pi(s)$を満たすような方策$\\pi^\\star$が最低でも一つ存在する問題\n",
                "3. TODO: Use optimality criteria which are sensitive to the rate at which $v^\\pi_N(s)$ diverges.\n",
                "\n",
                "**1が満足される条件**\n",
                "\n",
                "$$v_{+}^\\pi(s) \\equiv E_s^*\\left\\{\\sum_{t=1}^{\\infty} \\max\\{r\\left(X_t, Y_t\\right), 0\\}\\right\\}$$\n",
                "$$v_{-}^\\pi(s) \\equiv E_s^*\\left\\{\\sum_{t=1}^{\\infty} \\max\\{-r\\left(X_t, Y_t\\right), 0\\}\\right\\}$$\n",
                "\n",
                "とします．そして，\n",
                "* 任意の$\\pi \\in \\Pi^{\\mathrm{HR}}$について，$v_{+}^\\pi(s)$ か $v_{-}^\\pi(s)$が有限\n",
                "\n",
                "であれば，\n",
                "\n",
                "$$\n",
                "v^\\pi(s)=v_{+}^\\pi(s)-v_{-}^\\pi(s)\n",
                "$$\n",
                "\n",
                "となり，極限が存在します．\n",
                "また，特に次のケースを区別して考えます\n",
                "\n",
                "1. (Positive bounded model) 任意の$s \\in S$について，$r(s, a) \\geq 0$な$a \\in A_s$が存在かつ$v^\\pi_+(s)$が任意の$\\pi \\in \\Pi^{HR}$で有限\n",
                "    * この仮定のもとでは，任意の方策は非正の報酬の状態に吸収されます（そうでないと価値が有限になりません）\n",
                "    * 問題の例：\n",
                "        * [最適停止問題](https://ja.wikipedia.org/wiki/%E6%9C%80%E9%81%A9%E5%81%9C%E6%AD%A2%E5%95%8F%E9%A1%8C)\n",
                "        * 何らかの状態に到達する確率を最大化したい問題（ギャンブルなど）\n",
                "        * 何らかの状態に到達するまでの時間を可能な限り伸ばしたい問題（コンピュータゲームなど）\n",
                "2. (Negative model) 任意の$s \\in S$と$a \\in A_s$について，$r(s, a) \\leq 0$かつ，何らかの$\\pi \\in \\Pi^{HR}$について，$v^\\pi(s) > -\\infty$\n",
                "    * 仮定を満たす方策は報酬が０の状態に吸収されます（そうでないと価値が有限になりません）\n",
                "    * 問題の例：\n",
                "        * 何らかの状態に到達しない確率を最大化したい問題\n",
                "        * 何らかの状態に到達するまでの時間を最小化する問題\n",
                "        * 最適停止問題\n",
                "3. (Convergent model) 任意の$s \\in S$と任意の$\\pi \\in \\Pi^{HR}$について，$v_{+}^\\pi(s)$ and $v_{-}^\\pi(s)$が有限\n",
                "\n",
                "ちなみに，Positive bounded modelとNegative modelは符号を**逆転しただけではありません**．\n",
                "実際，Positive bounded modelでは報酬を最大化する一方で，Negative modelでは総報酬をゼロに近づけることが目標です．"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 2. 期待割引総報酬問題について\n",
                "\n",
                "割引問題は経済のモデルなどで便利です．また，かなり充実した理論が存在し，様々な解法が存在します．\n",
                "よく知られた話ですが，割引率はランダムなホライゾン$\\nu$とみなすこともできます．\n",
                "これは例えば経済モデルにおける工場の破産や生産設備の故障，生物系のモデルにおける生き物の死亡などをモデル化するときに便利です．\n",
                "\n",
                "$$\n",
                "v_\\nu^\\pi(s) \\equiv E_s^\\pi\\left[E_\\nu\\left\\{\\sum_{t=1}^\\nu r\\left(X_t, Y_t\\right)\\right\\}\\right]\n",
                "$$\n",
                "\n",
                "を，ランダムなホライゾン$\\nu$に対しての期待価値とします．ここで，$\\nu$が帰化分布に従う場合を考えてみましょう．つまり，\n",
                "\n",
                "$$\n",
                "P\\{\\nu=n\\}=(1-\\lambda) \\lambda^{n-1}, \\quad n=1,2, \\ldots\n",
                "$$\n",
                "\n",
                "です．幾何分布は上で紹介したような「故障のモデル」などを表すときに便利です．\n",
                "証明はMarkov decision processesのProposition 5.3.1に任せますが，$v_\\nu^\\pi(s)=v_\\lambda^\\pi(s)$ for all $s \\in S$が成立します．"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 最適性の評価指標\n",
                "\n",
                "* [Markov Decision Processes: Discrete Stochastic Dynamic Programming](https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316887)の５章\n",
                "* [Average reward reinforcement learning: Foundations, Algorithms, and Empirical Results](https://link.springer.com/content/pdf/10.1007/BF00114727.pdf)\n",
                "\n",
                "上で見た目的関数に対応する最適性の指標について見てみましょう．\n",
                "\n",
                "---\n",
                "\n",
                "#### 総報酬\n",
                "\n",
                "次を満たす$\\pi^\\star$は**総報酬最適**である，といいます\n",
                "\n",
                "$$\n",
                "v^{\\pi^*}(s) \\geq v^\\pi(s) \\quad \\text { for each } s \\in S \\text { and all } \\pi \\in \\Pi^{\\mathrm{HR}}\n",
                "$$\n",
                "\n",
                "この指標は\"positive bounded\", \"negative\"，そして\"convergent model\"に対して適用できます．\n",
                "\n",
                "また，次を**MDPの価値**，と呼びます\n",
                "\n",
                "$$\n",
                "v^*(s) \\equiv \\sup _{\\pi \\in I 1^{\\mathrm{HR}}} v^\\pi(s)\n",
                "$$\n",
                "\n",
                "$v^{\\pi^*}(s)=v^*(s) \\quad$ for all $s \\in S$であるならば，最適方策$\\pi^* \\in \\Pi^K(K=$ HR, HD, MR, or MD $)$が存在します．\n",
                "\n",
                "---\n",
                "\n",
                "#### 割引\n",
                "\n",
                "次を満たす方策$\\pi^\\star$は**割引最適**である，といいます．\n",
                "割引率$0 \\leq \\lambda < 1$について，\n",
                "\n",
                "$$\n",
                "v_\\lambda^{\\pi^*}(s) \\geq v_\\lambda^\\pi(s) \\quad \\text { for each } s \\in S \\text { and all } \\pi \\in \\Pi^{\\mathrm{HR}}\n",
                "$$\n",
                "\n",
                "また，割引モデルに置いて，MDPの価値は\n",
                "\n",
                "$$\n",
                "v_\\lambda^*(s) \\equiv \\sup _{\\pi \\in \\Pi^{\\mathrm{HR}}} v_\\lambda^\\pi(s)\n",
                "$$\n",
                "\n",
                "によって定義され，上と同様に，次を満たすときに割引最適な方策が存在します．\n",
                "\n",
                "$$\n",
                "v_\\lambda^{\\pi *}(s)=v_\\lambda^*(s) \\quad \\text { for all } s \\in S\n",
                "$$\n",
                "\n",
                "---\n",
                "\n",
                "#### 平均\n",
                "\n",
                "次を満たす方策$\\pi^\\star$は**平均(ゲイン)最適**である，といいます．\n",
                "\n",
                "$$\n",
                "g^{\\pi^*}(s) \\geq g^\\pi(s) \\quad \\text { for each } s \\in S \\text { and all } \\pi \\in \\Pi^{\\mathrm{HR}}\n",
                "$$\n",
                "\n",
                "が，この定義は方策に対して平均報酬が存在しない場合に困ります（上で見た可算無限状態集合の例など）．\n",
                "そのような場合は，次を満たす方策のことを最適方策とします（この定義を平均最適としたほうが便利です）．\n",
                "\n",
                "$$\n",
                "g_{-}^{\\pi^*}(s)=\\liminf _{N \\rightarrow \\infty} \\frac{1}{N} v_{N+1}^{\\pi^*}(s) \\geq \\limsup _{N \\rightarrow \\infty} \\frac{1}{N} v_{N+1}^\\pi(s)=g_{+}^\\pi(s)\n",
                "$$\n",
                "\n",
                "また，次を満たす方策をそれぞれ上極限，下極限最適方策，と呼びます．\n",
                "\n",
                "$$\n",
                "g_{+}^{\\pi^*}(s) \\geq g_{+}^\\pi(s)\n",
                "\\quad\n",
                "g_{-}^{\\pi^*}(s) \\geq g_{-}^\\pi(s)\n",
                "$$\n",
                "\n",
                "定義より，平均最適は上，下極限最適よりも強い最適性です（平均最適ならば，それは上，下極限最適でもあります）．\n",
                "\n",
                "ここで，ゲインに対して，次の補題が成り立ちます．つまり，方策の集合をマルコフ方策に限定しても問題ありません：\n",
                "\n",
                "各$\\pi \\in \\Pi^{HR}$と$s \\in S$にたいして，次を満たす$\\pi' \\in \\Pi^{MR}$が存在する： \n",
                "1. $g^{\\pi'}_+(s) = g^{\\pi}_+(s)$\n",
                "2. $g^{\\pi'}_-(s) = g^{\\pi}_-(s)$\n",
                "3. $g^{\\pi}_-(s) = g^{\\pi}_+(s)$ならば常に$g^{\\pi'}(s) = g^{\\pi}(s)$\n",
                "\n",
                "\n",
                "---\n",
                "\n",
                "TODO: Overtaking optimal方策について言及しよう．\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 定常分布について\n",
                "\n",
                "* [Average-reward model-free reinforcement learning: a systematic review and literature mapping](https://arxiv.org/abs/2010.08920)も参考\n",
                "\n",
                "**マルコフ連鎖についての定常分布**\n",
                "\n",
                "ここでの結果は有限と可算集合のどちらのマルコフ連鎖についても成立します．\n",
                "\n",
                "$\\left\\{A_n: n \\geq 0\\right\\}$を行列の系列とします．\n",
                "そして，$\\lim _{n \\rightarrow \\infty} A_n(j \\mid s)=A(j\\mid s)$ならば，$\\lim _{n \\rightarrow \\infty} A_n=A$であると表記します．\n",
                "periodicなマルコフ連鎖については，このような極限が存在しない場合があります．\n",
                "例えば\n",
                "$$\n",
                "P=\\left[\\begin{array}{ll}\n",
                "0 & 1 \\\\\n",
                "1 & 0\n",
                "\\end{array}\\right]\n",
                "$$\n",
                "\n",
                "を考えると，$2n$のときは$I$であり，$2n+1$は$P$になるので，極限が存在しません．変わりに，次のCesaro-limitを考えることができます．\n",
                "\n",
                "$$\n",
                "\\lim _{N \\rightarrow \\infty} \\frac{1}{N} \\sum_{n=0}^{N-1} A_n=A,\n",
                "$$\n",
                "\n",
                "これは省略して\n",
                "$$\n",
                "C-\\lim _{N \\rightarrow \\infty} A_N=A\n",
                "$$\n",
                "と書くこともあります．\n",
                "極限行列を次のように定義しましょう：\n",
                "\n",
                "$$\n",
                "P^*=C \\underset{N \\rightarrow \\infty}{-\\lim P^N}\n",
                "$$\n",
                "\n",
                "このCesaro-limitは$S$が有限であれば必ず存在します（また，それは確率行列になります）．\n",
                "可算状態の連鎖の場合は確率行列にならないこともあります．\n",
                "例えば$p(s+1 \\mid s)=1$ for $s=0,1, \\ldots$の場合は，$P^*$の要素はゼロになります．\n",
                "\n",
                "ちなみにCesaro-limitではなく，普通の極限$\\lim _{N \\rightarrow \\infty} p^N(j \\mid s)$は，irreducibleかつaperiodicなマルコフ連鎖では存在します．\n",
                "\n",
                "$P$をpositive recurrent irreducibleな連鎖のとき，\n",
                "$$\n",
                "q^T = q^T P\n",
                "$$\n",
                "はユニークな解を持ちます（ここで$q$は和が１のベクトルです）．\n",
                "この解のことを$P$についての**定常分布**と呼びます．\n",
                "\n",
                "\n",
                "**MDPついての定常分布**\n",
                "\n",
                "次を定義します：\n",
                "* 方策から誘導されるマルコフ連鎖：$\\boldsymbol{P}_\\pi \\in[0,1]^{|\\mathcal{S}| \\times|\\mathcal{S}|}$\n",
                "    * その$t$回繰り返したもの：$\\boldsymbol{P}_\\pi^t\\left[s_0, s\\right]=\\operatorname{Pr}\\left\\{S_t=s \\mid s_0, \\pi\\right\\}=: p_\\pi^t\\left(s \\mid s_0\\right)$\n",
                "\n",
                "このとき，このマルコフ連鎖の収束先は\n",
                "$$p_\\pi^{\\star}\\left(s \\mid s_0\\right)=\\lim _{t_{\\max } \\rightarrow \\infty} \\frac{1}{t_{\\max }} \\sum_{t=0}^{t_{\\max }-1} p_\\pi^t\\left(s \\mid s_0\\right)=\\lim _{t_{\\max } \\rightarrow \\infty} p_\\pi^{t_{\\max }}\\left(s \\mid s_0\\right), \\quad \\forall s \\in \\mathcal{S}$$\n",
                "\n",
                "を満たします．この最初のlimitは有限なMDPならば必ず存在し，２つ目のLimitはMDPがaperiodicであれば存在します（TODO: 証明）\n",
                "\n",
                "また，$p_\\pi^\\star(s|s_0)$をベクトル化したものを$\\boldsymbol{p}_\\pi^{\\star} \\in[0,1]^{|\\mathcal{S}|}$とします．\n",
                "これは\n",
                "$$\\left(\\boldsymbol{p}_\\pi^{\\star}\\right)^{\\top} \\boldsymbol{P}_\\pi=\\left(\\boldsymbol{p}_\\pi^{\\star}\\right)^{\\top}$$\n",
                "を満たします"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ゲインについて\n",
                "\n",
                "平均報酬はゲインとも呼ばれ，次で定義され，以下の変形が成立します：\n",
                "\n",
                "$$\n",
                "\\begin{aligned}\n",
                "v_g\\left(\\pi, s_0\\right) & :=\\lim _{t_{\\max } \\rightarrow \\infty} \\frac{1}{t_{\\max }} \\mathbb{E}_{S_t, A_t}\\left[\\sum_{t=0}^{t_{\\max }-1} r\\left(S_t, A_t\\right) \\mid S_0=s_0, \\pi\\right] \\\\\n",
                "& =\\sum_{s \\in \\mathcal{S}}\\left\\{\\lim _{t_{\\max } \\rightarrow \\infty} \\frac{1}{t_{\\max }} \\sum_{t=0}^{t_{\\max }-1} p_\\pi^t\\left(s \\mid s_0\\right)\\right\\} r_\\pi(s)=\\sum_{s \\in \\mathcal{S}} p_\\pi^{\\star}\\left(s \\mid s_0\\right) r_\\pi(s) \\\\\n",
                "& =\\lim _{t_{\\max } \\rightarrow \\infty} \\mathbb{E}_{S_{t_{\\max }}, A_{t_{\\max }}}\\left[r\\left(S_{t_{\\max }}, A_{t_{\\max }}\\right) \\mid S_0=s_0, \\pi\\right]\n",
                "\\end{aligned}\n",
                "$$\n",
                "\n",
                "ここで，\n",
                "* $r_\\pi(s)=\\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) r(s, a)$\n",
                "* 最初のlimitは方策が定常かつMDPが有限であれば存在します（TODO: 証明）\n",
                "* ２つ目のLimitは簡単です\n",
                "* ３つ目は方策が定常かつMDPがaperiodic，もしくは報酬に何らかの構造が入っている場合に存在します（TODO: 証明）\n",
                "\n",
                "行列形式で書けば，\n",
                "\n",
                "$$\n",
                "\\boldsymbol{v}_g(\\pi)=\\lim _{t_{\\max } \\rightarrow \\infty} \\frac{1}{t_{\\max }} \\boldsymbol{v}_{t_{\\max }}(\\pi)=\\lim _{t_{\\max } \\rightarrow \\infty} \\frac{1}{t_{\\max }} \\sum_{t=0}^{t_{\\max }-1} \\boldsymbol{P}_\\pi^t \\boldsymbol{r}_\\pi=\\boldsymbol{P}_\\pi^{\\star} \\boldsymbol{r}_\\pi\n",
                "$$\n",
                "\n",
                "です．ここで，$\\boldsymbol{r}_\\pi \\in \\mathbb{R}^{|\\mathcal{S}|}$ は $r_\\pi(s)$をstackしたものです．\n",
                "\n",
                "定常分布は$s_0$に依存しないので，$\\boldsymbol{P}_\\pi^{\\star}$の行は全て同じであることに気をつけましょう．つまり，\n",
                "\n",
                "* $v_g(\\pi)=\\boldsymbol{p}_\\pi^{\\star} \\cdot \\boldsymbol{r}_\\pi=v_g\\left(\\pi, s_0\\right), \\forall s_0 \\in \\mathcal{S}$\n",
                "* $\\boldsymbol{v}_g(\\pi)=v_g(\\pi) \\cdot \\mathbf{1}$\n",
                "\n",
                "です．"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### n-割引最適\n",
                "\n",
                "上の変形の補題を使うと，ゲイン最適，バイアス最適，そして普通の割引最適が一般化できます．\n",
                "次のn-割引最適を定義しましょう．\n",
                "\n",
                "---\n",
                "\n",
                "$n=-1, 0, 1, \\dots$について，次を満たす方策$\\pi^\\star$を$n$-割引最適と呼びます．\n",
                "\n",
                "任意の$s$と$\\pi$について，\n",
                "\n",
                "$$\n",
                "\\lim _{\\lambda \\rightarrow 1}(1-\\lambda)^{-n}\\left(v_\\lambda^{\\pi^*}(s)-v_\\lambda^\\pi(s)\\right) \\geq 0\n",
                "$$\n",
                "\n",
                "この$n$-割引最適について，次が成立します．\n",
                "\n",
                "1. $n=-1$のとき，ゲイン最適と等価．\n",
                "2. $n=0$のとき，バイアス最適と等価\n",
                "3. $n=0$かつ$\\lambda$を固定すると，割引最適と等価\n",
                "\n",
                "**証明**\n",
                "\n",
                "**$n=1$のとき**\n",
                "\n",
                "$n=-1$のときを考えましょう．つまり，\n",
                "\n",
                "$$\\lim_{\\lambda \\rightarrow 1}(1-\\lambda)\\left(v_\\lambda^{\\pi^*}(s)-v_\\lambda^\\pi(s)\\right) \\geq 0$$\n",
                "です．\n",
                "\n",
                "このとき，上でやった割引価値とバイアス関数の変形を使うと，\n",
                "$$\n",
                "\\lim _{\\lambda \\rightarrow 1}(1-\\lambda)\\left(\\frac{v_g^{\\pi^*}(s)-v_g^\\pi(s)}{1-\\lambda}+v_b^{\\pi^*}(s)+f^{\\pi^*}(s, \\lambda)-v_b^\\pi(s)-f^\\pi(s, \\lambda)\\right) \\geq 0\n",
                "$$\n",
                "です．$f$の部分は$\\lambda \\to 1$で$0$に収束するので，\n",
                "$$\n",
                "\\left(v_g^{\\pi^*}(s)-v_g^\\pi(s)\\right) \\geq 0 .\n",
                "$$\n",
                "が成り立ち，これはゲイン最適と同じです（多分バイアス関数は上界がある？）．\n",
                "\n",
                "**$n=0$のとき**\n",
                "\n",
                "$n=0$のときを考えましょう．つまり，\n",
                "$$\\lim_{\\lambda \\rightarrow 1}\\left(v_\\lambda^{\\pi^*}(s)-v_\\lambda^\\pi(s)\\right) \\geq 0$$\n",
                "です．\n",
                "このとき，$\\lambda \\to 1$を取るので，ゲイン最適ではない$\\pi$は考える必要がありません（最適方策の割引価値が支配的な項になるので）．\n",
                "\n",
                "$\\pi$がゲイン最適な方策の状況を考えましょう．\n",
                "上でやった割引価値とゲイン関数の変形を使うと，\n",
                "$$\n",
                "\\lim _{\\lambda \\rightarrow 1}\\left(\\left(\\frac{v_g^{\\pi^*}(s)}{1-\\lambda}+v_b^{\\pi^*}(s)+f^{\\pi^*}(s, \\lambda)\\right)-\\left(\\frac{v_g^\\pi(s)}{1-\\lambda}+v_b^\\pi(s)+f^\\pi(s, \\lambda)\\right)\\right) \\geq 0\n",
                "$$\n",
                "であり，$\\pi$はゲイン最適なので，\n",
                "$$\n",
                "\\lim _{\\lambda \\rightarrow 1}\\left(\\left(v_b^{\\pi^*}(s)+f^{\\pi^*}(s, \\lambda)\\right)-\\left(v_b^\\pi(s)+f^\\pi(s, \\lambda)\\right)\\right) \\geq 0\n",
                "$$\n",
                "です．$f$は$0$に収束するので，\n",
                "$$\n",
                "\\lim _{\\lambda \\rightarrow 1}v_b^{\\pi^*}(s)-v_b^\\pi(s) \\geq 0\n",
                "$$\n",
                "が成立します．\n",
                "\n",
                "---\n",
                "\n",
                "実は，$n < m$であるとき，$m$-割引最適方策は$n$-割引最適方策でもあります．\n",
                "そのため，バイアス最適な方策はゲイン最適でもあります（下の図参照）．\n",
                "\n",
                "![n-discount-optimal](figs/n-discount-optimal.png)\n",
                "\n",
                "全ての$n \\geq -1$で最適な方策は**Blackwell最適**とも呼ばれます．\n",
                "Blackwell最適な方策は次の方策と等価です．\n",
                "任意の$s$と$\\pi$について，次を満たす$\\lambda^\\star(s)$が存在する．\n",
                "\n",
                "$$\n",
                "v_\\lambda^{\\pi^*}(s)-v_\\lambda^\\pi(s) \\geq 0 \\quad \\forall \\lambda^\\star(s) \\leq \\lambda < 1\n",
                "$$\n",
                "\n",
                "つまり，Blackwell最適は割引最適でもあります．また，特に$\\sup_{s \\in S}\\lambda^\\star <1$のとき，strongly Blackwell最適とも呼びます．\n",
                "\n",
                "\n",
                "今回は$n=0, -1$のときだけ考えましょう．"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ベルマン方程式\n",
                "\n",
                "平均報酬についても以下のベルマン方程式が成立します：\n",
                "\n",
                "---\n",
                "\n",
                "**Puterman本定理8.2.6**\n",
                "\n",
                "$S$が有限の時、\n",
                "1. $(I - P)v_g$=0\n",
                "2. 次のベルマンの方程式が成り立つ：\n",
                "$$\n",
                "v_b\\left(\\pi^*, s\\right)+v_g\\left(\\pi^*\\right)=\\max _{a \\in \\mathcal{A}}\\left\\{r(s, a)+\\sum_{c^{\\prime} \\in \\mathcal{S}} p\\left(s^{\\prime} \\mid s, a\\right) v_b\\left(\\pi^*, s^{\\prime}\\right)\\right\\}, \\quad \\forall s \\in \\mathcal{S},\n",
                "$$\n",
                "3. $v_g$と$v_b$が上をそれぞれ満たす時，\n",
                "    * $v_g=P^*r$かつ$v_b=H_p r +  u$であり，ここで$(I-P)u=0$\n",
                "    * さらに$P^*h = 0$ならば，$v_b=H_p r$ \n",
                "    * ここで，$H_P$はDrazin逆行列です。定義は[Puterman本参照](RL_AverageReward_Basic_Puterman_memo.ipynb)\n",
                "\n",
                "**証明**\n",
                "\n",
                "1個目は定常分布の性質から，$(I-P)P^*=0$なので，$(I-P)P^*r=0$であり，あとは$P^*r=g$を使えば終わり。\n",
                "\n",
                "2個目はDrazin逆行列を使って，$P^* + (I-P)H_P=I$なので，両辺に$r$をかけるとベルマン方程式が出てきます。\n",
                "\n",
                "3個目と4個目は教科書参照。\n",
                "\n",
                "---\n",
                "\n",
                "ここで，$v_b$はバイアスと呼ばれ，次で定義されます：\n",
                "\n",
                "$$\n",
                "\\begin{aligned}\n",
                "v_b\\left(\\pi, s_0\\right) & :=\\lim _{t_{\\max } \\rightarrow \\infty} \\mathbb{E}_{S_t, A_t}\\left[\\sum_{t=0}^{t_{\\max }-1}\\left(r\\left(S_t, A_t\\right)-v_g(\\pi)\\right) \\mid S_0=s_0, \\pi\\right] \\\\\n",
                "& =\\lim _{t_{\\max } \\rightarrow \\infty} \\sum_{t=0}^{t_{\\max }-1} \\sum_{s \\in \\mathcal{S}}\\left(p_\\pi^t\\left(s \\mid s_0\\right)-p_\\pi^{\\star}(s)\\right) r_\\pi(s) \\\\\n",
                "& =\\underbrace{\\sum_{t=0}^{\\tau-1} \\sum_{s \\in \\mathcal{S}} p_\\pi^t\\left(s \\mid s_0\\right) r_\\pi(s)}_{\\text {the expected total reward } v_\\tau}-\\tau v_g(\\pi)+\\underbrace{\\lim _{t_{\\max } \\rightarrow \\infty} \\sum_{t=\\tau}^{t_{\\max }-1} \\sum_{s \\in \\mathcal{S}}\\left(p_\\pi^t\\left(s \\mid s_0\\right)-p_\\pi^{\\star}(s)\\right) r_\\pi(s)}_{\\text {approaches } 0 \\text { as } \\tau \\rightarrow \\infty} \\\\\n",
                "& =\\sum_{s \\in \\mathcal{S}}\\left\\{\\lim _{t_{\\max } \\rightarrow \\infty} \\sum_{t=0}^{t_{\\max }-1}\\left(p_\\pi^t\\left(s \\mid s_0\\right)-p_\\pi^{\\star}(s)\\right)\\right\\} r_\\pi(s)=\\sum_{s \\in \\mathcal{S}} d_\\pi\\left(s \\mid s_0\\right) r_\\pi(s),\n",
                "\\end{aligned}\n",
                "$$\n",
                "\n",
                "ここで，$d_\\pi(s|s_0)$は行列$\\boldsymbol{D}_\\pi:=\\left(\\boldsymbol{I}-\\boldsymbol{P}_\\pi+\\boldsymbol{P}_\\pi^{\\star}\\right)^{-1}\\left(\\boldsymbol{I}-\\boldsymbol{P}_\\pi^{\\star}\\right)$と同値です（limitと期待値を入れ替えて良い場合）．\n",
                "\n",
                "バイアスは以下の解釈ができます：\n",
                "\n",
                "1. （１行目）$s_0$から始まって$\\pi$に従うときの，即時報酬$r(s_t, a_t)$と定常報酬$v_g(\\pi)$の差\n",
                "2. （２行目）$s_0$から始まった過程と，$p_\\pi^\\star$からサンプルされた初期状態から始まった過程の，報酬の差．よって，$\\pi$によって得られる総報酬と，ずっと$v_g(\\pi)$の報酬の場合の総報酬の差．\n",
                "3. （３行目）$v_\\tau\\left(\\pi, s_0\\right) \\approx v_g(\\pi) \\tau+v_b\\left(\\pi, s_0\\right)$とみなせるので，期待総報酬$v_\\tau$がどれくらい振動するかを表している．\n",
                "4. （４行目）$\\left(p_\\pi^t\\left(s \\mid s_0\\right)-p_\\pi^{\\star}(s)\\right)$は定常状態になるまで非ゼロなので，バイアスは一時的な性能とみなせる．\n",
                "\n",
                "何らかの参照状態$s_{\\mathrm{ref}} \\in \\mathcal{S}$について，相対価値$v_{\\text {brel }}$が定義されます：\n",
                "\n",
                "$$\n",
                "v_{b r e l}(\\pi, s):=v_b(\\pi, s)-v_b\\left(\\pi, s_{\\text {ref }}\\right)=\\lim _{t_{\\max } \\rightarrow \\infty}\\left\\{v_{t_{\\max }}(\\pi, s)-v_{t_{\\max }}\\left(\\pi, s_{\\text {ref }}\\right)\\right\\}, \\quad \\forall \\pi \\in \\Pi_{\\mathrm{S}}, \\forall s \\in \\mathcal{S} .\n",
                "$$\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "vscode": {
                    "languageId": "plaintext"
                }
            },
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import jax.numpy as jnp\n",
                "from jax.random import PRNGKey\n",
                "import jax\n",
                "from typing import NamedTuple, Optional\n",
                "\n",
                "key = PRNGKey(0)\n",
                "\n",
                "S = 10  # 状態集合のサイズ\n",
                "A = 3  # 行動集合のサイズ\n",
                "S_set = jnp.arange(S)  # 状態集合\n",
                "A_set = jnp.arange(A)  # 行動集合\n",
                "gamma = 0.8  # 割引率\n",
                "\n",
                "\n",
                "# 報酬行列を適当に作ります\n",
                "key, _ = jax.random.split(key)\n",
                "rew = jax.random.uniform(key=key, shape=(S, A))\n",
                "assert rew.shape == (S, A)\n",
                "\n",
                "\n",
                "# 遷移確率行列を適当に作ります\n",
                "key, _ = jax.random.split(key)\n",
                "P = jax.random.uniform(key=key, shape=(S*A, S))\n",
                "P = P / jnp.sum(P, axis=-1, keepdims=True)  # 正規化して確率にします\n",
                "P = P.reshape(S, A, S)\n",
                "np.testing.assert_allclose(P.sum(axis=-1), 1, atol=1e-6)  # ちゃんと確率行列になっているか確認します\n",
                "\n",
                "\n",
                "# 状態集合, 行動集合, 割引率, 報酬行列, 遷移確率行列が準備できたのでMDPのクラスを作ります\n",
                "\n",
                "class MDP(NamedTuple):\n",
                "    S_set: jnp.array  # 状態集合\n",
                "    A_set: jnp.array  # 行動集合\n",
                "    rew: jnp.array  # 報酬行列\n",
                "    P: jnp.array  # 遷移確率行列\n",
                "\n",
                "    @property\n",
                "    def S(self) -> int:  # 状態空間のサイズ\n",
                "        return len(self.S_set)\n",
                "\n",
                "    @property\n",
                "    def A(self) -> int:  # 行動空間のサイズ\n",
                "        return len(self.A_set)\n",
                "\n",
                "\n",
                "mdp = MDP(S_set, A_set, rew, P)\n",
                "\n",
                "print(\"状態数：\", mdp.S)\n",
                "print(\"行動数：\", mdp.A)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
