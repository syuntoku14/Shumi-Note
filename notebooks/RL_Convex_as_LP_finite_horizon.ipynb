{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 強化学習と線型計画法（有限ホライゾン）\n",
    "\n",
    "参考文献\n",
    "\n",
    "* [Linear programming formulation for non-stationary, finite-horizon Markov decision process models](https://www.sciencedirect.com/science/article/abs/pii/S0167637717301372)\n",
    "* [Exploration-Exploitation in Constrained MDPs](https://arxiv.org/abs/2003.02189)の2.3章\n",
    "* [Robust Control of Markov Decision Processes with Uncertain Transition Matrices](https://people.eecs.berkeley.edu/~elghaoui/Pubs/RobMDP_OR2005.pdf)：今回は上の論文を使いましたが，こっちのほうが証明としてはわかりやすいかも．\n",
    "\n",
    "強化学習が扱う最適方策の導出（プランニング問題）は線形計画問題としても定式化できます。\n",
    "有限ホライゾンの場合でも、無限ホライゾン([RL_as_LP.ipynb](RL_as_LP.ipynb))と似たような形式で線形計画問題に落とし込むことができます。\n",
    "\n",
    "**表記**（[RL_utils.ipynb](RL_utils.ipynb)参照）\n",
    "\n",
    "MDPを次で定義します。\n",
    "\n",
    "1. 有限状態集合: $S=\\{1, \\dots, |S|\\}$\n",
    "2. 有限行動集合: $A=\\{1, \\dots, |A|\\}$\n",
    "3. $h$ステップ目の遷移確率行列: $P_h\\in \\mathbb{R}^{SA\\times S}$\n",
    "4. $h$ステップ目の報酬行列: $r_h\\in \\mathbb{R}^{S\\times A}$\n",
    "5. ホライゾン: $H$\n",
    "6. 初期状態: $\\mu \\in \\mathbb{R}^{S}$\n",
    "\n",
    "また、次の占有率を導入しておきます。　\n",
    "* 占有率：$d_h^\\pi(s, a ; p):=\\mathbb{E}\\left[\\mathbb{1}\\left\\{s_h=s, a_h=a\\right\\} \\mid s_1=s_1, p, \\pi\\right]=\\operatorname{Pr}\\left\\{s_h=s, a_h=a \\mid s_1=s_1, p, \\pi\\right\\}$\n",
    "* 価値関数：$V_1^\\pi\\left(s_1\\right)=\\sum_{h, s, a} d_h^\\pi(s, a) r_h(s, a):=r^T d^\\pi(p)$\n",
    "\n",
    "## 主問題\n",
    "\n",
    "[RL_as_LP.ipynb](RL_as_LP.ipynb)と同様にすると，価値関数は任意の$h \\in[H]$について次を満たします。\n",
    "\n",
    "$$\n",
    "v_h(s) \\geq r(s, a) + \\sum_{s'\\in \\mathcal{S}} p_{h}\\left(s' \\mid s, a\\right) v_{h+1}(s^{\\prime}) \\forall (s, a) \\in \\mathcal{S}\\times \\mathcal{A} \n",
    "$$\n",
    "\n",
    "$h=H+1$については\n",
    "$$v_{H+1}(s)=0 \\quad \\forall s$$\n",
    "\n",
    "とします．\n",
    "\n",
    "これを使うと、有限ホライゾンの最適価値は\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\min_v \\sum_{h}\\sum_{s} v_h(s) &&\\\\\n",
    "\\text { s.t. } \n",
    "&v_h(s) \\geq r(s, a) + \\sum_{s'\\in \\mathcal{S}} p_{h}\\left(s' \\mid s, a\\right) v_{h+1}(s^{\\prime}) & & \\forall (s, a) \\in \\mathcal{S}\\times \\mathcal{A} \\\\\n",
    "&v_{H+1}(s)=0 \\quad & & \\forall s\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "を解けば求まります．\n",
    "\n",
    "### 証明\n",
    "\n",
    "これを証明してみましょう．まず，各ステップに対して次のベルマン作用素を定義します．\n",
    "\n",
    "$$\n",
    "B_h V_{h+1} \\triangleq \\max_\\pi r_h^\\pi + P^\\pi_{h} V_{h+1}\n",
    "$$\n",
    "\n",
    "さらに，$\\mathbf{V}=(V_1, V_2, \\cdots, V_H)$に対して，\n",
    "\n",
    "$$\n",
    "\\mathbf{B} \\mathbf{V} \\triangleq (B_1 V_2, B_2 V_3, \\cdots B_{H-1} V_H, B_H 0)\n",
    "$$\n",
    "\n",
    "を定義します．この作用素について，次が成立します．\n",
    "\n",
    "---\n",
    "\n",
    "任意の$\\mathbf{V}\\in \\mathbb{V}^H$について，もし$\\mathbf{V}\\leq \\mathbf{B}\\mathbf{V}$（$\\mathbf{V}\\geq \\mathbf{B}\\mathbf{V}$）なら，$\\mathbf{V} \\leq \\mathbf{V}^*$（$\\mathbf{V} \\geq \\mathbf{V}^*$）\n",
    "\n",
    "これは有限ホライゾン版の上界と下界の証明になります（[RL_as_LP.ipynb](RL_as_LP.ipynb)参照）．\n",
    "\n",
    "**$\\mathbf{V}\\geq \\mathbf{B}\\mathbf{V}$のとき**\n",
    "\n",
    "まず$\\mathbf{V}\\geq \\mathbf{B}\\mathbf{V}$のときを考えてみましょう．定義から，$V_h \\geq B_h V_{h+1}$を意味します．\n",
    "\n",
    "ここで，$B_h V_{h+1} \\triangleq \\max_\\pi r_h^\\pi + P^\\pi_{h} V_{h+1}$なので，\n",
    "\n",
    "$$\n",
    "V_h  \\geq B_h V_{h+1} = \\max_\\pi r_h^\\pi + P^\\pi_{h} V_{h+1} \\geq r_h^{\\pi'} + P^{\\pi'}_{h} V_{h+1} \\quad \\forall \\pi' \\in \\Pi\n",
    "$$\n",
    "\n",
    "が成り立ちます．\n",
    "すると，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V_1  \n",
    "&\\geq r_1^{\\pi'} + P^{\\pi'}_{1} V_{2}\\\\\n",
    "&\\geq r_1^{\\pi'} + P^{\\pi'}_{1} \\left(r_2^{\\pi'} + P^{\\pi'}_{2} V_{3}\\right)\n",
    "= r_1^{\\pi'} + P^{\\pi'}_{1} r_2^{\\pi'} + P^{\\pi'}_{1} P^{\\pi'}_{2} V_{3}\\\\\n",
    "&\\geq \\cdots  \\\\\n",
    "& \\geq r_1^{\\pi'} + P^{\\pi'}_{1} r_2^{\\pi'} + \\cdots \\\\\n",
    "&= V_1^{\\pi'}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "が任意の$\\pi'\\in \\Pi$について成り立ちます．よって，$V_1 \\geq V_1^*$も成り立ちます．\n",
    "\n",
    "**$\\mathbf{V}\\leq \\mathbf{B}\\mathbf{V}$のとき**\n",
    "\n",
    "定義から$V_h \\leq B_h V_{h+1}$です．\n",
    "\n",
    "まず，任意の非負のベクトル$\\epsilon=[\\epsilon_1, \\cdots, \\epsilon_H]$を考えます．\n",
    "このとき，$B_h V_{h+1} \\triangleq \\max_\\pi r_h^\\pi + P^\\pi_{h} V_{h+1}$なので，$\\max$の定義（今回のやり方なら$\\sup$でも大丈夫です）から，\n",
    "\n",
    "$$\n",
    "V_h \\leq B_h V_{h+1} = \\max_\\pi r_h^\\pi + P^\\pi_{h} V_{h+1}\n",
    "\\leq  r_h^{\\pi'} + P^{\\pi'}_{h} V_{h+1} + \\epsilon_h\n",
    "$$\n",
    "\n",
    "を満たす$\\pi'\\in \\Pi$が存在します．ここでさらに，適当な非負のベクトル$\\nu=[\\nu_1, \\cdots, \\nu_H]$を考え\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nu_h &\\geq \\sum^{h}_{i=1} \\epsilon_i \\prod^{i-1}_{j=1} P^{\\pi'}_{j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となるように$\\nu$を作ります（空な積は１とします）．\n",
    "すると，$\\nu_1 \\geq \\epsilon_1$, $\\nu_2 \\geq \\epsilon_1 + P_{1}^{\\pi'}\\epsilon_2$, ...\n",
    "なので，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V_1  \n",
    "&\\leq r_1^{\\pi'} + P^{\\pi'}_{1} V_{2} + \\epsilon_1\\\\\n",
    "&\\leq r_1^{\\pi'} + P^{\\pi'}_{1} r_2^{\\pi'} + P^{\\pi'}_{1} P^{\\pi'}_{2} V_{3} + \\epsilon_1 + P_1^{\\pi'}\\epsilon_2\\\\\n",
    "&\\leq \\cdots  \\\\\n",
    "& \\leq r_1^{\\pi'} + P^{\\pi'}_{1} r_2^{\\pi'} + \\cdots \\\\\n",
    "&= V_1^{\\pi'} + \\epsilon_1 +  P_1^{\\pi'}\\epsilon_2 +  P_1^{\\pi'}P_2^{\\pi'}\\epsilon_3 + \\cdots\\\\\n",
    "&\\leq V_1^{\\pi'} + \\nu_{H}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "が成り立ちます．\n",
    "一方で，$V_1^{\\pi'} \\leq V_1^{*}$であり，$\\epsilon$は任意に小さくできるので，$V_1 \\leq V_1^{*}$が成り立ちます．\n",
    "\n",
    "---\n",
    "\n",
    "以上より，$\\mathbf{B}$の不動点は$\\mathbf{V}^*$であり，前回のContraction Lemmaより，これを使った線形計画問題は最適価値関数を求めます．\n",
    "\n",
    "**（[Robust Control of Markov Decision Processes with Uncertain Transition Matrices](https://people.eecs.berkeley.edu/~elghaoui/Pubs/RobMDP_OR2005.pdf)：こっちのほうが証明としてはわかりやすいかも．）**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 双対問題\n",
    "\n",
    "占有率は任意の$h \\in[H] \\backslash\\{1\\}$について次を満たします。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sum_a d_h^\\pi(s, a) & =\\sum_{s^{\\prime}, a^{\\prime}} p_{h-1}\\left(s \\mid s^{\\prime}, a^{\\prime}\\right) d_{h-1}^\\pi\\left(s^{\\prime}, a^{\\prime}\\right) & & \\forall s \\in \\mathcal{S} \\\\\n",
    "d_h^\\pi(s, a) & \\geq 0 & & \\forall s, a\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$h=1$については\n",
    "$$d_1^\\pi(s, a)=\\pi_1(a \\mid s) \\cdot \\mu(s) \\quad \\forall s, a$$\n",
    "\n",
    "です。\n",
    "\n",
    "これを使うと、有限ホライゾンの最適方策は\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\max_d \\sum_{s, a, h} d_h(s, a) r_h(s, a) &&\\\\\n",
    "\\text { s.t. } &\\sum_a d_h(s, a) =\\sum_{s^{\\prime}, a^{\\prime}} p_{h-1}\\left(s \\mid s^{\\prime}, a^{\\prime}\\right) d_{h-1}\\left(s^{\\prime}, a^{\\prime}\\right) & & \\forall h \\in[H] \\backslash\\{1\\} \\\\\n",
    "& \\sum_a d_1(s, a) =\\mu(s) & & \\forall s \\in \\mathcal{S}\\\\\n",
    "& d_h(s, a) \\geq 0 & &\\forall(s, a, h) \\in \\mathcal{S} \\times \\mathcal{A} \\times[H]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "を解き、\n",
    "\n",
    "$$\n",
    "\\pi_h^d(a \\mid s)=\\frac{d_h(s, a)}{\\sum_b d_h(s, b)}, \\quad \\forall(s, a, h) \\in \\mathcal{S} \\times \\mathcal{A} \\times[H]\n",
    "$$\n",
    "\n",
    "とすれば求まります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状態数： 10\n",
      "行動数： 3\n",
      "ホライゾン： 30\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from typing import NamedTuple, Optional\n",
    "from jax.random import PRNGKey\n",
    "\n",
    "key = PRNGKey(0)\n",
    "\n",
    "S = 10  # 状態集合のサイズ\n",
    "A = 3  # 行動集合のサイズ\n",
    "S_set = jnp.arange(S)  # 状態集合\n",
    "A_set = jnp.arange(A)  # 行動集合\n",
    "H = 30  # ホライゾン\n",
    "\n",
    "# 報酬行列を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "rew = jax.random.uniform(key=key, shape=(H, S, A))\n",
    "assert rew.shape == (H, S, A)\n",
    "\n",
    "\n",
    "# 遷移確率行列を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "P = jax.random.uniform(key=key, shape=(H, S*A, S))\n",
    "P = P / jnp.sum(P, axis=-1, keepdims=True)  # 正規化して確率にします\n",
    "P = P.reshape(H, S, A, S)\n",
    "np.testing.assert_allclose(P.sum(axis=-1), 1, atol=1e-6)  # ちゃんと確率行列になっているか確認します\n",
    "\n",
    "\n",
    "# 初期状態分布を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "init_dist = jax.random.uniform(key, shape=(S,))\n",
    "init_dist = init_dist / jnp.sum(init_dist)\n",
    "np.testing.assert_allclose(init_dist.sum(axis=-1), 1, atol=1e-6)  # ちゃんと確率行列になっているか確認します\n",
    "\n",
    "\n",
    "# 状態集合, 行動集合, 割引率, 報酬行列, 遷移確率行列が準備できたのでMDPのクラスを作ります\n",
    "\n",
    "class MDP(NamedTuple):\n",
    "    S_set: jnp.array  # 状態集合\n",
    "    A_set: jnp.array  # 行動集合\n",
    "    H: int  # ホライゾン\n",
    "    rew: jnp.array  # 報酬行列\n",
    "    P: jnp.array  # 遷移確率行列\n",
    "    init_dist: jnp.array  # 初期分布\n",
    "    optimal_Q: Optional[jnp.ndarray] = None  # 最適Q値\n",
    "\n",
    "    @property\n",
    "    def S(self) -> int:  # 状態空間のサイズ\n",
    "        return len(self.S_set)\n",
    "\n",
    "    @property\n",
    "    def A(self) -> int:  # 行動空間のサイズ\n",
    "        return len(self.A_set)\n",
    "\n",
    "\n",
    "mdp = MDP(S_set, A_set, H, rew, P, init_dist)\n",
    "\n",
    "print(\"状態数：\", mdp.S)\n",
    "print(\"行動数：\", mdp.A)\n",
    "print(\"ホライゾン：\", mdp.H)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （準備）動的計画法\n",
    "\n",
    "**表記**\n",
    "\n",
    "* ステップ$h$の方策行列（$\\Pi_h^\\pi \\in \\mathbb{R}^{S\\times SA}$）：$\\langle \\pi_h, q\\rangle$を行列で書きたいときに便利。\n",
    "    * $\\Pi_h^\\pi(s,(s, a))=\\pi_h(a \\mid s)$ \n",
    "    * $\\Pi_h^\\pi q_h^\\pi = \\langle \\pi, q_h^\\pi \\rangle = v_h^\\pi$が成立。\n",
    "* ステップ$h$の遷移確率行列１（$P_h^\\pi \\in \\mathbb{R}^{SA\\times SA}$）: 次の状態についての方策の情報を追加したやつ。\n",
    "    * $P_h^\\pi = P_h \\Pi_h^\\pi$\n",
    "    * Q値を使ったベルマン期待作用素とかで便利。$q_h^\\pi = r_h + P_h^\\pi q^\\pi$が成立。\n",
    "* ステップ$h$の遷移確率行列２（$\\bar{P}_h^\\pi \\in \\mathbb{R}^{S\\times S}$）: 方策$\\pi$のもとでの状態遷移の行列。\n",
    "    * $\\bar{P}_h^\\pi = \\Pi_h^\\pi P_h$\n",
    "    * V値を使ったベルマン期待作用素とかで便利。$v_h^\\pi = \\Pi^\\pi r_h + \\gamma \\bar{P}_h^\\pi v^\\pi$。\n",
    "* ステップ$h$の訪問頻度（$d^\\pi_{h,\\mu} \\in \\mathbb{R}^{SA}$）：S, Aについての累積訪問頻度\n",
    "    * ${d}^\\pi_{h,\\mu} (s, a) = \\pi(a|s) \\sum_{s_0} \\mu(s_0) \\sum_{t=0}^h \\mathrm{Pr}\\left(S_t=s|S_0=s_0, M(\\pi)\\right)$\n",
    "\n",
    "\n",
    "**実装した関数**\n",
    "\n",
    "* ``compute_greedy_policy``: Q関数 ($H\\times S \\times A \\to \\mathcal{R}$) の貪欲方策を返します\n",
    "* ``compute_optimal_Q``: MDPの最適Q関数 $q_* : H\\times S \\times A \\to \\mathcal{R}$ を返します。\n",
    "* ``compute_policy_Q``: 方策 $\\pi$ のQ関数 $q_\\pi : H\\times S \\times A \\to \\mathcal{R}$ を返します。\n",
    "* ``compute_policy_matrix``: 方策$\\pi$の行列${\\Pi}^{\\pi} : H \\times S \\times SA$を返します。\n",
    "* ``compute_policy_visit``: 方策 $\\pi$ の割引訪問頻度${d}^\\pi_{\\mu} : {H\\times S \\times A}$ を返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最適価値関数と最適方策の価値関数の差 0.0\n",
      "0ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 1.9073486e-06\n",
      "1ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 1.9073486e-06\n",
      "2ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 1.9073486e-06\n",
      "3ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 0.0\n",
      "4ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 1.9073486e-06\n",
      "5ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 1.9073486e-06\n",
      "6ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 5.722046e-06\n",
      "7ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 9.536743e-06\n",
      "8ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 9.536743e-06\n",
      "9ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 3.8146973e-06\n",
      "10ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 3.8146973e-06\n",
      "11ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 9.536743e-07\n",
      "12ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 9.536743e-07\n",
      "13ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 9.536743e-07\n",
      "14ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 0.0\n",
      "15ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 0.0\n",
      "16ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 9.536743e-07\n",
      "17ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 9.536743e-07\n",
      "18ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 2.861023e-06\n",
      "19ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 1.9073486e-06\n",
      "20ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 1.4305115e-06\n",
      "21ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 9.536743e-07\n",
      "22ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 9.536743e-07\n",
      "23ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 0.0\n",
      "24ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 4.7683716e-07\n",
      "25ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 2.3841858e-07\n",
      "26ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 2.3841858e-07\n",
      "27ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 2.3841858e-07\n",
      "28ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 0.0\n",
      "29ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 0.0\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import jax\n",
    "import chex\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_greedy_policy(Q: jnp.ndarray):\n",
    "    \"\"\"Q関数の貪欲方策を返します\n",
    "\n",
    "    Args:\n",
    "        Q (jnp.ndarray): (HxSxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        greedy_policy (jnp.ndarray): (HxSxA)の行列\n",
    "    \"\"\"\n",
    "    greedy_policy = jnp.zeros_like(Q)\n",
    "    H, S, A = Q.shape\n",
    "    \n",
    "    def body_fn(i, greedy_policy):\n",
    "        greedy_policy = greedy_policy.at[i, jnp.arange(S), Q[i].argmax(axis=-1)].set(1)\n",
    "        return greedy_policy\n",
    "\n",
    "    greedy_policy = jax.lax.fori_loop(0, H, body_fn, greedy_policy)\n",
    "    chex.assert_shape(greedy_policy, (H, S, A))\n",
    "    return greedy_policy\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"H\", \"S\", \"A\"))\n",
    "def _compute_optimal_Q(mdp: MDP, H: int, S: int, A: int):\n",
    "    \"\"\"ベルマン最適作用素をホライゾン回走らせて最適価値関数を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "\n",
    "    Returns:\n",
    "        optimal_Q (jnp.ndarray): (HxSxA)の行列\n",
    "    \"\"\"\n",
    "\n",
    "    def backup(i, optimal_Q):\n",
    "        h = H - i - 1\n",
    "        max_Q = optimal_Q[h+1].max(axis=1)\n",
    "        next_v = mdp.P[h] @ max_Q\n",
    "        chex.assert_shape(next_v, (S, A))\n",
    "        optimal_Q = optimal_Q.at[h].set(mdp.rew[h] + next_v)\n",
    "        return optimal_Q\n",
    "    \n",
    "    optimal_Q = jnp.zeros((H+1, S, A))\n",
    "    optimal_Q = jax.lax.fori_loop(0, mdp.H, backup, optimal_Q)\n",
    "    return optimal_Q[:-1]\n",
    "\n",
    "compute_optimal_Q = lambda mdp: _compute_optimal_Q(mdp, mdp.H, mdp.S, mdp.A)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_Q(mdp: MDP, policy: jnp.ndarray):\n",
    "    \"\"\"ベルマン期待作用素をホライゾン回走らせて価値関数を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "        policy (np.ndarray): (HxSxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        optimal_Q (jnp.ndarray): (HxSxA)の行列\n",
    "    \"\"\"\n",
    "    H, S, A = policy.shape\n",
    "\n",
    "    def backup(i, policy_Q):\n",
    "        h = H - i - 1\n",
    "        max_Q = (policy[h+1] * policy_Q[h+1]).sum(axis=1)\n",
    "        next_v = mdp.P[h] @ max_Q\n",
    "        chex.assert_shape(next_v, (S, A))\n",
    "        policy_Q = policy_Q.at[h].set(mdp.rew[h] + next_v)\n",
    "        return policy_Q\n",
    "    \n",
    "    policy_Q = jnp.zeros((H+1, S, A))\n",
    "    policy_Q = jax.lax.fori_loop(0, mdp.H, backup, policy_Q)\n",
    "    return policy_Q[:-1]\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_matrix(policy: jnp.ndarray):\n",
    "    \"\"\"\n",
    "    上で定義した方策行列を計算します。方策についての内積が取りたいときに便利です。\n",
    "    Args:\n",
    "        policy (jnp.ndarray): (HxSxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        policy_matrix (jnp.ndarray): (HxSxSA)の行列\n",
    "    \"\"\"\n",
    "    H, S, A = policy.shape\n",
    "    PI = policy.reshape(H, 1, S, A)\n",
    "    PI = jnp.tile(PI, (1, S, 1, 1))\n",
    "    eyes = jnp.tile(jnp.eye(S).reshape(1, S, S, 1), (H, 1, 1, 1))\n",
    "    PI = (eyes * PI).reshape(H, S, S*A)\n",
    "    return PI\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_visit(mdp: MDP, policy: jnp.ndarray, init_dist: jnp.ndarray):\n",
    "    \"\"\"MDPと方策について、訪問頻度を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "        policy (jnp.ndarray): (HxSxA)の行列\n",
    "        init_dist (jnp.ndarray): (S) 初期状態の分布\n",
    "\n",
    "    Returns:\n",
    "        visit (jnp.ndarray): (HxSxA)のベクトル\n",
    "    \"\"\"\n",
    "    H, S, A = policy.shape\n",
    "    Pi = compute_policy_matrix(policy)\n",
    "    P = mdp.P.reshape(H, S*A, S)\n",
    "\n",
    "    def body_fn(h, visit):\n",
    "        next_visit = visit[h] @ P[h] @ Pi[h+1]\n",
    "        visit = visit.at[h+1].set(next_visit)\n",
    "        return visit\n",
    "    \n",
    "    visit = jnp.zeros((H+1, S*A))\n",
    "    visit = visit.at[0].set((init_dist @ Pi[0]))\n",
    "    visit = jax.lax.fori_loop(0, mdp.H, body_fn, visit)\n",
    "    visit = visit[:-1].reshape(H, S, A)\n",
    "    return visit\n",
    "\n",
    "\n",
    "# 動的計画法による最適価値関数\n",
    "optimal_Q_DP = compute_optimal_Q(mdp)\n",
    "optimal_V_DP = optimal_Q_DP.max(axis=-1)\n",
    "optimal_policy = compute_greedy_policy(optimal_Q_DP)\n",
    "optimal_policy_Q_DP = compute_policy_Q(mdp, optimal_policy)\n",
    "mdp = mdp._replace(optimal_Q=optimal_Q_DP)\n",
    "print(\"最適価値関数と最適方策の価値関数の差\", jnp.abs(optimal_Q_DP - optimal_policy_Q_DP).max())\n",
    "\n",
    "# 訪問頻度によるリターンの計算\n",
    "policy_visit = compute_policy_visit(mdp, optimal_policy, mdp.init_dist)\n",
    "np.testing.assert_allclose(policy_visit.sum(axis=(1, 2)), 1.0, atol=1e-6)\n",
    "np.testing.assert_allclose(policy_visit[0].sum(axis=-1), mdp.init_dist, atol=1e-6)\n",
    "for h in range(H):\n",
    "    return_by_visit = (policy_visit * mdp.rew)[h:].sum()\n",
    "    return_by_DP = (optimal_Q_DP[h] * policy_visit[h]).sum()\n",
    "    print(f\"{h}ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差\", np.abs(return_by_visit - return_by_DP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CBC MILP Solver \n",
      "Version: 2.10.3 \n",
      "Build Date: Dec 15 2019 \n",
      "\n",
      "command line - /home/toshinori/shumi-note/.venv/lib/python3.9/site-packages/pulp/solverdir/cbc/linux/64/cbc /tmp/bded243e260f4ccda06e94bae5c3b865-pulp.mps timeMode elapsed branch printingOptions all solution /tmp/bded243e260f4ccda06e94bae5c3b865-pulp.sol (default strategy 1)\n",
      "At line 2 NAME          MODEL\n",
      "At line 3 ROWS\n",
      "At line 915 COLUMNS\n",
      "At line 11136 RHS\n",
      "At line 12047 BOUNDS\n",
      "At line 12358 ENDATA\n",
      "Problem MODEL has 910 rows, 310 columns and 9910 elements\n",
      "Coin0008I MODEL read with 0 errors\n",
      "Option for timeMode changed from cpu to elapsed\n",
      "Presolve 610 (-300) rows, 210 (-100) columns and 6610 (-3300) elements\n",
      "Perturbing problem by 0.001% of 33.819188 - largest nonzero change 0.00098628197 ( 0.20168331%) - largest zero change 0\n",
      "0  Obj 1408.2352 Primal inf 6292.115 (384) Dual inf 3.9312638 (110) w.o. free dual inf (0)\n",
      "14  Obj -4.1014187e+11 Primal inf 6.0289691e+12 (349) Dual inf 27.22492 (85) w.o. free dual inf (6)\n",
      "14  Obj -6.5136045e+11 Primal inf 1.2049153e+13 (319) Dual inf 6.9504331e+15 (133) w.o. free dual inf (37)\n",
      "End of values pass after 15 iterations\n",
      "15  Obj -6.5136045e+11 Primal inf 1.2049153e+13 (318) Dual inf 6.9476587e+15 (132) w.o. free dual inf (37)\n",
      "65  Obj -6.8658465e+09 Primal inf 4.4811884e+11 (220) Dual inf 1.672107e+16 (100) w.o. free dual inf (55)\n",
      "116  Obj 3603.6071 Primal inf 329.91044 (145) Dual inf 9.075729e+15 (118)\n",
      "203  Obj 4621.0739 Primal inf 98.320099 (65) Dual inf 3.3069453e+15 (81)\n",
      "259  Obj 5351.9501 Primal inf 16.068121 (19) Dual inf 1.0054754e+15 (102)\n",
      "316  Obj 4049.6471 Dual inf 246.71125 (35)\n",
      "382  Obj 3520.5106\n",
      "Optimal - objective value 3520.5106\n",
      "After Postsolve, objective 3520.5106, infeasibilities - dual 0 (0), primal 9.7650157e-08 (5)\n",
      "Presolved model was optimal, full model needs cleaning up\n",
      "Optimal - objective value 3520.5106\n",
      "Optimal objective 3520.510596 - 382 iterations time 0.012, Presolve 0.00\n",
      "Option for printingOptions changed from normal to all\n",
      "Total time (CPU seconds):       0.01   (Wallclock seconds):       0.01\n",
      "\n",
      "最適価値関数とLPによる価値関数の差 3.8146973e-06\n"
     ]
    }
   ],
   "source": [
    "import pulp\n",
    "from itertools import product\n",
    "\n",
    "# 主問題を解きます\n",
    "prob = pulp.LpProblem(name=\"MDP\", sense=pulp.LpMinimize)\n",
    "hsa_indices = [(h, s, a) for h, s, a in product(range(H), range(S), range(A))]\n",
    "hs_indices = [(h, s) for h, s in product(range(H+1), range(S))]\n",
    "sa_indices = [(s, a) for s, a in product(range(S), range(A))]\n",
    "v = pulp.LpVariable.dicts(\"v\", hs_indices, cat=\"Continuous\")\n",
    "\n",
    "# 目的関数\n",
    "prob += pulp.lpSum([v[hs] for hs in hs_indices])\n",
    "\n",
    "# h=Hについての制約\n",
    "for s in range(S):\n",
    "    prob += v[(H, s)] == 0.0\n",
    "\n",
    "# 各ステップについての制約\n",
    "for h in range(H):\n",
    "    for s in range(S):\n",
    "        for a in range(A):\n",
    "            v_hs = v[(h, s)]\n",
    "            bel_v_hs = mdp.rew[h, s, a].item() + pulp.lpSum([mdp.P[h, s, a, ns].item() * v[(h+1, ns)] for ns in range(S)])\n",
    "            prob += v_hs >= bel_v_hs\n",
    "\n",
    "\n",
    "sol = prob.solve()\n",
    "v_LP = jnp.array([pulp.value(v[h, s]) for (h, s) in hs_indices])\n",
    "V_LP = v_LP.reshape((H+1, S))[:-1]\n",
    "\n",
    "print(\"最適価値関数とLPによる価値関数の差\", jnp.abs(optimal_Q_DP.max(axis=-1) - V_LP).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CBC MILP Solver \n",
      "Version: 2.10.3 \n",
      "Build Date: Dec 15 2019 \n",
      "\n",
      "command line - /home/toshinori/shumi-note/.venv/lib/python3.9/site-packages/pulp/solverdir/cbc/linux/64/cbc /tmp/9d2831d4dfa649c7b8311f2be3283686-pulp.mps max timeMode elapsed branch printingOptions all solution /tmp/9d2831d4dfa649c7b8311f2be3283686-pulp.sol (default strategy 1)\n",
      "At line 2 NAME          MODEL\n",
      "At line 3 ROWS\n",
      "At line 305 COLUMNS\n",
      "At line 10806 RHS\n",
      "At line 11107 BOUNDS\n",
      "At line 11108 ENDATA\n",
      "Problem MODEL has 300 rows, 900 columns and 9600 elements\n",
      "Coin0008I MODEL read with 0 errors\n",
      "Option for timeMode changed from cpu to elapsed\n",
      "Presolve 190 (-110) rows, 550 (-350) columns and 5950 (-3650) elements\n",
      "0  Obj -0 Primal inf 1.9339191 (10) Dual inf 1046.5361 (550)\n",
      "0  Obj -0 Primal inf 1.9339191 (10) Dual inf 1.3796436e+12 (550)\n",
      "31  Obj -0 Primal inf 1.9339191 (10) Dual inf 5.542992e+12 (445)\n",
      "62  Obj -0 Primal inf 1.9339191 (10) Dual inf 7.0748679e+12 (370)\n",
      "93  Obj -0 Primal inf 1.9339191 (10) Dual inf 8.3266145e+12 (315)\n",
      "124  Obj -0 Primal inf 1.9339191 (10) Dual inf 8.3284794e+12 (252)\n",
      "173  Obj -0 Primal inf 1.9339191 (10) Dual inf 3.6353634e+12 (162)\n",
      "224  Obj 22.022114 Dual inf 22.945284 (57)\n",
      "255  Obj 22.857271 Dual inf 1.3440467 (10)\n",
      "265  Obj 22.890361\n",
      "Optimal - objective value 22.890361\n",
      "After Postsolve, objective 22.890361, infeasibilities - dual 0 (0), primal 0 (0)\n",
      "Optimal objective 22.89036147 - 265 iterations time 0.002, Presolve 0.00\n",
      "Option for printingOptions changed from normal to all\n",
      "Total time (CPU seconds):       0.00   (Wallclock seconds):       0.01\n",
      "\n",
      "最適価値関数とLP-Dualによる価値関数の差 0.0\n"
     ]
    }
   ],
   "source": [
    "import pulp\n",
    "from itertools import product\n",
    "\n",
    "# 双対問題を解きます\n",
    "prob = pulp.LpProblem(name=\"MDP\", sense=pulp.LpMaximize)\n",
    "hsa_indices = [(h, s, a) for h, s, a in product(range(H), range(S), range(A))]\n",
    "sa_indices = [(s, a) for s, a in product(range(S), range(A))]\n",
    "d = pulp.LpVariable.dicts(\"d\", hsa_indices, lowBound=0, cat=\"Continuous\")\n",
    "\n",
    "# 目的関数\n",
    "prob += pulp.lpSum([d[hsa] * mdp.rew[hsa[0], hsa[1], hsa[2]] for hsa in hsa_indices])\n",
    "\n",
    "# 初期状態についての制約\n",
    "for s in range(S):\n",
    "    d_0sa = [d[(0, s, a)] for a in range(A)]\n",
    "    prob += pulp.lpSum(d_0sa) == mdp.init_dist[s].item()\n",
    "\n",
    "# 各ステップについての制約\n",
    "for h in range(1, H):\n",
    "    for ns in range(S):\n",
    "        d_hns = pulp.lpSum([d[(h, ns, na)] for na in range(A)])\n",
    "        d_phns = pulp.lpSum([d[(h-1, sa[0], sa[1])] * mdp.P[h-1, sa[0], sa[1], ns] for sa in sa_indices])\n",
    "        prob += d_hns == d_phns\n",
    "\n",
    "\n",
    "sol = prob.solve()\n",
    "d_arr = jnp.array([pulp.value(d[h, s, a]) for (h, s, a) in hsa_indices])\n",
    "d_arr = d_arr.reshape(H, S, A)\n",
    "\n",
    "np.testing.assert_allclose(d_arr.sum(axis=(1, 2)), 1.0, atol=1e-4)\n",
    "policy = d_arr / d_arr.sum(axis=-1, keepdims=True)\n",
    "Q_LP = compute_policy_Q(mdp, policy)\n",
    "\n",
    "print(\"最適価値関数とLP-Dualによる価値関数の差\", jnp.abs(optimal_Q_DP - Q_LP).max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PULPを使ってもいいですが，かなり遅いです．なんとか通常の行列形式に書き換えてみましょう．\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 行列形式の主問題\n",
    "\n",
    "元の問題は\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\min_v \\sum_{h}\\sum_{s} v_h(s) &&\\\\\n",
    "\\text { s.t. } \n",
    "&\\sum_{s'\\in \\mathcal{S}} p_{h}\\left(s' \\mid s, a\\right) v_{h+1}(s^{\\prime}) - v_h(s) \\leq - r(s, a) & & \\forall (h, s, a) \\in [H]\\times \\mathcal{S}\\times \\mathcal{A} \\\\\n",
    "&v_{H+1}(s)=0 \\quad & & \\forall s\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "でした．この制約の部分を行列の不等式と等式の制約に直します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "Bup = np.zeros((H+1, S, A, H+1, S))\n",
    "Beq = np.zeros((H+1, S, H+1, S))\n",
    "\n",
    "# h=H+1についての制約\n",
    "for s in range(S):\n",
    "    Beq[H, s, H, s] = 1\n",
    "\n",
    "# 遷移についての制約\n",
    "for h, s, a in product(range(H), range(S), range(A)):\n",
    "    Bup[h, s, a, h, s] = -1  # v(h, s) を実現します\n",
    "    Bup[h, s, a, h+1] = mdp.P[h, s, a]  # sum_s' p(s', s, a)v(s') を実現します\n",
    "\n",
    "Bup = Bup.reshape(((H+1)*S*A, (H+1)*S))\n",
    "Beq = Beq.reshape(((H+1)*S, (H+1)*S))\n",
    "bup = np.hstack((-mdp.rew.reshape(-1), np.zeros(S*A)))\n",
    "beq = np.zeros((H+1)*S)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで行列形式での制約ができました．これを使って，次の問題をscipyで解きます．\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\min_v w^T v \\;\\; \\text { s.t. }  B_{\\text{up}} v \\leq b_{\\text{up}} \\; \\text{ and }\\; B_{\\text{eq}} v = 0\\;\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最適価値関数と行列形式のLPによる価値関数の差 3.8146973e-06\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import linprog\n",
    "\n",
    "w = np.ones((H+1)*S)\n",
    "lin_res = linprog(w, A_eq=Beq, b_eq=beq, A_ub=Bup, b_ub=bup)\n",
    "v_LP_matrix = lin_res.x.reshape(H+1, S)[:-1]\n",
    "\n",
    "print(\"最適価値関数と行列形式のLPによる価値関数の差\", jnp.abs(optimal_Q_DP.max(axis=-1) - v_LP_matrix).max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 行列形式の双対問題\n",
    "\n",
    "元の問題は\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\max_d \\sum_{s, a, h} d_h(s, a) r_h(s, a) &&\\\\\n",
    "\\text { s.t. } &\\sum_a d_h(s, a) - \\sum_{s^{\\prime}, a^{\\prime}} p_{h-1}\\left(s \\mid s^{\\prime}, a^{\\prime}\\right) d_{h-1}\\left(s^{\\prime}, a^{\\prime}\\right) = 0& & \\forall h \\in[H] \\backslash\\{1\\} \\\\\n",
    "& \\sum_a d_1(s, a) =\\mu(s) & & \\forall s \\in \\mathcal{S}\\\\\n",
    "& d_h(s, a) \\geq 0 & &\\forall(s, a, h) \\in \\mathcal{S} \\times \\mathcal{A} \\times[H]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "でした．この制約の部分を\n",
    "\n",
    "$$\n",
    "B d = b\n",
    "$$\n",
    "\n",
    "の形に直します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "d = d_arr\n",
    "d = d.reshape(H * S * A)\n",
    "\n",
    "B = np.zeros((H, S, A, H, S, A))\n",
    "\n",
    "# 初期状態についての制約\n",
    "for s, a in product(range(S), range(A)):\n",
    "    B[0, s, a, 0, s] = 1\n",
    "\n",
    "\n",
    "# 遷移についての制約\n",
    "for h, s, a in product(range(1, H), range(S), range(A)):\n",
    "    B[h, s, a, h, s] = 1  # sum_a d(h, s, a) を実現します\n",
    "    B[h, s, a, h-1] = -mdp.P[h-1, :, :, s]  # sum_a d(h, s, a) を実現します\n",
    "\n",
    "\n",
    "B = B.reshape((H*S*A, H*S*A))\n",
    "mu = np.repeat(mdp.init_dist[:, None], A, axis=1).reshape(-1)\n",
    "b = np.hstack((mu, np.zeros((H-1)*S*A)))\n",
    "\n",
    "np.testing.assert_almost_equal(B @ d, b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで行列形式での制約ができました．これを使って，次の問題をscipyで解きます．\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\max d^T r \\;\\; \\text { s.t. }  B d = b \\; \\text{ and }\\; d \\geq 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最適価値関数とLPによる価値関数の差 0.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import linprog\n",
    "\n",
    "r = - mdp.rew.reshape(-1)\n",
    "lin_res = linprog(r, A_eq=B, b_eq=b, bounds=(0, None))\n",
    "\n",
    "d_arr_matrix = lin_res.x.reshape(H, S, A)\n",
    "policy = d_arr_matrix / d_arr_matrix.sum(axis=-1, keepdims=True)\n",
    "Q_LP_matrix = compute_policy_Q(mdp, policy)\n",
    "\n",
    "print(\"最適価値関数と行列形式のLPによる価値関数の差\", jnp.abs(optimal_Q_DP - Q_LP_matrix).max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('shumi-VTLwuKSy-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6b7cac5e0d2ff733f340da4d53ae5ecfef7f7ad39623f5982b029a09306b36b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
