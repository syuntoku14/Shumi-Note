{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLの証明に便利な定理\n",
    "\n",
    "表記は元論文や雰囲気におまかせします．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 制約付き凸最適化\n",
    "\n",
    "参考：\n",
    "* [Exploration-Exploitation in Constrained MDPs](https://arxiv.org/abs/2003.02189)のAppendix G\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "準備：\n",
    "\n",
    "次の制約付き最適化を考えましょう．\n",
    "\n",
    "$$\n",
    "f_{\\mathrm{opt}}=\\min _{\\mathbf{x} \\in X}\\{f(\\mathbf{x}): \\mathbf{g}(\\mathbf{x}) \\leq \\mathbf{0}, \\mathbf{A} \\mathbf{x}+\\mathbf{b}=\\mathbf{0}\\}\n",
    "$$\n",
    "\n",
    "ここで，$\\mathbf{g}(\\mathbf{x}):=\\left(g_1(\\mathbf{x}), . ., g_I(\\mathbf{x})\\right)^T$と$f, g_1, . ., g_m: \\mathbb{E} \\rightarrow(-\\infty, \\infty)$は凸な実数関数です．また，$\\mathbf{A} \\in \\mathbb{R}^{p \\times n}, \\mathbf{b} \\in \\mathbb{R}^p$とします．\n",
    "\n",
    "これに対して，感度関数を\n",
    "\n",
    "$$\n",
    "v(\\mathbf{u}, \\mathbf{t})=\\min _{\\mathbf{x} \\in X}\\{f(\\mathbf{x}): \\mathbf{g}(\\mathbf{x}) \\leq \\mathbf{u}, \\mathbf{A} \\mathbf{x}+\\mathbf{b}=\\mathbf{t}\\}\n",
    "$$\n",
    "\n",
    "とします．\n",
    "また，双対関数を\n",
    "\n",
    "$$\n",
    "q(\\lambda, \\mu)=\\min _{x \\in X}\\left\\{L(\\mathbf{x}, \\lambda, \\mu)=f(\\mathbf{x})+\\lambda^T \\mathbf{g}(\\mathbf{x})+\\mu^T(\\mathbf{A} \\mathbf{x}+\\mathbf{b})\\right\\}\n",
    "$$\n",
    "\n",
    "として（$\\lambda \\in \\mathbb{R}_{+}^m, \\mu \\in \\mathbb{R}^p$です．），双対問題を\n",
    "\n",
    "$$\n",
    "q_{\\mathrm{opt}}=\\max _{\\lambda \\in \\mathbb{R}_{+}^m, \\mu \\in \\mathbb{R}^p}\\{q(\\lambda, \\mu):(\\lambda, \\mu) \\in \\operatorname{dom}(-q)\\}\n",
    "$$\n",
    "\n",
    "とします．ここで，$\\operatorname{dom}(-q)=\\left\\{(\\lambda, \\mu) \\in \\mathbb{R}_{+}^m, \\mu \\in \\mathbb{R}^p: q(\\lambda, \\mu)>-\\infty\\right\\}$です．\n",
    "\n",
    "---\n",
    "\n",
    "仮定：\n",
    "\n",
    "最適値は有限であり，$g(\\overline{\\mathbf{x}})<0$および$\\mathbf{A} \\widehat{\\mathbf{x}}+\\mathbf{b}=0$であるような$\\bar{x}$と$\\widehat{\\mathbf{x}} \\in \\operatorname{ri}(X)$が存在するとします．\n",
    "ここで，$\\operatorname{ri}(X)$は$X$の相対的内部です．\n",
    "\n",
    "---\n",
    "\n",
    "**定理**\n",
    "\n",
    "$(\\lambda^*, \\mu^*)$は次を満たし，またそのときに限り最適解です．\n",
    "\n",
    "$$\n",
    "-\\left(\\lambda^*, \\mu^*\\right) \\in \\partial v(\\mathbf{0}, \\mathbf{0})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**定理**\n",
    "\n",
    "$2\\left\\|\\lambda^*\\right\\|_1 \\leq \\rho$とします．\n",
    "$\\widetilde{\\mathbf{x}}$ は$\\mathbf{A} \\widetilde{\\mathbf{x}}+\\mathbf{b}=0$\n",
    "および\n",
    "$$\n",
    "f(\\widetilde{\\mathbf{x}})-f_{o p t}+\\rho\\left\\|[g(\\widetilde{\\mathbf{x}})]_{+}\\right\\|_{\\infty} \\leq \\delta,\n",
    "$$\n",
    "とします．このとき，\n",
    "\n",
    "$$\n",
    "\\left\\|[g(\\widetilde{\\mathbf{x}})]_{+}\\right\\|_{\\infty} \\leq \\frac{\\delta}{\\rho}\n",
    "$$\n",
    "\n",
    "が成立します．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 制約付きMDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "表記（[Last-Iterate Convergent Policy Gradient Primal-Dual Methods for Constrained MDPs](https://arxiv.org/abs/2306.11700)参照）：\n",
    "\n",
    "* 制約付きMDP：$\\underset{\\pi \\in \\Pi}{\\operatorname{maximize}} V_r^\\pi(\\rho) \\quad \\text { subject to } V_u^\\pi(\\rho) \\geq b$：\n",
    "* $g:=u-(1-\\gamma) b$とする\n",
    "* $L(\\pi, \\lambda):=V_r^\\pi(\\rho)+\\lambda V_g^\\pi(\\rho)$\n",
    "* ラグランジュ形式：$\\underset{\\pi \\in \\Pi}{\\operatorname{maximize}} \\underset{\\lambda \\in[0, \\infty]}{\\operatorname{minimize}} V_{r+\\lambda g}^\\pi(\\rho)$\n",
    "    * 鞍点$(\\pi', \\lambda')$：$V_{r+\\lambda^{\\prime} g}^\\pi(\\rho) \\leq V_{r+\\lambda^{\\prime} g}^{\\pi^{\\prime}}(\\rho) \\leq V_{r+\\lambda g}^{\\pi^{\\prime}}(\\rho)$\n",
    "* 主問題：$V_P^\\pi(\\rho):=\\inf _{\\lambda \\in[0, \\infty]} V_{r+\\lambda g}^\\pi(\\rho)$\n",
    "* 双対問題：$V_D^\\lambda(\\rho):=\\max _{\\pi \\in \\Pi} V_{r+\\lambda g}^\\pi(\\rho)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**補題：強双対性**\n",
    "\n",
    "参考：\n",
    "* [Safe Policies for Reinforcement Learning via Primal-Dual Methods](https://arxiv.org/abs/1911.09101)のTheorem 3\n",
    "\n",
    "$$\n",
    "V_P^{\\pi^{\\star}}(\\rho)=V_D^{\\lambda^{\\star}}(\\rho)\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**補題：未定乗数の範囲**\n",
    "\n",
    "参考：\n",
    "* [Natural Policy Gradient Primal-Dual Method for Constrained Markov Decision Processes](https://proceedings.neurips.cc/paper/2020/hash/5f7695debd8cde8db5abcb9f161b49ea-Abstract.html)のLemma 1\n",
    "\n",
    "次のFeasibilityが成立するとする：$\\bar{\\pi} \\in \\Pi$ and $\\xi>0$ such that $V_g^{\\bar{\\pi}}(\\rho) \\geq \\xi$．\n",
    "このとき，\n",
    "\n",
    "$$\n",
    "\\lambda^{\\star} \\in\\left[0,\\left(V_r^{\\bar{\\pi}}-V_r^{\\pi^{\\star}}\\right) / \\xi\\right]\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Lemma\n",
    "\n",
    "---\n",
    "\n",
    "**補題: Extended Value Difference**\n",
    "\n",
    "参考：\n",
    "* [Exploration-Exploitation in Constrained MDPs](https://arxiv.org/abs/2003.02189)の補題34など\n",
    "\n",
    "表記：\n",
    "* 方策：$\\pi, \\pi'$\n",
    "* MDP：$\\mathcal{M}=(\\mathcal{S}, \\mathcal{A}, \\{p_h\\}_{h=1}^H, \\{r_h\\}_{h=1}^H)$と$\\mathcal{M}'=(\\mathcal{S}, \\mathcal{A}, \\{p_h'\\}_{h=1}^H, \\{r_h'\\}_{h=1}^H)$\n",
    "* $\\widehat{Q}_h^\\pi(s, a; r, p)$を$\\mathcal{M}$でのQ関数の近似\n",
    "* $\\widehat{V}_h^\\pi(s; r, p)=\\left\\langle \\widehat{Q}_h^\\pi(s, \\cdot; r, p), \\pi_h(\\cdot \\mid s)\\right\\rangle$\n",
    "\n",
    "のとき，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{V}_h^\\pi(s; r, p)\n",
    "-\n",
    "{V}_h^{\\pi'}(s; r', p')\n",
    "=\n",
    "&\\sum^{H}_{h=1}\\mathbb{E}\\left[\n",
    "\\left\\langle \\widehat{Q}_h^\\pi(s_h, \\cdot; r, p), \\pi_h'(\\cdot \\mid s_h) - \\pi_h(\\cdot \\mid s_h)\\right\\rangle\n",
    "\\mid s_1, \\pi', p'\n",
    "\\right]\n",
    "+ \\\\\n",
    "&\\sum^{H}_{h=1}\\mathbb{E}\\left[\n",
    "\\widehat{Q}_h^\\pi(s_h, a_h; r, p) - r_h'(s_h, a_h) - p_h'(\\cdot \\mid s_h, a_h)\\widehat{V}_{h+1}^\\pi(\\cdot; r, p) \\mid s_1, \\pi', p'\n",
    "\\right]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Good Set\n",
    "\n",
    "表記\n",
    "* $w_{tj}(s, a)$：エピソード$j$のステップ$t \\in [H]$で$(s, a)$を訪れる確率\n",
    "* $w_j(s, a):= \\sum_{t=1}^H w_{tj}(s, a)$\n",
    "* $n_j(s, a)$：エピソード$j$までに$(s, a)$を訪れる回数\n",
    "\n",
    "直感的には，次の$L_k$は状態行動空間を「十分訪れたことがある集合」と「そんなに訪れたことがない集合」に分割します．\n",
    "$L_k$の中身の状態行動の訪問回数は，そのoccupancy measureで下から抑えることができます．\n",
    "これはリグレットのバウンドに便利です．\n",
    "\n",
    "---\n",
    "\n",
    "**定義: Good Set**\n",
    "\n",
    "参考：\n",
    "* [Tight Regret Bounds for Model-Based Reinforcement Learning with Greedy Policies](https://arxiv.org/abs/1905.11527)のAppendix F.1\n",
    "* [Tighter Problem-Dependent Regret Bounds in Reinforcement Learning without Domain Knowledge using Value Function Bounds](https://arxiv.org/abs/1901.00210)のAppendix G．こっちのほうがわかりやすいかも．\n",
    "\n",
    "次の集合$L_k$を定義します：\n",
    "\n",
    "$$\n",
    "L_k:= \\left\\{\n",
    "(s, a) \\in \\mathcal{S}\\times \\mathcal{A}: \n",
    "\\frac{1}{4}\n",
    "\\sum_{j<k} w_j(s, a) \\geq H \\ln \\frac{SAH}{\\delta'} + H\n",
    "\\right\\}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**補題: Failure event**\n",
    "\n",
    "参考：\n",
    "* [Policy Certificates: Towards Accountable Reinforcement Learning](https://arxiv.org/abs/1811.03056)のLemma 6\n",
    "\n",
    "次のFailure Eventを考えます．\n",
    "\n",
    "$$\n",
    "F^N = \\left\\{\\exist k, s, a : n_k(s, a) < \\frac{1}{2}\\sum_{i < k} w_i(s, a) - H \\ln \\frac{SAH}{\\delta'}\\right\\}\n",
    "$$\n",
    "\n",
    "このとき，\n",
    "$$\n",
    "\\mathbb{P}\\left(F^N\\right) \\leq S A H \\delta^{\\prime}\n",
    "$$\n",
    "\n",
    "が成り立ちます．\n",
    "\n",
    "**証明**\n",
    "\n",
    "* $s \\in \\mathcal{S}, a \\in \\mathcal{A}, t \\in [H]$を固定します．\n",
    "* $\\mathcal{F}_k$を，$k$エピソード目の初期状態$s_{k, 1}$と，$k-1$エピソードまでから誘導される$\\sigma$代数とします．\n",
    "* $X_k$を，$s, a$がエピソード$k$の$t$ステップ目に訪問される指示関数とします．\n",
    "\n",
    "このとき，$X_k=1$である確率，つまり\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\left(s=s_{k, t}, a=a_{k, t} \\mid s_{k, 1}, \\pi_k\\right)\n",
    "$$\n",
    "\n",
    "は，$\\mathcal{F}_k$-measurableです．あとは次の補題に$W=\\ln \\frac{SAH}{\\delta'}$を当てはめて，Union boundを取れば成立します．\n",
    "\n",
    "**補題**\n",
    "\n",
    "* $i=1 \\ldots$について，$\\mathcal{F}_i$をフィルトレーションとする\n",
    "* $X_1, \\dots, X_n$をベルヌーイ確率変数とする\n",
    "    * ここで，$\\mathbb{P}\\left(X_i=1 \\mid \\mathcal{F}_{i-1}\\right)=P_i$であり，$P_i$は$\\mathcal{F}_{i-1}$-measurableかつ$X_i$は$\\mathcal{F}_i$-measurableである．\n",
    "  \n",
    "このとき，\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\left(\\exists n: \\sum_{t=1}^n X_t<\\sum_{t=1}^n P_t / 2-W\\right) \\leq e^{-W}\n",
    "$$\n",
    "\n",
    "が成り立つ．\n",
    "\n",
    "証明は[Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning](https://arxiv.org/abs/1703.07710)のLemma F.4参照．\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**補題: Visitation Ratio**\n",
    "\n",
    "参考：\n",
    "* [Tighter Problem-Dependent Regret Bounds in Reinforcement Learning without Domain Knowledge using Value Function Bounds](https://arxiv.org/abs/1901.00210)のLemma 6\n",
    "\n",
    "$(F^N)^c$が成り立っているとき，$(s, a) \\in L_k$については，\n",
    "\n",
    "$$\n",
    "n_k(s, a) \\geq \\frac{1}{4} \\sum_{j \\leq k} w_j(s, a)\n",
    "$$\n",
    "\n",
    "が成り立つ．\n",
    "\n",
    "**証明**\n",
    "\n",
    "$(F^N)^c$が成り立っているので，次が成立します．\n",
    "$$\n",
    "\\begin{aligned}\n",
    "n_k(s, a) \\geq \\frac{1}{2} \\sum_{j<k} w_j(s, a)-H \\ln \\frac{S A H}{\\delta^{\\prime}}\n",
    "&=\\frac{1}{4} \\sum_{j<k} w_j(s, a)+\\frac{1}{4} \\sum_{j<k} w_j(s, a)-H \\ln \\frac{S A H}{\\delta^{\\prime}} \\\\\n",
    "&\\geq \\frac{1}{4} \\sum_{j<k} w_j(s, a)+H \\\\\n",
    "&\\geq \\frac{1}{4} \\sum_{j<k} w_j(s, a)+w_k(s, a) \\\\\n",
    "&\\geq \\frac{1}{4} \\sum_{j \\leq k} w_j(s, a)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "ここで2つ目の不等式は$(s, a) \\in L_k$を使ってます．\n",
    "\n",
    "---\n",
    "\n",
    "次の補題は，$(s, a) \\notin L_k$であれば，リグレットにはほぼ寄与しないことを示しています．\n",
    "\n",
    "**補題（Minimal Contribution）**\n",
    "\n",
    "参考：\n",
    "* [Tighter Problem-Dependent Regret Bounds in Reinforcement Learning without Domain Knowledge using Value Function Bounds](https://arxiv.org/abs/1901.00210)のLemma 7\n",
    "\n",
    "次が成立します．\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^K \\sum_{t=1}^H \\sum_{(s, a) \\notin L_k} w_{t k}(s, a) \\leq \\tilde{\\mathcal{O}}(SAH)\n",
    "$$\n",
    "\n",
    "**証明**\n",
    "\n",
    "定義より，$(s, a) \\notin L_k$であれば，\n",
    "\n",
    "$$\n",
    "\\frac{1}{4} \\sum_{j \\leq k} w_j(s, a)<H \\ln \\frac{S A H}{\\delta^{\\prime}}+H\n",
    "$$\n",
    "\n",
    "が成立します．よって，\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^K \\sum_{t=1}^H \\sum_{(s, a) \\notin L_k} w_{t k}(s, a)=\\sum_{s, a} \\sum_{k=1}^K w_k(s, a) 1\\left\\{(s, a) \\notin L_k\\right\\} \\leq \\sum_{s, a}\\left(4 H \\ln \\frac{S A H}{\\delta^{\\prime}}+4 H\\right) \\leq \\tilde{\\mathcal{O}}(SAH)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**補題（Visitation Ratio）**\n",
    "\n",
    "\n",
    "参考：\n",
    "* [Tighter Problem-Dependent Regret Bounds in Reinforcement Learning without Domain Knowledge using Value Function Bounds](https://arxiv.org/abs/1901.00210)のLemma 13\n",
    "\n",
    "$(F^N)^c$が成り立っているとき，次が成立します：\n",
    "\n",
    "$$\n",
    "{\\sum_{k=1}^K \\sum_{t=1}^H \\sum_{(s, a) \\in L_k} \\frac{w_{t k}(s, a)}{n_k(s, a)}} \\leq \\tilde{\\mathcal{O}}(\\sqrt{S A })\n",
    "$$\n",
    "\n",
    "証明は省略\n",
    "\n",
    "---\n",
    "\n",
    "**補題**\n",
    "\n",
    "参考\n",
    "* [Tight Regret Bounds for Model-Based Reinforcement Learning with Greedy Policies](https://arxiv.org/abs/1905.11527)のLemma 38\n",
    "* [Exploration Exploitation in Constrained MDPs](https://arxiv.org/abs/2003.02189)のLemma 36では$H$が$H^2$になってる．なぜ？\n",
    "* 結果自体は$\\frac{1}{\\sqrt{n}}$のバウンドであり，これは[RL_UCB_VI_regret_proof.ipynb](RL_UCB_VI_regret_proof.ipynb)では$H\\sqrt{SAK}$でバウンドされてます．もしかしたら下のやつ間違ってるかも．$H^2$のほうが正しいかもですね．\n",
    "\n",
    "$(F^N)^c$が成り立っているとき，次が成立します：\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^K \\sum_{t=1}^H \\mathbb{E}\\left[\\sqrt{\\frac{1}{n_{k-1}\\left(s_t^k, \\pi_k\\left(s_t^k\\right)\\right) \\vee 1}} \\mid \\mathcal{F}_{k-1}\\right] \\leq \\tilde{\\mathcal{O}}(\\sqrt{S A KH}+S A H)\n",
    "$$\n",
    "\n",
    "**証明**\n",
    "\n",
    "次が成立します．\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\sum_{k=1}^K \\sum_{t=1}^H \\mathbb{E}\\left[\\sqrt{\\frac{1}{n_{k-1}\\left(s_t^k, \\pi_k\\left(s_t^k\\right)\\right) \\vee 1}} \\mid \\mathcal{F}_{k-1}\\right] \\\\\n",
    "& =\\sum_{k=1}^K \\sum_{t=1}^H \\sum_{s, a} w_{t k}(s, a) \\sqrt{\\frac{1}{n_{k-1}(s, a) \\vee 1}} \\\\\n",
    "& \\leq \\sum_{k=1}^K \\sum_{t=1}^H \\sum_{s, a \\in L_k} w_{t k}(s, a) \\sqrt{\\frac{1}{n_{k-1}(s, a)}}+\\sum_{k=1}^K \\sum_{t=1}^H \\sum_{s, a \\notin \\in L_k} w_{t k}(s, a) \\\\\n",
    "& \\leq \\sum_{k=1}^K \\sum_{t=1}^H \\sum_{s, a \\in L_k} w_{t k}(s, a) \\sqrt{\\frac{1}{n_{k-1}(s, a)}}+S A H .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "また，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\sum_{k=1}^K \\sum_{t=1}^H \\sum_{s, a \\in L_k} w_{t k}(s, a) \\sqrt{\\frac{1}{n_{k-1}(s, a)}} \\\\\n",
    "& \\leq \\sqrt{\\sum_{k=1}^K \\sum_{t=1}^H \\sum_{s, a \\in L_k} w_{t k}(s, a)} \\sqrt{\\sum_{k=1}^K \\sum_{t=1}^H \\sum_{s, a \\in L_k} \\frac{w_{t k}(s, a)}{n_{k-1}(s, a)}} \\\\\n",
    "& \\leq \\sqrt{\\sum_{k=1}^K \\sum_{t=1}^H \\sum_{s, a} w_{t k}(s, a)} \\sqrt{\\sum_{k=1}^K \\sum_{t=1}^H \\sum_{s, a \\in L_k} \\frac{w_{t k}(s, a)}{n_{k-1}(s, a)}} \\\\\n",
    "& =\\sqrt{KH} \\sqrt{\\sum_{k=1}^K \\sum_{t=1}^H \\sum_{s, a} \\frac{w_{t k}(s, a)}{n_{k-1}(s, a)}} \\lesssim \\tilde{\\mathcal{O}}(\\sqrt{S A KH}) .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "が成立します．ここで，$\\sum_{t=1}^H \\sum_{s, a} w_{t k}(s, a)=H$を使いました．\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regret Bound\n",
    "\n",
    "---\n",
    "\n",
    "**補題：On Policy Errors for Optimistic Model**\n",
    "\n",
    "参考：\n",
    "* [Exploration-Exploitation in Constrained MDPs](https://arxiv.org/abs/2003.02189)の補題29\n",
    "\n",
    "次が成立するとします：\n",
    "\n",
    "$$\n",
    "\\left|\\widetilde{p}_h^k\\left(s^{\\prime} \\mid s, a\\right)-p_h\\left(s^{\\prime} \\mid s, a\\right)\\right| \\lesssim \\sqrt{\\frac{p_h\\left(s^{\\prime} \\mid s, a\\right)}{n_h^{k-1}(s, a) \\vee 1}}+\\frac{1}{n_h^{k-1}(s, a) \\vee 1} .\n",
    "$$\n",
    "\n",
    "および\n",
    "\n",
    "$$\n",
    "n_h^{k-1}(s, a) \\leq \\frac{1}{2} \\sum_{j<k} q_h^{\\pi_k}(s, a \\mid p)-H \\ln \\frac{S A H}{\\delta^{\\prime}}\n",
    "$$\n",
    "\n",
    "また，$\\pi_k$を$k$エピソード目の方策とします．このとき，任意の$K'\\in [K]$について，\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^{K^{\\prime}}\\left|V_1^{\\pi_k}\\left(s_1 ; l, p\\right)-V_1^{\\pi_k}\\left(s_1 ; \\widetilde{l}_k, \\widetilde{p}_k\\right)\\right| \\leq \\widetilde{\\mathcal{O}}\\left(\\sqrt{S \\mathcal{N} H^4 K}+(\\sqrt{\\mathcal{N}}+H) H^2 S A\\right)\n",
    "$$\n",
    "\n",
    "が成立します．ここで，$\\mathcal{N}:=\\max _{s, a, h}\\left|\\left\\{s^{\\prime}: p_h\\left(s^{\\prime} \\mid s, a\\right)>0\\right\\}\\right|$は次状態への遷移確率が非ゼロになる最大個数です．\n",
    "\n",
    "**証明**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\sum_{k=1}^{K^{\\prime}}\\left|V_1^{\\pi_k}\\left(s_1 ; p\\right)-V_1^{\\pi_k}\\left(s_1 ; \\widetilde{p}_k\\right)\\right| \\\\\n",
    "& =\\sum_{k=1}^{K^{\\prime}}\\left|\\mathbb{E}\\left[\\sum_{h=1}^H\\left(p_h-\\widetilde{p}_h^k\\right)\\left(\\cdot \\mid s_h, a_h\\right) {V}_{h+1}^{\\pi_k}(\\cdot; \\widetilde{p}_k) \\mid s_1, p, \\pi_k\\right]\\right| \\\\\n",
    "&\\leq \\underbrace{\\sum_{k=1}^{K^{\\prime}} \\mathbb{E}\\left[\\sum_{h=1}^H \\sum_{s^{\\prime}}\\left|\\left(p_h-\\widetilde{p}_h^k\\right)\\left(s^{\\prime} \\mid s_h, a_h\\right)\\right|\\left|{V}_{h+1}^{\\pi_k}\\left(s^{\\prime} ; \\widetilde{p}_k\\right)\\right| \\mid s_1, p, \\pi_k\\right]}_{(i)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "最後の(i)をバウンドしましょう．\n",
    "$V^\\pi$の部分は$H$以下なので外に出し，仮定を代入すると，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(i) & \\lesssim H \\sum_{k=1}^{K^{\\prime}} \\sum_{h=1}^H \\mathbb{E}\\left[\\sqrt{\\frac{1}{n_h^k\\left(s_h, a_h\\right) \\vee 1}} \\sum_{s^{\\prime}} \\sqrt{p_h\\left(s^{\\prime} \\mid s_h, a_h\\right)}+\\frac{S}{n_h^k\\left(s_h, a_h\\right) \\vee 1} \\mid s_1, p, \\pi_k\\right] \\\\\n",
    "& \\leq H \\sum_{k=1}^{K^{\\prime}} \\sum_{h=1}^H \\mathbb{E}\\left[\\sqrt{\\frac{1}{n_h^k\\left(s_h, a_h\\right) \\vee 1}} \\sqrt{\\mathcal{N}}\\sqrt{\\sum_{s^{\\prime}} p_h\\left(s^{\\prime} \\mid s_h, a_h\\right)}+\\frac{S}{n_h^k\\left(s_h, a_h\\right) \\vee 1} \\mid s_1, p, \\pi_k\\right] \\\\\n",
    "& =H \\sum_{k=1}^{K^{\\prime}} \\sum_{h=1}^H \\mathbb{E}\\left[\\sqrt{\\frac{1}{n_h^k\\left(s_h, a_h\\right) \\vee 1}} \\sqrt{\\mathcal{N}}+\\frac{S}{n_h^k\\left(s_h, a_h\\right) \\vee 1} \\mid s_1, p, \\pi_k\\right] \\\\\n",
    "& =H \\sum_{k=1}^{K^{\\prime}} \\sum_{h=1}^H \\mathbb{E}\\left[\\sqrt{\\frac{1}{n_h^k\\left(s_h^k, a_h^k\\right) \\vee 1}} \\sqrt{\\mathcal{N}}+\\frac{S}{n_h^k\\left(s_h^k, a_h^k\\right) \\vee 1} \\mid \\mathcal{F}_{k-1}\\right] \\\\\n",
    "& \\lesssim \\sqrt{S \\mathcal{N} H^4 K}+\\sqrt{\\mathcal{N}} H^2 S A+S H^3 A \\leq \\widetilde{\\mathcal{O}}\\left(\\sqrt{S \\mathcal{N} H^4 K}+(\\sqrt{\\mathcal{N}}+H) H^2 S A\\right) .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* １行目は$p$についての仮定を代入してます．\n",
    "* ２行目はJensenの不等式です．\n",
    "* ５行目はGood setの節でやった補題を使っています．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 連続MDP\n",
    "\n",
    "---\n",
    "\n",
    "**補題: Lipschitz MDPでは価値関数もLipschitz**\n",
    "\n",
    "参考：\n",
    "* [Kernel-Based Reinforcement Learning: A Finite-Time Analysis](https://arxiv.org/abs/2004.05599)\n",
    "\n",
    "状態空間$\\mathcal{X}$と行動空間$\\mathcal{A}$について，$\\rho$を$\\rho\\left[(x, a),\\left(x^{\\prime}, a^{\\prime}\\right)\\right]=\\rho_{\\mathcal{X}}\\left(x, x^{\\prime}\\right)+\\rho_{\\mathcal{A}}(a, a')$であるような測度とします．\n",
    "次を仮定します：\n",
    "* 報酬関数が$\\lambda_r$-Lipshitzです．つまり，$\\left|r_h(x, a)-r_h\\left(x^{\\prime}, a^{\\prime}\\right)\\right| \\leq \\lambda_r \\rho\\left[(x, a),\\left(x^{\\prime}, a^{\\prime}\\right)\\right]$です．\n",
    "* 遷移確率は1-WassersteinについてLipschitzです．つまり，$W_1\\left(P_h(\\cdot \\mid x, a), P_h\\left(\\cdot \\mid x^{\\prime}, a^{\\prime}\\right)\\right) \\leq \\lambda_p \\rho\\left[(x, a),\\left(x^{\\prime}, a^{\\prime}\\right)\\right]$です．\n",
    "\n",
    "このとき，次が成立します．\n",
    "\n",
    "$$\\forall\\left(x, a, x^{\\prime}, a^{\\prime}\\right), \\forall h \\in[H], \\quad\\left|Q_h^*(x, a)-Q_h^*\\left(x^{\\prime}, a^{\\prime}\\right)\\right| \\leq L_h \\rho\\left[(x, a),\\left(x^{\\prime}, a^{\\prime}\\right)\\right]$$\n",
    "\n",
    "ここで，$L_h \\stackrel{\\text { def }}{=} \\sum_{h^{\\prime}=h}^H \\lambda_r \\lambda_p^{H-h^{\\prime}}$としました．\n",
    "\n",
    "**証明**\n",
    "\n",
    "帰納法で示します．$h=H$なら明らかに成立します．\n",
    "続いて，$h+1$のときに成立するか考えましょう．\n",
    "\n",
    "まず，$V_{h+1}^*(x)$はLipschitzになります．\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V_{h+1}^*(x)-V_{h+1}^*\\left(x^{\\prime}\\right) & =\\max _a Q_{h+1}^*(x, a)-\\max _a Q_{h+1}^*\\left(x^{\\prime}, a\\right) \\leq \\max _a\\left(Q_{h+1}^*(x, a)-Q_{h+1}^*\\left(x^{\\prime}, a\\right)\\right) \\\\\n",
    "& \\leq \\max _a \\sum_{h^{\\prime}=h+1}^H \\lambda_r \\lambda_p^{H-h^{\\prime}} \\rho\\left[(x, a),\\left(x^{\\prime}, a\\right)\\right]=\\sum_{h^{\\prime}=h+1}^H \\lambda_r \\lambda_p^{H-h^{\\prime}} \\rho_{\\mathcal{X}}\\left(x, x^{\\prime}\\right),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "最後の部分では$\\rho\\left[(x, a),\\left(x^{\\prime}, a^{\\prime}\\right)\\right]=\\rho_{\\mathcal{X}}\\left(x, x^{\\prime}\\right)+\\rho_{\\mathcal{A}}\\left(a, a^{\\prime}\\right)$であることを使っています．\n",
    "$x$と$x'$を逆にして同じことを示せば，\n",
    "\n",
    "$$\n",
    "\\left|V_{h+1}^*(x)-V_{h+1}^*\\left(x^{\\prime}\\right)\\right| \\leq \\sum_{h^{\\prime}=h+1}^H \\lambda_r \\lambda_p^{H-h^{\\prime}} \\rho_{\\mathcal{X}}\\left(x, x^{\\prime}\\right) .\n",
    "$$\n",
    "\n",
    "であることがわかります．\n",
    "最後に，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q_h^*(x, a)-Q_h^*\\left(x^{\\prime}, a^{\\prime}\\right) & \\leq \\lambda_r \\rho\\left[(x, a),\\left(x^{\\prime}, a^{\\prime}\\right)\\right]+\\int_{\\mathcal{X}} V_{h+1}^*(y)\\left(P_h(\\mathrm{~d} y \\mid x, a)-P_h\\left(\\mathrm{~d} y \\mid x^{\\prime}, a^{\\prime}\\right)\\right) \\\\\n",
    "& \\leq \\lambda_r \\rho\\left[(x, a),\\left(x^{\\prime}, a^{\\prime}\\right)\\right]+L_{h+1} \\int_{\\mathcal{X}} \\frac{V_{h+1}^*(y)}{L_{h+1}}\\left(P_h(\\mathrm{~d} y \\mid x, a)-P_h\\left(\\mathrm{~d} y \\mid x^{\\prime}, a^{\\prime}\\right)\\right) \\\\\n",
    "& \\leq\\left[\\lambda_r+\\lambda_p \\sum_{h^{\\prime}=h+1}^H \\lambda_r \\lambda_p^{H-h^{\\prime}}\\right] \\rho\\left[(x, a),\\left(x^{\\prime}, a^{\\prime}\\right)\\right]=\\sum_{h^{\\prime}=h}^H \\lambda_r \\lambda_p^{H-h^{\\prime}} \\rho\\left[(x, a),\\left(x^{\\prime}, a^{\\prime}\\right)\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "が成立します．ここで，$V_{h+1}^* / L_{h+1}$が$1$-Lipschitzであることと，Wasserstein距離の定義を使いました．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**補題：Covering Numberを使った２つの関数の大小の確率バウンド**\n",
    "\n",
    "参考：\n",
    "* [Kernel-Based Reinforcement Learning: A Finite-Time Analysis](https://arxiv.org/abs/2004.05599)のLemma 6\n",
    "\n",
    "$\\mathcal{N}(\\epsilon, \\mathcal{X} \\times \\mathcal{A}, \\rho)$を測度空間$(\\mathcal{X}\\times \\mathcal{A}, \\rho)$についての$\\epsilon$-covering numberとします．\n",
    "\n",
    "$(\\Omega, \\mathcal{T}, \\mathbb{P})$の確率空間を考えます．\n",
    "$F$, $G$を$\\mathcal{X} \\times \\mathcal{A} \\times \\Omega$ to $\\mathbb{R}$なる関数とし，\n",
    "$\\omega \\rightarrow F(x, a, \\omega)$ and $\\omega \\rightarrow G(x, a, \\omega)$を確率変数とします．\n",
    "\n",
    "また，関数$(x, a) \\rightarrow F(x, a, \\omega)$と$(x, a) \\rightarrow G(x, a, \\omega)$は，任意の$\\omega \\in \\Omega$について，それぞれ$L_F$と$L_G$-Lipschitzとします．\n",
    "\n",
    "このとき，\n",
    "\n",
    "$$\n",
    "\\forall(x, a), \\quad \\mathbb{P}[\\omega \\in \\Omega: G(x, a, \\omega) \\geq F(x, a, \\omega)] \\leq \\delta\n",
    "$$\n",
    "\n",
    "であれば，\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\left[\\omega \\in \\Omega: \\exists(x, a), G(x, a, \\omega) \\geq F(x, a, \\omega)+\\left(L_G+L_f\\right) \\epsilon\\right] \\leq \\delta \\mathcal{N}(\\epsilon, \\mathcal{X} \\times \\mathcal{A}, \\rho)\n",
    "$$\n",
    "\n",
    "が成り立ちます．\n",
    "\n",
    "証明は省略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**補題：Kernelを使った$\\frac{1}{n}$のバウンド**\n",
    "\n",
    "参考：\n",
    "* [Kernel-Based Reinforcement Learning: A Finite-Time Analysis](https://arxiv.org/abs/2004.05599)のLemma 6\n",
    "\n",
    "\n",
    "$u, v \\in \\mathcal{X} \\times \\mathcal{A}$とします．関数$g: \\mathbb{R}_{\\geq 0} \\rightarrow[0,1]$について，カーネル関数を\n",
    "\n",
    "$$\\psi_\\sigma(u, v) \\stackrel{\\text { def }}{=} g(\\rho[u, v] / \\sigma)$$\n",
    "とします．\n",
    "\n",
    "$(x, a)$ and $(s, h) \\in[K] \\times[H]$に対して，重み関数を\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& w_h^s(x, a) \\stackrel{\\text { def }}{=} \\psi_\\sigma\\left((x, a),\\left(x_h^s, a_h^s\\right)\\right) \\quad \\text { and } \\\\\n",
    "& \\widetilde{w}_h^s(x, a) \\stackrel{\\text { def }}{=} \\frac{w_h^s(x, a)}{\\beta+\\sum_{l=1}^{k-1} w_h^l(x, a)},\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "として，疑似訪問頻度を\n",
    "\n",
    "$$\n",
    "\\mathbf{C}_h^k(x, a) \\stackrel{\\text { def }}{=} \\beta+\\sum_{s=1}^{k-1} w_h^s(x, a)\n",
    "$$\n",
    "\n",
    "とします．このとき，\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^K \\sum_{h=1}^H \\frac{1}{\\mathbf{C}_h^k\\left(\\tilde{x}_h^k, \\tilde{a}_h^k\\right)} \\mathbb{I}\\left\\{\\rho\\left[\\left(\\tilde{x}_h^k, \\tilde{a}_h^k\\right),\\left(x_h^k, a_h^k\\right)\\right] \\leq 2 \\sigma\\right\\} \\lesssim H\\left|\\mathcal{C}_\\sigma\\right|\n",
    "$$\n",
    "\n",
    "および\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^K \\sum_{h=1}^H \\frac{1}{\\sqrt{\\mathbf{C}_h^k\\left(\\tilde{x}_h^k, \\tilde{a}_h^k\\right)}} \\mathbb{I}\\left\\{\\rho\\left[\\left(\\tilde{x}_h^k, \\tilde{a}_h^k\\right),\\left(x_h^k, a_h^k\\right)\\right] \\leq 2 \\sigma\\right\\} \\lesssim H\\left|\\mathcal{C}_\\sigma\\right|+H \\sqrt{\\left|\\mathcal{C}_\\sigma\\right| K}\n",
    "$$\n",
    "\n",
    "が成立します．\n",
    "\n",
    "証明は省略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 集中不等式\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**補題：重み付きのHoeffdingに便利**\n",
    "\n",
    "参考：\n",
    "* [Kernel-Based Reinforcement Learning: A Finite-Time Analysis](https://arxiv.org/abs/2004.05599)のLemma 7\n",
    "\n",
    "非負の実数列$\\left\\{z_s\\right\\}_{s=1}^t$と，関数$g: \\mathbb{R}_{+} \\rightarrow[0,1]$を考えます．ここで，$g$は次を満たすとします．\n",
    "\n",
    "====\n",
    "\n",
    "関数$g: \\mathbb{R}_{\\geq 0} \\rightarrow[0,1]$は微分可能，non-increasing，$g(4) > 0$を満たし，また，次を満たす定数$C^g_1, C^g_2$が存在します：\n",
    "\n",
    "$$\n",
    "g(z) \\leq C_1^g \\exp \\left(-z^2 / 2\\right) \\text { and } \\sup _z\\left|g^{\\prime}(z)\\right| \\leq C_2^g\n",
    "$$\n",
    "\n",
    "例えば$g(z)=\\exp \\left(-z^2 / 2\\right)$のようなガウスカーネルが仮定を満たしています．\n",
    "\n",
    "====\n",
    "\n",
    "$\\beta > 0$について，\n",
    "\n",
    "$$\n",
    "w_s \\stackrel{\\text { def }}{=} g\\left(\\frac{z_s}{\\sigma}\\right) \\text { and } \\widetilde{w}_s \\stackrel{\\text { def }}{=} \\frac{w_s}{\\beta+\\sum_{s^{\\prime}=1}^t w_{s^{\\prime}}} \\text {. }\n",
    "$$\n",
    "\n",
    "のとき，$t\\geq 1$について，\n",
    "\n",
    "$$\n",
    "\\sum_{s=1}^t \\widetilde{w}_s z_s \\leq 2 \\sigma\\left(1+\\sqrt{\\log \\left(C_1^g t / \\beta+e\\right)}\\right)\n",
    "$$\n",
    "\n",
    "が成立します．\n",
    "\n",
    "証明は省略．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**一様な集中不等式（無限のエピソードについてのUnion boundを取るときに便利です）**\n",
    "\n",
    "参考：\n",
    "* [Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning](https://arxiv.org/abs/1703.07710)のLemma F.1\n",
    "\n",
    "準備：\n",
    "\n",
    "* $\\operatorname{lnp}(x) =\\ln (\\ln (\\max \\{x, e\\}))$とします．\n",
    "\n",
    "$X_1, X_2, \\dots$をフィルトレーション$\\{\\mathcal{F}_t\\}_{t=1}^\\infty$についての\n",
    "$\\sigma^2$-subgaussianなマルチンゲール差分列とします．つまり，\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\left[\\exp \\left(\\lambda\\left(X_t-\\mu\\right)\\right) \\mid \\mathcal{F}_{t-1}\\right] \\leq \\exp \\left(\\lambda^2 \\sigma^2 / 2\\right)\n",
    "$$\n",
    "\n",
    "が全ての$\\lambda$についてa.s.で成立するとします．このとき，$\\hat{\\mu}_t=\\frac{1}{t} \\sum_{i=1}^t X_i$について，\n",
    "$$\n",
    "\\mathbb{P}\\left(\\exists t:\\left|\\hat{\\mu}_t-\\mu\\right| \\geq \\sqrt{\\frac{4 \\sigma^2}{t}\\left(2 \\operatorname{lnp}(t)+\\ln \\frac{3}{\\delta}\\right)}\\right) \\leq 2 \\delta\n",
    "$$\n",
    "が成り立ちます．\n",
    "\n",
    "**証明**\n",
    "\n",
    "$S_t=\\sum_{s=1}^t\\left(X_s-\\mu\\right)$とします．このとき，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\mathbb{P}\\left(\\exists t: \\hat{\\mu}_t-\\mu \\geq \\sqrt{\\frac{4 \\sigma^2}{t}\\left(2 \\operatorname{lnp}(t)+\\ln \\frac{3}{\\delta}\\right)}\\right) \\\\\n",
    "= & \\mathbb{P}\\left(\\exists t: S_t \\geq \\sqrt{4 \\sigma^2 t\\left(2 \\operatorname{lnp}(t)+\\ln \\frac{3}{\\delta}\\right)}\\right) \\\\\n",
    "\\leq & \\sum_{k=0}^{\\infty} \\mathbb{P}\\left(\\exists t \\in\\left[2^k, 2^{k+1}\\right]: S_t \\geq \\sqrt{4 \\sigma^2 t\\left(2 \\operatorname{lnp}(t)+\\ln \\frac{3}{\\delta}\\right)}\\right) \\\\\n",
    "\\leq & \\sum_{k=0}^{\\infty} \\mathbb{P}\\left(\\exists t \\leq 2^{k+1}: S_t \\geq \\sqrt{2 \\sigma^2 2^{k+1}\\left(2 \\operatorname{lnp}\\left(2^k\\right)+\\ln \\frac{3}{\\delta}\\right)}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "最初の不等式はpeeling deviceってやつです（バンディット本のLemma 9.3あたりでも使ってます）．\n",
    "$t \\geq 0$について証明するときに便利ですね．\n",
    "\n",
    "続いて，$\\lambda > 0$について$M_t=\\exp(\\lambda S_t)$とします．\n",
    "これは非負のsubmartingaleです．\n",
    "また，\n",
    "\n",
    "$$\n",
    "f=\\sqrt{2 \\sigma^2 2^{k+1}\\left(2 \\ln p\\left(2^k\\right)+\\ln \\frac{3}{\\delta}\\right)}\n",
    "$$\n",
    "\n",
    "とします．\n",
    "まず\n",
    "$$\n",
    "\\mathbb{P}\\left(\\exists t \\leq 2^{k+1}: S_t \\geq f\\right)=\\mathbb{P}\\left(\\max _{t \\leq 2^{k+1}} M_t \\geq \\exp (\\lambda f)\\right) $$\n",
    "として変形できます．さらに，DoobのMaximal inequality（バンディット本のTheorem 3.9）より，\n",
    "$$\n",
    "\\mathbb{P}\\left(\\max _{t \\leq 2^{k+1}} M_t \\geq \\exp (\\lambda f)\\right)\n",
    "\\leq \\frac{\\mathbb{E}\\left[M_{2^{k+1}}\\right]}{\\exp (\\lambda f)} \\leq \\exp \\left(2^{k+1} \\frac{\\lambda^2 \\sigma^2}{2}-\\lambda f\\right)\n",
    "$$\n",
    "\n",
    "が成り立ちます．ここで，$\\lambda=\\frac{f}{\\sigma^2 2^{k+1}}$とおけば，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{P}\\left(\\exists t \\leq 2^{k+1}: S_t \\geq f\\right) & \\leq \\exp \\left(-\\frac{f^2}{2^{k+2} \\sigma^2}\\right)\\\\\n",
    "&=\\exp \\left(-2 \\operatorname{lnp}\\left(2^k\\right)-\\ln \\frac{3}{\\delta}\\right)\\\\\n",
    "&=\\frac{\\delta}{3} \\exp \\left(-2 \\operatorname{lnp}\\left(2^k\\right)\\right) \\\\\n",
    "& =\\frac{\\delta}{3} \\exp \\left(-\\max \\left\\{0,2 \\ln \\max \\left\\{0, \\ln 2^k\\right\\}\\right\\}\\right)\\\\\n",
    "&=\\frac{\\delta}{3} \\min \\left\\{1,(k \\ln 2)^{-2}\\right\\} \\\\\n",
    "& \\leq \\frac{\\delta}{3} \\min \\left\\{1, \\frac{1}{k^2 \\ln 2}\\right\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "この結果を最初のPeeling deviceの不等式に代入して，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{P}\\left(\\exists t: \\hat{\\mu}_t-\\mu \\geq \\sqrt{\\frac{4 \\sigma^2}{t}\\left(2 \\ln \\mathrm{p}(t)+\\ln \\frac{3}{\\delta}\\right)}\\right) & \\leq \\frac{\\delta}{3} \\sum_{k=0}^{\\infty} \\min \\left\\{1, \\frac{1}{k^2 \\ln (2)}\\right\\} \\\\\n",
    "& =\\delta \\frac{1}{3}\\left(\\frac{\\pi^2}{6 \\ln 2}+2-1 / \\ln (2)\\right) \\leq \\delta\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "が得られます．\n",
    "これは片側のバウンドですが，逆側も同様にして出すことができます．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mirror Descent\n",
    "\n",
    "オンラインのMirror descentでは次の問題をときます：\n",
    "\n",
    "$$\n",
    "x_{k+1} \\in \\arg \\min _{x \\in C} t_K\\left\\langle g_k, x-x_k\\right\\rangle+B_\\omega\\left(x, x_k\\right)\n",
    "$$\n",
    "\n",
    "ここで$t_K$はステップサイズで，$B_\\omega$はBregman divergenceです．\n",
    "特に$C$を単位単体としてKL divergenceを選択すれば，\n",
    "\n",
    "$$\n",
    "x_{k+1} \\in \\arg \\min _{x \\in C}\\left\\{t_K\\left\\langle\\nabla f_k\\left(x_k\\right), x-x_k\\right\\rangle+d_{K L}\\left(x \\| x_k\\right)\\right\\}\n",
    "$$\n",
    "\n",
    "と同じです．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**補題：オンラインMirror descentの基本の不等式**\n",
    "\n",
    "参考：\n",
    "* [Exploration Exploitation in Constrained MDPs](https://arxiv.org/abs/2003.02189)のLemma 39\n",
    "\n",
    "\n",
    "準備：\n",
    "* $g_{k, i} \\geq 0$が$k=1, \\dots, K$と$i=1, \\dots, d$で成立．これが勾配に相当．\n",
    "* $C=\\Delta_d$\n",
    "* 初期化は一様に$x_1=[1 / d, \\ldots, 1 / d]$とする\n",
    "* 学習率$t_K$\n",
    "\n",
    "このとき，任意の$u \\in \\Delta_d$について，\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^K\\left\\langle g_t, x_k-u\\right\\rangle \\leq \\frac{\\log d}{t_K}+\\frac{t_K}{2} \\sum_{k=1}^K \\sum_{i=1}^d x_{k, i} g_{k, i}^2\n",
    "$$\n",
    "\n",
    "が成立する．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**補題：オンラインのMirror descentをするときに便利**\n",
    "\n",
    "参考：\n",
    "* [Exploration Exploitation in Constrained MDPs](https://arxiv.org/abs/2003.02189)\n",
    "\n",
    "準備：\n",
    "* $t_K > 0$\n",
    "* $\\pi_h^1(\\cdot \\mid s)$：任意の$h$と$s$について一様分布．\n",
    "* $Q_h^k(s, a) \\in[0, M]$が任意の$k$と$h$で成立\n",
    "\n",
    "このとき，\n",
    "\n",
    "$$\n",
    "\\pi_h^{k+1}(\\cdot \\mid s) \\in \\arg \\min _{\\pi \\in \\Delta_A} t_K\\left\\langle Q_h^k(s, \\cdot), \\pi-x_h^k(\\cdot \\mid s)\\right\\rangle+d_{K L}\\left(\\pi \\| \\pi_h^k(\\cdot \\mid s)\\right) .\n",
    "$$\n",
    "\n",
    "に従ってMirror descentを実行すると，任意の$k \\in [K], h\\in[H], s \\in \\mathcal{S}$について，任意の$\\pi$について次が成立する．\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^K\\left\\langle Q_h^k(\\cdot \\mid s), \\pi_h^k(\\cdot \\mid s)-\\pi_h(\\cdot \\mid s)\\right\\rangle \\leq \\frac{\\log A}{t_K}+\\frac{t_K M^2 K}{2}\n",
    "$$\n",
    "\n",
    "**証明**\n",
    "\n",
    "**補題：オンラインMirror descentの基本の不等式**を，$g_k=Q^k_h(s, \\cdot)$と$x_k=\\pi^k_h(s, \\cdot)$として適用すれば成立する．\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**補題：Mirror descentをKLで抑える**\n",
    "\n",
    "$X \\subseteq \\Delta(A)$を凸集合として，$g$を$\\mathbb{R}^{|A|}$上の非負のベクトルとします．\n",
    "$x'=\\operatorname{argmin}_{\\bar{x} \\in X}\\langle\\bar{x}, g\\rangle+\\frac{1}{\\eta} \\mathrm{KL}(\\bar{x}, x)$\n",
    "であれば，任意の$x^\\star \\in X$について，\n",
    "\n",
    "$$\n",
    "\\left\\langle x^{\\prime}-x^{\\star}, g\\right\\rangle \\leq \\frac{\\mathrm{KL}\\left(x^{\\star}, x\\right)-\\mathrm{KL}\\left(x^{\\star}, x^{\\prime}\\right)}{\\eta}+\\eta \\sum_{a \\in A} x_a\\left(g_a\\right)^2 .\n",
    "$$\n",
    "\n",
    "が成立します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
