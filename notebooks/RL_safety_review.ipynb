{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 強化学習における安全性のレビュー\n",
    "\n",
    "参考：\n",
    "* [A Review of Safe Reinforcement Learning: Methods, Theory and Applications](https://arxiv.org/abs/2205.10330)\n",
    "\n",
    "強化学習の安全性にも色々あります．レビュー論文を通じて，いくつか確認してみましょう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安全性の種類\n",
    "\n",
    "「安全性」の定義も色々あり，５つほどに大別されます．\n",
    "\n",
    "1. 危険やリスク，怪我を追う可能性から保護されている状態．(CMDPなど？) \n",
    "2. 認識された危険をコントロールして，受け入れ可能なリスクレベルに達すること．\n",
    "3. 危険な状態に達しない．この辺が近いかも？[Controlled Markov Processes With Safety State Constraints](https://ieeexplore.ieee.org/document/8391697)\n",
    "4. 人間の意図に従って行動，推論，汎化する場合．逆強化学習とかに相当しそう[Cooperative Inverse Reinforcement Learning](https://arxiv.org/abs/1606.03137)\n",
    "5. 訪れた状態から他のどの状態にも到達できるエルゴード性の要件を満たす場合．[RL_CMDP_safe_exploration.ipynb](RL_CMDP_safe_exploration.ipynb)の内容．\n",
    "\n",
    "このような安全性について，\n",
    "\n",
    "1. どのように安全な方策を見つけることができるか？\n",
    "2. 安全な方策を見つけるためにどれくらいのデータが必要か？\n",
    "3. 安全なRLの実応用にはどのようなものが存在するのか？\n",
    "   1. 5ページの真ん中らへん参照\n",
    "4. 安全性のベンチマークにはどのようなものが存在するか？\n",
    "   1. 5ページの下辺り参照\n",
    "5. 安全なRLの研究のopen problemは何か？\n",
    "\n",
    "について，レビュー論文では回答しています．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
