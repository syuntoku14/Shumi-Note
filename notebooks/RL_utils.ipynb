{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 強化学習の実験に便利なコード\n",
    "\n",
    "タイトルの通りです。よく使う関数をまとめます。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 割引無限ホライゾン"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### マルコフ決定過程の生成\n",
    "\n",
    "[強化学習の青本](https://amzn.asia/d/2epmlxT)に従ったMDPの定義用のコードです。\n",
    "MDPを次で定義します。\n",
    "\n",
    "1. 有限状態集合: $S=\\{1, \\dots, |S|\\}$\n",
    "2. 有限行動集合: $A=\\{1, \\dots, |A|\\}$\n",
    "3. 遷移確率行列: $P\\in \\mathbb{R}^{SA\\times S}$\n",
    "4. 報酬行列: $r\\in \\mathbb{R}^{S\\times A}$\n",
    "5. 割引率: $\\gamma \\in [0, 1)$\n",
    "6. 初期状態: $\\mu \\in \\mathbb{R}^{S}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状態数： 10\n",
      "行動数： 3\n",
      "割引率： 0.8\n",
      "ホライゾン： 5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax.random import PRNGKey\n",
    "import jax\n",
    "from typing import NamedTuple, Optional\n",
    "\n",
    "key = PRNGKey(0)\n",
    "\n",
    "S = 10  # 状態集合のサイズ\n",
    "A = 3  # 行動集合のサイズ\n",
    "S_set = jnp.arange(S)  # 状態集合\n",
    "A_set = jnp.arange(A)  # 行動集合\n",
    "gamma = 0.8  # 割引率\n",
    "\n",
    "\n",
    "# 報酬行列を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "rew = jax.random.uniform(key=key, shape=(S, A))\n",
    "assert rew.shape == (S, A)\n",
    "\n",
    "\n",
    "# 遷移確率行列を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "P = jax.random.uniform(key=key, shape=(S*A, S))\n",
    "P = P / jnp.sum(P, axis=-1, keepdims=True)  # 正規化して確率にします\n",
    "P = P.reshape(S, A, S)\n",
    "np.testing.assert_allclose(P.sum(axis=-1), 1, atol=1e-6)  # ちゃんと確率行列になっているか確認します\n",
    "\n",
    "\n",
    "# 初期状態分布を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "mu = jax.random.uniform(key, shape=(S,))\n",
    "mu = mu / jnp.sum(mu)\n",
    "np.testing.assert_allclose(mu.sum(axis=-1), 1, atol=1e-6)  # ちゃんと確率行列になっているか確認します\n",
    "\n",
    "\n",
    "# 状態集合, 行動集合, 割引率, 報酬行列, 遷移確率行列が準備できたのでMDPのクラスを作ります\n",
    "\n",
    "class MDP(NamedTuple):\n",
    "    S_set: jnp.array  # 状態集合\n",
    "    A_set: jnp.array  # 行動集合\n",
    "    gamma: float  # 割引率\n",
    "    H: int  # エフェクティブホライゾン\n",
    "    rew: jnp.array  # 報酬行列\n",
    "    P: jnp.array  # 遷移確率行列\n",
    "    mu: jnp.array  # 初期分布\n",
    "    optimal_Q: Optional[jnp.ndarray] = None  # 最適Q値\n",
    "\n",
    "    @property\n",
    "    def S(self) -> int:  # 状態空間のサイズ\n",
    "        return len(self.S_set)\n",
    "\n",
    "    @property\n",
    "    def A(self) -> int:  # 行動空間のサイズ\n",
    "        return len(self.A_set)\n",
    "\n",
    "\n",
    "H = int(1 / (1 - gamma))\n",
    "mdp = MDP(S_set, A_set, gamma, H, rew, P, mu)\n",
    "\n",
    "print(\"状態数：\", mdp.S)\n",
    "print(\"行動数：\", mdp.A)\n",
    "print(\"割引率：\", mdp.gamma)\n",
    "print(\"ホライゾン：\", mdp.H)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 動的計画法\n",
    "\n",
    "参考\n",
    "\n",
    "* [Safe Policy Iteration](http://proceedings.mlr.press/v28/pirotta13.pdf)の２ページ目\n",
    "* [Reinforcement Learning via Fenchel-Rockafellar Duality](https://arxiv.org/abs/2001.01866)：割引訪問頻度の変形は参考になるかも\n",
    "\n",
    "**表記**\n",
    "\n",
    "* 内積の記法: $f_1, f_2 \\in \\mathbb{R}^{S\\times A}$に対して、$\\langle f_1, f_2 \\rangle = (\\sum_{a\\in A} f_1(s, a)f_2(s, a))_s \\in \\mathbb{R}^S$とします。これは方策についての和を省略するときなどに便利です。例えば$\\langle \\pi, q_\\pi\\rangle = v_\\pi$です。\n",
    "* 方策行列（$\\Pi^\\pi \\in \\mathbb{R}^{S\\times SA}$）：$\\langle \\pi, q\\rangle$を行列で書きたいときに便利。\n",
    "    * $\\Pi^\\pi(s,(s, a))=\\pi(a \\mid s)$ \n",
    "    * $\\Pi^\\pi q^\\pi = \\langle \\pi, q^\\pi \\rangle = v^\\pi$が成立。\n",
    "* 遷移確率行列１（$P^\\pi \\in \\mathbb{R}^{SA\\times SA}$）: 次の状態についての方策の情報を追加したやつ。\n",
    "    * $P^\\pi = P \\Pi^\\pi$\n",
    "    * Q値を使ったベルマン期待作用素とかで便利。$q^\\pi = r + \\gamma P^\\pi q^\\pi$が成立。\n",
    "    * $(I - \\gamma P^\\pi)^{-1}r = q^\\pi$が成立する。\n",
    "* 遷移確率行列２（$\\bar{P}^\\pi \\in \\mathbb{R}^{S\\times S}$）: 方策$\\pi$のもとでの状態遷移の行列。\n",
    "    * $\\bar{P}^\\pi = \\Pi^\\pi P$\n",
    "    * V値を使ったベルマン期待作用素とかで便利。$v^\\pi = \\Pi^\\pi r + \\gamma \\bar{P}^\\pi v^\\pi$。\n",
    "    * $(I - \\gamma \\bar{P}^\\pi)^{-1}\\Pi^\\pi r = v^\\pi$が成立する。\n",
    "* 割引訪問頻度１（$d^\\pi_\\mu \\in \\mathbb{R}^{SA}$）：S, Aについての割引累積訪問頻度\n",
    "    * ${d}^\\pi_\\mu (s, a) = \\pi(a|s) \\sum_{s_0} \\mu(s_0) \\sum_{t=0}^\\infty \\mathrm{Pr}\\left(S_t=s|S_0=s_0, M(\\pi)\\right)$がで定義される。\n",
    "    * $d^\\pi_\\mu = \\mu \\Pi^\\pi (I - \\gamma P^\\pi)^{-1} = \\mu (I - \\gamma \\bar{P}^\\pi)^{-1} \\Pi^\\pi$が成立。\n",
    "    * $d^\\pi_\\mu = \\mu \\Pi^\\pi + \\gamma d^\\pi_\\mu P^\\pi$が成立。動的計画法のように解ける。\n",
    "* 割引訪問頻度２（$\\bar{d}^\\pi_\\mu \\in \\mathbb{R}^{S}$）：Sについての割引累積訪問頻度\n",
    "    * $\\bar{d}^\\pi_\\mu (s) = \\sum_{s_0} \\mu(s_0) \\sum_{t=0}^\\infty \\mathrm{Pr}\\left(S_t=s|S_0=s_0, M(\\pi)\\right)$で定義される。\n",
    "    * $\\bar{d}^\\pi_\\mu = \\mu (I - \\gamma \\bar{P}^\\pi)^{-1}$が成立。\n",
    "    * $\\bar{d}^\\pi_\\mu = \\mu + \\gamma \\bar{d}^\\pi_\\mu \\bar{P}^\\pi$が成立。動的計画法のように解ける。\n",
    "\n",
    "\n",
    "**実装した関数**\n",
    "\n",
    "* ``compute_greedy_policy``: Q関数 ($S \\times A \\to \\mathcal{R}$) の貪欲方策を返します\n",
    "* ``compute_optimal_Q``: MDPの最適Q関数 $q_* : S \\times A \\to \\mathcal{R}$ を返します。\n",
    "* ``compute_policy_Q``: 方策 $\\pi$ のQ関数 $q_\\pi : S \\times A \\to \\mathcal{R}$ を返します。\n",
    "* ``compute_policy_matrix``: 方策$\\pi$の行列${\\Pi}^{\\pi}$を返します。\n",
    "* ``compute_policy_visit_sa``: 方策 $\\pi$ の割引訪問頻度１${d}^\\pi_\\mu \\in \\mathbb{R}^{S\\times A}$ を返します。\n",
    "* ``compute_policy_visit_s``: 方策 $\\pi$ の割引訪問頻度２$\\bar{d}^\\pi_\\mu \\in \\mathbb{R}^{S}$ を返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qベースの動的計画法と逆行列の解の差： 4.7683716e-07\n",
      "Vベースの動的計画法と逆行列の解の差： 4.7683716e-07\n",
      "動的計画法で計算した割引訪問頻度と逆行列の解の差 1.1920929e-07\n",
      "割引訪問頻度で計算した期待リターンと動的計画法の解の差 0.0\n",
      "動的計画法で計算した割引訪問頻度と逆行列の解の差 1.1920929e-07\n",
      "割引訪問頻度で計算した期待リターンと動的計画法の解の差 0.0\n",
      "SAについての割引訪問頻度の求め方２つの差： 5.9604645e-08\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import chex\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_greedy_policy(Q: jnp.ndarray):\n",
    "    \"\"\"Q関数の貪欲方策を返します\n",
    "\n",
    "    Args:\n",
    "        Q (jnp.ndarray): (SxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        greedy_policy (jnp.ndarray): (SxA)の行列\n",
    "    \"\"\"\n",
    "    greedy_policy = jnp.zeros_like(Q)\n",
    "    S, A = Q.shape\n",
    "    greedy_policy = greedy_policy.at[jnp.arange(S), Q.argmax(axis=1)].set(1)\n",
    "    assert greedy_policy.shape == (S, A)\n",
    "    return greedy_policy\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"S\", \"A\"))\n",
    "def _compute_optimal_Q(mdp: MDP, S: int, A: int):\n",
    "    \"\"\"MDPについて、ベルマン最適作用素を複数回走らせて最適価値関数を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "\n",
    "    Returns:\n",
    "        optimal_Q (jnp.ndarray): (SxA)の行列\n",
    "    \"\"\"\n",
    "\n",
    "    def backup(optimal_Q):\n",
    "        next_v = mdp.P @ optimal_Q.max(axis=1)\n",
    "        assert next_v.shape == (S, A)\n",
    "        return mdp.rew + mdp.gamma * next_v\n",
    "    \n",
    "    optimal_Q = jnp.zeros((S, A))\n",
    "    body_fn = lambda i, Q: backup(Q)\n",
    "    return jax.lax.fori_loop(0, mdp.H + 100, body_fn, optimal_Q)\n",
    "\n",
    "compute_optimal_Q = lambda mdp: _compute_optimal_Q(mdp, mdp.S, mdp.A)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_Q(mdp: MDP, policy: jnp.ndarray):\n",
    "    \"\"\"MDPと方策について、ベルマン期待作用素を複数回走らせて価値関数を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "        policy (jnp.ndarray): (SxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        optimal_Q (jnp.ndarray): (SxA)の行列\n",
    "    \"\"\"\n",
    "    S, A = policy.shape\n",
    "    chex.assert_shape(policy, (mdp.S, mdp.A))\n",
    "\n",
    "    def backup(policy_Q):\n",
    "        max_Q = (policy * policy_Q).sum(axis=1)\n",
    "        next_v = mdp.P @ max_Q\n",
    "        assert next_v.shape == (S, A)\n",
    "        return mdp.rew + mdp.gamma * next_v\n",
    "    \n",
    "    policy_Q = jnp.zeros((S, A))\n",
    "    body_fn = lambda i, Q: backup(Q)\n",
    "    return jax.lax.fori_loop(0, mdp.H + 100, body_fn, policy_Q)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_matrix(policy: jnp.ndarray):\n",
    "    \"\"\"\n",
    "    上で定義した方策行列を計算します。方策についての内積が取りたいときに便利です。\n",
    "    Args:\n",
    "        policy (jnp.ndarray): (SxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        policy_matrix (jnp.ndarray): (SxSA)の行列\n",
    "    \"\"\"\n",
    "    S, A = policy.shape\n",
    "    PI = policy.reshape(1, S, A)\n",
    "    PI = jnp.tile(PI, (S, 1, 1))\n",
    "    eyes = jnp.eye(S).reshape(S, S, 1)\n",
    "    PI = (eyes * PI).reshape(S, S*A)\n",
    "    return PI\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_visit_sa(mdp: MDP, policy: jnp.ndarray, init_dist: jnp.ndarray):\n",
    "    \"\"\"MDPと方策について、割引訪問頻度１を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "        policy (jnp.ndarray): (SxA)の行列\n",
    "        init_dist (jnp.ndarray): (S) 初期状態の分布\n",
    "\n",
    "    Returns:\n",
    "        visit (jnp.ndarray): (SxA)の行列\n",
    "    \"\"\"\n",
    "    S, A = policy.shape\n",
    "    chex.assert_shape(policy, (mdp.S, mdp.A))\n",
    "    Pi = compute_policy_matrix(policy)\n",
    "    PPi = mdp.P.reshape(S*A, S) @ Pi \n",
    "\n",
    "    def backup(visit):\n",
    "        next_visit = mdp.gamma * visit @ PPi\n",
    "        return init_dist @ Pi + next_visit\n",
    "    \n",
    "    body_fn = lambda i, visit: backup(visit)\n",
    "    visit = jnp.zeros(S*A)\n",
    "    visit = jax.lax.fori_loop(0, mdp.H + 100, body_fn, visit)\n",
    "    visit = visit.reshape(S, A)\n",
    "    return visit\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_visit_s(mdp: MDP, policy: jnp.ndarray, init_dist: jnp.ndarray):\n",
    "    \"\"\"MDPと方策について、割引訪問頻度２を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "        policy (jnp.ndarray): (SxA)の行列\n",
    "        init_dist (jnp.ndarray): (S) 初期状態の分布\n",
    "\n",
    "    Returns:\n",
    "        visit (jnp.ndarray): (S)のベクトル\n",
    "    \"\"\"\n",
    "    S, A = policy.shape\n",
    "    chex.assert_shape(policy, (mdp.S, mdp.A))\n",
    "    Pi = compute_policy_matrix(policy)\n",
    "    PiP = Pi @ mdp.P.reshape(S*A, S) \n",
    "\n",
    "    def backup(visit):\n",
    "        next_visit = mdp.gamma * visit @ PiP\n",
    "        return init_dist + next_visit\n",
    "    \n",
    "    body_fn = lambda i, visit: backup(visit)\n",
    "    visit = jnp.zeros(S)\n",
    "    visit = jax.lax.fori_loop(0, mdp.H + 100, body_fn, visit)\n",
    "    return visit\n",
    "\n",
    "\n",
    "# 動的計画法による最適価値関数\n",
    "optimal_Q_DP = compute_optimal_Q(mdp)\n",
    "optimal_V_DP = optimal_Q_DP.max(axis=1)\n",
    "optimal_policy = compute_greedy_policy(optimal_Q_DP)\n",
    "mdp = mdp._replace(optimal_Q=optimal_Q_DP)\n",
    "\n",
    "\n",
    "# 逆行列による解法 Q\n",
    "Pi = compute_policy_matrix(optimal_policy)\n",
    "PPi = mdp.P.reshape(S*A, S) @ Pi\n",
    "optimal_Q_inv = jnp.linalg.inv(jnp.eye(S*A) - mdp.gamma * PPi) @ mdp.rew.reshape(S*A)\n",
    "print(\"Qベースの動的計画法と逆行列の解の差：\", jnp.abs(optimal_Q_inv - optimal_Q_DP.reshape(-1)).max())\n",
    "\n",
    "\n",
    "# 逆行列による解法 V\n",
    "Pi = compute_policy_matrix(optimal_policy)\n",
    "PiP = Pi @ mdp.P.reshape(S*A, S) \n",
    "Pirew = Pi @ mdp.rew.reshape(S*A)\n",
    "optimal_V_inv = jnp.linalg.inv(jnp.eye(S) - mdp.gamma * PiP) @ Pirew\n",
    "print(\"Vベースの動的計画法と逆行列の解の差：\", jnp.abs(optimal_V_inv - optimal_V_DP.reshape(-1)).max())\n",
    "\n",
    "\n",
    "# 割引訪問頻度の計算１\n",
    "d_pi_DP = compute_policy_visit_sa(mdp, optimal_policy, mdp.mu).reshape(-1)\n",
    "d_pi_inv = (mdp.mu @ jnp.linalg.inv(jnp.eye(S) - mdp.gamma * PiP) @ Pi)\n",
    "print(\"動的計画法で計算した割引訪問頻度と逆行列の解の差\", jnp.abs(d_pi_DP - d_pi_inv).max())\n",
    "optimal_return_DP = mdp.mu @ optimal_V_DP\n",
    "optimal_return_visit = Pi @ d_pi_inv @ Pirew\n",
    "print(\"割引訪問頻度で計算した期待リターンと動的計画法の解の差\", jnp.abs(optimal_return_DP - optimal_return_visit).max())\n",
    "\n",
    "\n",
    "\n",
    "# 割引訪問頻度の計算２\n",
    "d_pi_DP = compute_policy_visit_s(mdp, optimal_policy, mdp.mu)\n",
    "d_pi_inv = mdp.mu @ jnp.linalg.inv(jnp.eye(S) - mdp.gamma * PiP)\n",
    "print(\"動的計画法で計算した割引訪問頻度と逆行列の解の差\", jnp.abs(d_pi_DP - d_pi_inv).max())\n",
    "optimal_return_DP = mdp.mu @ optimal_V_DP\n",
    "optimal_return_visit = d_pi_inv @ Pirew\n",
    "print(\"割引訪問頻度で計算した期待リターンと動的計画法の解の差\", jnp.abs(optimal_return_DP - optimal_return_visit).max())\n",
    "\n",
    "\n",
    "d_pi_inv_SA1 = mdp.mu @ jnp.linalg.inv(jnp.eye(S) - mdp.gamma * PiP) @ Pi\n",
    "d_pi_inv_SA2 = mdp.mu @ Pi @ jnp.linalg.inv(jnp.eye(S*A) - mdp.gamma * PPi)\n",
    "\n",
    "print(\"SAについての割引訪問頻度の求め方２つの差：\", jnp.abs(d_pi_inv_SA1 - d_pi_inv_SA2).max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 強化学習用\n",
    "\n",
    "**実装した関数**\n",
    "\n",
    "* ``sample_next_state``: 状態・行動の集合$D$のそれぞれについて次状態を$N$個返します。訪問した(状態, 行動, 次状態)のカウントも返します。\n",
    "* ``collect_samples_eps_greedy``: $q\\in \\mathbb{R}^{S\\times A}$のε-貪欲方策で$N$回インタラクションしてサンプルを集めます。訪問した(状態, 行動, 次状態)のカウントも返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.random import PRNGKey\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"N\",))\n",
    "def sample_next_state(mdp: MDP, N: int, key: PRNGKey, D: jnp.array):\n",
    "    \"\"\" 遷移行列Pに従って次の状態をN個サンプルします\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "        N (int): サンプルする個数\n",
    "        key (PRNGKey)\n",
    "        D (jnp.ndarray): 状態行動対の集合 [(s1, a1), (s2, a2), ...]\n",
    "\n",
    "    Returns:\n",
    "        new_key (PRNGKey)\n",
    "        next_s_set (jnp.ndarray): (len(D) x N) の次状態の集合\n",
    "        count_SAS (jnp.ndarray): 各(状態, 行動, 次状態)のペアの出現回数を格納した(S x A x S) の行列\n",
    "    \"\"\"\n",
    "\n",
    "    # 次状態をサンプルします\n",
    "    new_key, key = jax.random.split(key)\n",
    "    keys = jax.random.split(key, num=len(D))\n",
    "    @jax.vmap\n",
    "    def choice(key, sa):\n",
    "        return jax.random.choice(key, mdp.S_set, shape=(N,), p=P[sa[0], sa[1]])\n",
    "    next_s = choice(keys, D)\n",
    "\n",
    "    # 集めたサンプルについて、(s, a, ns)が何個出たかカウントします。\n",
    "    S, A, S = mdp.P.shape\n",
    "    count_SAS = jnp.zeros((S*A, S))\n",
    "    count_D_next_S = jax.vmap(lambda next_s: jnp.bincount(next_s, minlength=S, length=S))(next_s)\n",
    "    D_ravel = jnp.ravel_multi_index(D.T, (S, A), mode=\"wrap\")\n",
    "    count_SAS = count_SAS.at[D_ravel].add(count_D_next_S)\n",
    "    return new_key, next_s, count_SAS\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "N = 20000\n",
    "D = jnp.array([(1, 2), (2, 1), (0, 0), (3, 1), (0, 0)])\n",
    "key, next_states, count_SAS = sample_next_state(mdp, N, key, D)\n",
    "assert count_SAS.sum() == N * len(D)\n",
    "assert next_states.shape == (len(D), N)\n",
    "s, a = D[0]\n",
    "P0_approx = jnp.bincount(next_states[0], minlength=S) / N\n",
    "np.testing.assert_allclose(P0_approx, mdp.P[s, a], atol=1e-2)\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"N\",))\n",
    "def collect_samples_eps_greedy(mdp: MDP, N: int, key: PRNGKey, q: jnp.array, init_s: int, epsilon: float=0.0):\n",
    "    \"\"\" MDPとインタラクションしてサンプルをN個集めます。qの貪欲方策に従って動きます。\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "        N (int): サンプルする個数\n",
    "        key (PRNGKey)\n",
    "        q (jnp.ndarray): 行動価値関数\n",
    "        init_s (int): 初期状態\n",
    "        epsilon (float): ε-貪欲のパラメータ\n",
    "\n",
    "    Returns:\n",
    "        new_key (PRNGKey)\n",
    "        sars (jnp.ndarray): (状態, 行動, 報酬, 次状態) x N の軌跡\n",
    "        count_SAS (jnp.ndarray): 各(状態, 行動, 次状態)のペアの出現回数を格納した(S x A x S) の行列\n",
    "    \"\"\"\n",
    "    chex.assert_shape(q, (mdp.S, mdp.A))\n",
    "    S, A = q.shape\n",
    "\n",
    "    def body_fn(n, args):\n",
    "        key, sars, s, count_SAS = args\n",
    "\n",
    "        # ε-貪欲方策を実行します\n",
    "        a = q[s].argmax()\n",
    "        key, key1, key2 = jax.random.split(key, num=3)\n",
    "        random_a = jax.random.choice(key1, A)\n",
    "        a = jnp.where(jax.random.uniform(key2) > epsilon, a, random_a)\n",
    "        \n",
    "        # 次状態をサンプルします\n",
    "        key, key1 = jax.random.split(key)\n",
    "        next_s = jax.random.choice(key1, mdp.S_set, p=P[s, a])\n",
    "\n",
    "        # 集めたデータを記録します\n",
    "        r = mdp.rew[s, a]\n",
    "        sars = sars.at[n].set((s, a, r, next_s))\n",
    "        count_SAS = count_SAS.at[s, a, next_s].add(1)\n",
    "        return key, sars, next_s, count_SAS\n",
    "\n",
    "    sars = jnp.zeros((N, 4))\n",
    "    count_SAS = jnp.zeros((S, A, S))\n",
    "    args = key, sars, init_s, count_SAS\n",
    "    key, sars, next_s, count_SAS = jax.lax.fori_loop(0, N, body_fn, args)\n",
    "    return key, sars, next_s, count_SAS\n",
    "\n",
    "\n",
    "key, sars, next_s, count_SAS = collect_samples_eps_greedy(mdp, N, key, mdp.optimal_Q, 0)\n",
    "assert sars.shape == (N, 4)\n",
    "assert count_SAS.sum() == N"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 有限ホライゾン"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### マルコフ決定過程の生成\n",
    "\n",
    "**参考**\n",
    "\n",
    "有限MDPの定義については[Reinforcement Learning: Theory and Algorithms](https://rltheorybook.github.io/)の1.2章を参照しています。\n",
    "有限ホライゾンの場合、遷移行列や報酬関数が各ステップで変わる設定を考えます。\n",
    "\n",
    "1. 有限状態集合: $S=\\{1, \\dots, |S|\\}$\n",
    "2. 有限行動集合: $A=\\{1, \\dots, |A|\\}$\n",
    "3. $h$ステップ目の遷移確率行列: $P_h\\in \\mathbb{R}^{SA\\times S}$\n",
    "4. $h$ステップ目の報酬行列: $r_h\\in \\mathbb{R}^{S\\times A}$\n",
    "5. ホライゾン: $H$\n",
    "6. 初期状態: $\\mu \\in \\mathbb{R}^{S}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状態数： 10\n",
      "行動数： 3\n",
      "ホライゾン： 5\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from typing import NamedTuple, Optional\n",
    "from jax.random import PRNGKey\n",
    "\n",
    "key = PRNGKey(0)\n",
    "\n",
    "S = 10  # 状態集合のサイズ\n",
    "A = 3  # 行動集合のサイズ\n",
    "S_set = jnp.arange(S)  # 状態集合\n",
    "A_set = jnp.arange(A)  # 行動集合\n",
    "H = 5  # ホライゾン\n",
    "\n",
    "# 報酬行列を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "rew = jax.random.uniform(key=key, shape=(H, S, A))\n",
    "assert rew.shape == (H, S, A)\n",
    "\n",
    "\n",
    "# 遷移確率行列を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "P = jax.random.uniform(key=key, shape=(H, S*A, S))\n",
    "P = P / jnp.sum(P, axis=-1, keepdims=True)  # 正規化して確率にします\n",
    "P = P.reshape(H, S, A, S)\n",
    "np.testing.assert_allclose(P.sum(axis=-1), 1, atol=1e-6)  # ちゃんと確率行列になっているか確認します\n",
    "\n",
    "\n",
    "# 初期状態分布を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "mu = jax.random.uniform(key, shape=(S,))\n",
    "mu = mu / jnp.sum(mu)\n",
    "np.testing.assert_allclose(mu.sum(axis=-1), 1, atol=1e-6)  # ちゃんと確率行列になっているか確認します\n",
    "\n",
    "\n",
    "# 状態集合, 行動集合, 割引率, 報酬行列, 遷移確率行列が準備できたのでMDPのクラスを作ります\n",
    "\n",
    "class MDP(NamedTuple):\n",
    "    S_set: jnp.array  # 状態集合\n",
    "    A_set: jnp.array  # 行動集合\n",
    "    H: int  # ホライゾン\n",
    "    rew: jnp.array  # 報酬行列\n",
    "    P: jnp.array  # 遷移確率行列\n",
    "    mu: jnp.array  # 初期分布\n",
    "    optimal_Q: Optional[jnp.ndarray] = None  # 最適Q値\n",
    "\n",
    "    @property\n",
    "    def S(self) -> int:  # 状態空間のサイズ\n",
    "        return len(self.S_set)\n",
    "\n",
    "    @property\n",
    "    def A(self) -> int:  # 行動空間のサイズ\n",
    "        return len(self.A_set)\n",
    "\n",
    "\n",
    "mdp = MDP(S_set, A_set, H, rew, P, mu)\n",
    "\n",
    "print(\"状態数：\", mdp.S)\n",
    "print(\"行動数：\", mdp.A)\n",
    "print(\"ホライゾン：\", mdp.H)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 動的計画法\n",
    "\n",
    "**表記**\n",
    "\n",
    "* ステップ$h$の方策行列（$\\Pi_h^\\pi \\in \\mathbb{R}^{S\\times SA}$）：$\\langle \\pi_h, q\\rangle$を行列で書きたいときに便利。\n",
    "    * $\\Pi_h^\\pi(s,(s, a))=\\pi_h(a \\mid s)$ \n",
    "    * $\\Pi_h^\\pi q_h^\\pi = \\langle \\pi, q_h^\\pi \\rangle = v_h^\\pi$が成立。\n",
    "* ステップ$h$の遷移確率行列１（$P_h^\\pi \\in \\mathbb{R}^{SA\\times SA}$）: 次の状態についての方策の情報を追加したやつ。\n",
    "    * $P_h^\\pi = P_h \\Pi_h^\\pi$\n",
    "    * Q値を使ったベルマン期待作用素とかで便利。$q_h^\\pi = r_h + P_h^\\pi q^\\pi$が成立。\n",
    "* ステップ$h$の遷移確率行列２（$\\bar{P}_h^\\pi \\in \\mathbb{R}^{S\\times S}$）: 方策$\\pi$のもとでの状態遷移の行列。\n",
    "    * $\\bar{P}_h^\\pi = \\Pi_h^\\pi P_h$\n",
    "    * V値を使ったベルマン期待作用素とかで便利。$v_h^\\pi = \\Pi^\\pi r_h + \\gamma \\bar{P}_h^\\pi v^\\pi$。\n",
    "* ステップ$h$の訪問頻度（$d^\\pi_{h,\\mu} \\in \\mathbb{R}^{SA}$）：S, Aについての累積訪問頻度\n",
    "    * ${d}^\\pi_{h,\\mu} (s, a) = \\pi(a|s) \\sum_{s_0} \\mu(s_0) \\sum_{t=0}^h \\mathrm{Pr}\\left(S_t=s|S_0=s_0, M(\\pi)\\right)$\n",
    "\n",
    "\n",
    "**実装した関数**\n",
    "\n",
    "* ``compute_greedy_policy``: Q関数 ($H\\times S \\times A \\to \\mathcal{R}$) の貪欲方策を返します\n",
    "* ``compute_optimal_Q``: MDPの最適Q関数 $q_* : H\\times S \\times A \\to \\mathcal{R}$ を返します。\n",
    "* ``compute_policy_Q``: 方策 $\\pi$ のQ関数 $q_\\pi : H\\times S \\times A \\to \\mathcal{R}$ を返します。\n",
    "* ``compute_policy_matrix``: 方策$\\pi$の行列${\\Pi}^{\\pi} : H \\times S \\times SA$を返します。\n",
    "* ``compute_policy_visit``: 方策 $\\pi$ の割引訪問頻度${d}^\\pi_{\\mu} : {H\\times S \\times A}$ を返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最適価値関数と最適方策の価値関数の差 0.0\n",
      "0ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 2.3841858e-07\n",
      "1ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 2.3841858e-07\n",
      "2ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 4.7683716e-07\n",
      "3ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 1.1920929e-07\n",
      "4ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 0.0\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import jax\n",
    "import chex\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_greedy_policy(Q: jnp.ndarray):\n",
    "    \"\"\"Q関数の貪欲方策を返します\n",
    "\n",
    "    Args:\n",
    "        Q (jnp.ndarray): (HxSxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        greedy_policy (jnp.ndarray): (HxSxA)の行列\n",
    "    \"\"\"\n",
    "    greedy_policy = jnp.zeros_like(Q)\n",
    "    H, S, A = Q.shape\n",
    "    \n",
    "    def body_fn(i, greedy_policy):\n",
    "        greedy_policy = greedy_policy.at[i, jnp.arange(S), Q[i].argmax(axis=-1)].set(1)\n",
    "        return greedy_policy\n",
    "\n",
    "    greedy_policy = jax.lax.fori_loop(0, H, body_fn, greedy_policy)\n",
    "    chex.assert_shape(greedy_policy, (H, S, A))\n",
    "    return greedy_policy\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"H\", \"S\", \"A\"))\n",
    "def _compute_optimal_Q(mdp: MDP, H: int, S: int, A: int):\n",
    "    \"\"\"ベルマン最適作用素をホライゾン回走らせて最適価値関数を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "\n",
    "    Returns:\n",
    "        optimal_Q (jnp.ndarray): (HxSxA)の行列\n",
    "    \"\"\"\n",
    "\n",
    "    def backup(i, optimal_Q):\n",
    "        h = H - i - 1\n",
    "        max_Q = optimal_Q[h+1].max(axis=1)\n",
    "        next_v = mdp.P[h] @ max_Q\n",
    "        chex.assert_shape(next_v, (S, A))\n",
    "        optimal_Q = optimal_Q.at[h].set(mdp.rew[h] + next_v)\n",
    "        return optimal_Q\n",
    "    \n",
    "    optimal_Q = jnp.zeros((H+1, S, A))\n",
    "    optimal_Q = jax.lax.fori_loop(0, mdp.H, backup, optimal_Q)\n",
    "    return optimal_Q[:-1]\n",
    "\n",
    "compute_optimal_Q = lambda mdp: _compute_optimal_Q(mdp, mdp.H, mdp.S, mdp.A)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_Q(mdp: MDP, policy: jnp.ndarray):\n",
    "    \"\"\"ベルマン期待作用素をホライゾン回走らせて価値関数を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "        policy (np.ndarray): (HxSxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        optimal_Q (jnp.ndarray): (HxSxA)の行列\n",
    "    \"\"\"\n",
    "    H, S, A = policy.shape\n",
    "\n",
    "    def backup(i, policy_Q):\n",
    "        h = H - i - 1\n",
    "        max_Q = (policy[h+1] * policy_Q[h+1]).sum(axis=1)\n",
    "        next_v = mdp.P[h] @ max_Q\n",
    "        chex.assert_shape(next_v, (S, A))\n",
    "        policy_Q = policy_Q.at[h].set(mdp.rew[h] + next_v)\n",
    "        return policy_Q\n",
    "    \n",
    "    policy_Q = jnp.zeros((H+1, S, A))\n",
    "    policy_Q = jax.lax.fori_loop(0, mdp.H, backup, policy_Q)\n",
    "    return policy_Q[:-1]\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_matrix(policy: jnp.ndarray):\n",
    "    \"\"\"\n",
    "    上で定義した方策行列を計算します。方策についての内積が取りたいときに便利です。\n",
    "    Args:\n",
    "        policy (jnp.ndarray): (HxSxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        policy_matrix (jnp.ndarray): (HxSxSA)の行列\n",
    "    \"\"\"\n",
    "    H, S, A = policy.shape\n",
    "    PI = policy.reshape(H, 1, S, A)\n",
    "    PI = jnp.tile(PI, (1, S, 1, 1))\n",
    "    eyes = jnp.tile(jnp.eye(S).reshape(1, S, S, 1), (H, 1, 1, 1))\n",
    "    PI = (eyes * PI).reshape(H, S, S*A)\n",
    "    return PI\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_visit(mdp: MDP, policy: jnp.ndarray, init_dist: jnp.ndarray):\n",
    "    \"\"\"MDPと方策について、訪問頻度を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "        policy (jnp.ndarray): (HxSxA)の行列\n",
    "        init_dist (jnp.ndarray): (S) 初期状態の分布\n",
    "\n",
    "    Returns:\n",
    "        visit (jnp.ndarray): (HxSxA)のベクトル\n",
    "    \"\"\"\n",
    "    H, S, A = policy.shape\n",
    "    Pi = compute_policy_matrix(policy)\n",
    "    P = mdp.P.reshape(H, S*A, S)\n",
    "\n",
    "    def body_fn(h, visit):\n",
    "        next_visit = visit[h] @ P[h] @ Pi[h+1]\n",
    "        visit = visit.at[h+1].set(next_visit)\n",
    "        return visit\n",
    "    \n",
    "    visit = jnp.zeros((H+1, S*A))\n",
    "    visit = visit.at[0].set((init_dist @ Pi[0]))\n",
    "    visit = jax.lax.fori_loop(0, mdp.H, body_fn, visit)\n",
    "    visit = visit[:-1].reshape(H, S, A)\n",
    "    return visit\n",
    "\n",
    "\n",
    "# 動的計画法による最適価値関数\n",
    "optimal_Q_DP = compute_optimal_Q(mdp)\n",
    "optimal_V_DP = optimal_Q_DP.max(axis=-1)\n",
    "optimal_policy = compute_greedy_policy(optimal_Q_DP)\n",
    "optimal_policy_Q_DP = compute_policy_Q(mdp, optimal_policy)\n",
    "mdp = mdp._replace(optimal_Q=optimal_Q_DP)\n",
    "print(\"最適価値関数と最適方策の価値関数の差\", jnp.abs(optimal_Q_DP - optimal_policy_Q_DP).max())\n",
    "\n",
    "# 訪問頻度によるリターンの計算\n",
    "policy_visit = compute_policy_visit(mdp, optimal_policy, mdp.mu)\n",
    "np.testing.assert_allclose(policy_visit.sum(axis=(1, 2)), 1.0, atol=1e-6)\n",
    "np.testing.assert_allclose(policy_visit[0].sum(axis=-1), mdp.mu, atol=1e-6)\n",
    "for h in range(H):\n",
    "    return_by_visit = (policy_visit * mdp.rew)[h:].sum()\n",
    "    return_by_DP = (optimal_Q_DP[h] * policy_visit[h]).sum()\n",
    "    print(f\"{h}ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差\", np.abs(return_by_visit - return_by_DP))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 強化学習用\n",
    "\n",
    "**実装した関数**\n",
    "\n",
    "有限ホライゾンは本質的にリセット機能がついているので、あんまりGenerative modelの仮定がありません。\n",
    "\n",
    "* ``sample_next_state``: ステップ・状態・行動の集合$D$のそれぞれについて次状態を$N$個返します\n",
    "* ``collect_samples_eps_greedy``: $q\\in \\mathbb{R}^{H\\times S\\times A}$のε-貪欲方策で$H$回インタラクションしてサンプルを集めます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.random import PRNGKey\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"N\",))\n",
    "def sample_next_state(mdp: MDP, N: int, key: PRNGKey, D: jnp.array):\n",
    "    \"\"\" 遷移行列Pに従って次の状態をN個サンプルします\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "        N (int): サンプルする個数\n",
    "        key (PRNGKey)\n",
    "        D (np.ndarray): 状態行動対の集合 [(h1, s1, a1), (h1, s2, a2), ...]\n",
    "\n",
    "    Returns:\n",
    "        new_key (PRNGKey)\n",
    "        next_s_set (np.ndarray): (len(D) x N) の次状態の集合\n",
    "        count_HSAS (jnp.ndarray): 各(ステップ, 状態, 行動, 次状態)のペアの出現回数を格納した(H x S x A x S) の行列\n",
    "    \"\"\"\n",
    "    new_key, key = jax.random.split(key)\n",
    "    keys = jax.random.split(key, num=len(D))\n",
    "\n",
    "    @jax.vmap\n",
    "    def choice(key, hsa):\n",
    "        return jax.random.choice(key, mdp.S_set, shape=(N,), p=P[hsa[0], hsa[1], hsa[2]])\n",
    "\n",
    "    next_s = choice(keys, D)\n",
    "\n",
    "    # 集めたサンプルについて、(h, s, a, ns)が何個出たかカウントします。\n",
    "    H, S, A, S = mdp.P.shape\n",
    "    count_HSAS = jnp.zeros((H*S*A, S))\n",
    "    count_D_next_S = jax.vmap(lambda next_s: jnp.bincount(next_s, minlength=S, length=S))(next_s)\n",
    "    D_ravel = jnp.ravel_multi_index(D.T, (H, S, A), mode=\"wrap\")\n",
    "    count_HSAS = count_HSAS.at[D_ravel].add(count_D_next_S)\n",
    "    count_HSAS = count_HSAS.reshape(H, S, A, S)\n",
    "    return new_key, next_s, count_HSAS\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "N = 20000\n",
    "D = jnp.array([(0, 1, 2), (1, 2, 1), (0, 0, 0), (4, 3, 1)])\n",
    "key, next_states, count_HSAS = sample_next_state(mdp, N, key, D)\n",
    "\n",
    "# next_statesによるPの推定\n",
    "assert next_states.shape == (len(D), N)\n",
    "\n",
    "for i, d in enumerate(D):\n",
    "    h, s, a = d\n",
    "    P0_approx1 = jnp.bincount(next_states[i], minlength=S) / N\n",
    "    np.testing.assert_allclose(P0_approx1, mdp.P[h, s, a], atol=1e-2)\n",
    "\n",
    "    # count_HSASによるPの推定\n",
    "    P0_approx2 = count_HSAS[h, s, a] / N\n",
    "    assert np.all(P0_approx1 == P0_approx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def collect_samples_eps_greedy(mdp: MDP, key: PRNGKey, q: jnp.array, init_s: int, epsilon: float=0.0):\n",
    "    \"\"\" エピソードの開始から終了まで、MDPとインタラクションしてサンプルをH個集めます。qのε-貪欲方策に従って動きます。\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "        H (int): ホライゾン\n",
    "        key (PRNGKey)\n",
    "        q (jnp.ndarray): 行動価値関数\n",
    "        init_s (int): 初期状態\n",
    "        epsilon (float): ε-貪欲のパラメータ\n",
    "\n",
    "    Returns:\n",
    "        new_key (PRNGKey)\n",
    "        sars (jnp.ndarray): (状態, 行動, 報酬, 次状態) x H の軌跡\n",
    "        count_HSAS (jnp.ndarray): 各(ステップ, 状態, 行動, 次状態)のペアの出現回数を格納した(H x S x A x S) の行列\n",
    "    \"\"\"\n",
    "    H, S, A, S = mdp.P.shape\n",
    "    chex.assert_shape(q, (H, S, A))\n",
    "\n",
    "    def body_fn(h, args):\n",
    "        key, sars, s, count_HSAS = args\n",
    "\n",
    "        # ε-貪欲方策を実行します\n",
    "        a = q[h, s].argmax()\n",
    "        key, key1, key2 = jax.random.split(key, num=3)\n",
    "        random_a = jax.random.choice(key1, A)\n",
    "        a = jnp.where(jax.random.uniform(key2) > epsilon, a, random_a)\n",
    "        \n",
    "        # 次状態をサンプルします\n",
    "        key, key1 = jax.random.split(key)\n",
    "        next_s = jax.random.choice(key1, mdp.S_set, p=P[h, s, a])\n",
    "\n",
    "        # 集めたデータを記録します\n",
    "        r = mdp.rew[h, s, a]\n",
    "        sars = sars.at[h].set((s, a, r, next_s))\n",
    "        count_HSAS = count_HSAS.at[h, s, a, next_s].add(1)\n",
    "        return key, sars, next_s, count_HSAS\n",
    "\n",
    "    sars = jnp.zeros((H, 4))\n",
    "    count_HSAS = jnp.zeros((H, S, A, S))\n",
    "    args = key, sars, init_s, count_HSAS\n",
    "    key, sars, _, count_HSAS = jax.lax.fori_loop(0, H, body_fn, args)\n",
    "    return key, sars, count_HSAS\n",
    "\n",
    "key, sars, count_HSAS = collect_samples_eps_greedy(mdp, key, mdp.optimal_Q, 0, epsilon=1.0)\n",
    "assert sars.shape == (mdp.H, 4)\n",
    "assert count_HSAS.sum() == mdp.H\n",
    "np.testing.assert_allclose(count_HSAS.sum(axis=(1, 2, 3)), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 平均報酬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状態数： 3\n",
      "行動数： 2\n"
     ]
    }
   ],
   "source": [
    "# https://arxiv.org/abs/2406.01234 のriver swim環境\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax.random import PRNGKey\n",
    "import jax\n",
    "from typing import NamedTuple, Optional\n",
    "\n",
    "key = PRNGKey(0)\n",
    "\n",
    "S = 3  # 状態集合のサイズ\n",
    "A = 2  # 行動集合のサイズ．LEFTが0, RIGHTが1とします\n",
    "S_set = jnp.arange(S)  # 状態集合\n",
    "A_set = jnp.arange(A)  # 行動集合\n",
    "\n",
    "\n",
    "# 報酬行列（論文中では確率的ですが，今回は面倒なので決定的にします）\n",
    "rew = np.zeros((S, A))\n",
    "rew[0, 0] = 0.05\n",
    "rew[-1, 1] = 0.95\n",
    "rew = jnp.array(rew)\n",
    "assert rew.shape == (S, A)\n",
    "\n",
    "\n",
    "# 遷移確率行列\n",
    "P = np.zeros((S, A, S))\n",
    "for s in range(1, S-1):\n",
    "    P[s, 0, s-1] = 1  # LEFT\n",
    "    P[s, 1, s-1] = 0.05  # RIGHT\n",
    "    P[s, 1, s] = 0.6  # RIGHT\n",
    "    P[s, 1, s+1] = 0.35  # RIGHT\n",
    "\n",
    "# at s1\n",
    "P[0, 0, 0] = 1  # LEFT\n",
    "P[0, 1, 0] = 0.6  # RIGHT\n",
    "P[0, 1, 1] = 0.4  # RIGHT\n",
    "P[-1, 0, -2] = 1  # LEFT\n",
    "P[-1, 1, -2] = 0.05  # RIGHT\n",
    "P[-1, 1, -1] = 0.95  # RIGHT\n",
    "\n",
    "P = P.reshape(S, A, S)\n",
    "P = jnp.array(P)\n",
    "np.testing.assert_allclose(P.sum(axis=-1), 1, atol=1e-6)  # ちゃんと確率行列になっているか確認します\n",
    "\n",
    "class MDP(NamedTuple):\n",
    "    S_set: jnp.array  # 状態集合\n",
    "    A_set: jnp.array  # 行動集合\n",
    "    rew: jnp.array  # 報酬行列\n",
    "    P: jnp.array  # 遷移確率行列\n",
    "\n",
    "    @property\n",
    "    def S(self) -> int:  # 状態空間のサイズ\n",
    "        return len(self.S_set)\n",
    "\n",
    "    @property\n",
    "    def A(self) -> int:  # 行動空間のサイズ\n",
    "        return len(self.A_set)\n",
    "\n",
    "\n",
    "mdp = MDP(S_set, A_set, rew, P)\n",
    "\n",
    "print(\"状態数：\", mdp.S)\n",
    "print(\"行動数：\", mdp.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve bellman equation\n",
    "\n",
    "ref_state = 0 \n",
    "\n",
    "@jax.jit\n",
    "def V_value_iteration(mdp: MDP, tol: float = 1e-6) -> jnp.array:\n",
    "    def condition_fun(nV_V):\n",
    "        nV, V = nV_V\n",
    "        span_diff = (nV - V).max()\n",
    "        return span_diff > tol\n",
    "\n",
    "    def body_fun(nV_V):\n",
    "        V, _ = nV_V\n",
    "        gain = V[ref_state]\n",
    "        next_v = mdp.P @ V\n",
    "        nV = (mdp.rew + next_v).max(axis=1) - gain\n",
    "        return (nV, V)\n",
    "\n",
    "    init_V = jnp.zeros((mdp.S))\n",
    "    nV_V = body_fun((init_V, init_V))\n",
    "    V, _ = jax.lax.while_loop(condition_fun, body_fun, nV_V)\n",
    "    return V\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Q_value_iteration(mdp: MDP, tol: float = 1e-5) -> jnp.array:\n",
    "    def condition_fun(nQ_Q):\n",
    "        nQ, Q = nQ_Q\n",
    "        nbias = nQ.max(axis=1)  # S -> R\n",
    "        bias = Q.max(axis=1)  # S -> R\n",
    "        span_diff = (nbias - bias).max()\n",
    "        return span_diff > tol\n",
    "\n",
    "    def body_fun(nQ_Q):\n",
    "        Q, _ = nQ_Q\n",
    "        next_v = mdp.P @ Q.max(axis=1)\n",
    "        gain = Q[ref_state].max()\n",
    "        nQ = mdp.rew + next_v - gain\n",
    "        return (nQ, Q)\n",
    "\n",
    "    init_Q = jnp.zeros((mdp.S, mdp.A))\n",
    "    nQ_Q = (init_Q, init_Q)\n",
    "    nQ_Q = body_fun(nQ_Q)\n",
    "    Q, _ = jax.lax.while_loop(condition_fun, body_fun, nQ_Q)\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81846076\n",
      "[-4.281539   -2.2353868   0.39538145]\n"
     ]
    }
   ],
   "source": [
    "V = V_value_iteration(mdp)\n",
    "print(V[ref_state])\n",
    "print(V - 5.1)  # 5.1引くとだいたい元論文と同じ値になる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8184559\n",
      "[-4.281544   -2.2354004   0.39536333]\n"
     ]
    }
   ],
   "source": [
    "Q = Q_value_iteration(mdp)\n",
    "print(Q[ref_state].max())\n",
    "print(Q.max(axis=1) - 5.1)  # 5.1引くとだいたい元論文と同じ値になる"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
