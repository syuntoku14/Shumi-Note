{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 勾配ブースティング\n",
    "\n",
    "弱い学習器をいっぱい集めたらマシな学習器になるでしょうか？\n",
    "歴史的には[Kearns](https://www.cis.upenn.edu/~mkearns/papers/crypto.pdf)がこの疑問に取り組み始め，Adaboostと呼ばれるBoostingアルゴリズムにつながっていきました．今回は勾配ブースティングについて見ていきます．\n",
    "\n",
    "特に次のPAC（Probability Approximately Correct）学習と弱学習器について考えます：\n",
    "\n",
    "---\n",
    "\n",
    "**PAC学習**\n",
    "\n",
    "ある真の仮説$h^*$がPAC学習可能であるとは，任意の近似誤差$\\epsilon$と$\\delta$に対して，以下を満たす仮説$h$を出力する多項式時間アルゴリズムが存在すること\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\left[\\mathbb{P}\\left[h \\neq h^*(x)\\right]>\\epsilon\\right] \\leq 1-\\delta\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**弱学習器**\n",
    "\n",
    "固定された$\\epsilon$と$\\delta$で学習可能な学習器のこと．\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "このとき，弱学習器から強学習器（PAC学習を実現する学習器）は作れるか？について取り組んでいきます．\n",
    "基本的には次の形式の学習器について考えます：\n",
    "\n",
    "**基本的な集団学習アルゴリズム**\n",
    "\n",
    "* 観測データ：$\\mathcal{D}=\\left\\{y_i, x_i\\right\\}_{i=1}^n\\left(y_i=\\{-1,+1\\}, \\boldsymbol{x}_i \\in \\mathcal{X} \\subset \\mathbb{R}^d\\right)$\n",
    "* $T$をアンサンブルする数とします．ステップ$t=1, 2, \\dots, T$に対して，\n",
    "  * $\\mathcal{D}$からデータ$\\mathcal{D}_t$を生成\n",
    "  * $\\mathcal{D}_t$から識別器$h_t$を求める\n",
    "* $\\left\\{h_t: \\mathcal{X} \\rightarrow\\{-1,+1\\}\\right\\}_{t=1}^T$を組み合わせて識別器$H$を構築\n",
    "\n",
    "します．特に$\\mathcal{D}_t$を$\\mathcal{D}$からサンプリングして作成するアルゴリズムをバギング，$\\mathcal{D}_t$を$\\mathcal{D}$と$\\{h_1, \\dots, h_{t-1}\\}$の出力を利用して作成するアルゴリズムをブースティングといいます．\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回帰におけるGradient boosting\n",
    "\n",
    "さて，回帰を実現するGradient boostingアルゴリズムについて見ていきましょう．先に回帰について確認して，その仕組み（なぜ勾配なのか？）を見ます．\n",
    "\n",
    "* 観測データ：$\\left\\{\\left(y_i, \\boldsymbol{x}_i\\right)\\right\\}_{i=1}^n \\boldsymbol{x}_i \\in \\mathcal{X} \\subset \\mathbb{R}^d, y_i \\in \\mathbb{R}$\n",
    "* 弱学習器：$h: \\mathcal{X} \\rightarrow \\mathbb{R}$\n",
    "* 集団学習における回帰問題($H(x_i)$は弱学習器を混ぜることで実現します)：\n",
    "$$\n",
    "y_i=H\\left(\\boldsymbol{x}_i\\right)=\\sum_{t=1}^T \\alpha_t h_t\\left(\\boldsymbol{x}_i\\right) .(i=1,2, \\ldots, n)\n",
    "$$\n",
    "\n",
    "以降，弱学習器$h_t$の選び方と，その混ぜ方$\\alpha_t$について考えていきます．\n",
    "\n",
    "---\n",
    "\n",
    "**モチベーション**\n",
    "\n",
    "まずはモチベーションを確認しましょう．結局，弱学習器を追加するときに，各$(y_i, x_i)$に対して\n",
    "$$\n",
    "y_i \\approx H_{t-1}\\left(\\boldsymbol{x}_i\\right)+\\alpha_t h_t\\left(\\boldsymbol{x}_i\\right)(i=1,2, \\ldots, n)\n",
    "$$\n",
    "を満たすように$h_t$を選びたいので，\n",
    "\n",
    "$$\n",
    "y_i-H_{t-1}\\left(\\boldsymbol{x}_i\\right) \\approx \\alpha_t h_t\\left(\\boldsymbol{x}_i\\right)(i=1,2, \\ldots, n)\n",
    "$$\n",
    "\n",
    "が成立すればOKです．よって，\n",
    "\n",
    "$$\n",
    "\\left\\{\\left(\\tilde{y}_i, \\boldsymbol{x}_i\\right)\\right\\}_{i=1}^n,\\left(\\tilde{y}_i=y_i-H_{t-1}\\left(\\boldsymbol{x}_i\\right)\\right)\n",
    "$$\n",
    "\n",
    "を使って弱学習器$h_t$を学べば良さそうです．すなわち，これまで構成した学習器$H_{t-1}$の残差$\\tilde{y}_i$を使って学習器を構成しているわけですね．\n",
    "\n",
    "これまでの学習器$H_{t-1}$の損失関数を，回帰なので二乗損失として，\n",
    "$$\n",
    "\\mathcal{L}\\left(H_{t-1}\\right)=\\sum_{i=1}^n \\ell\\left(y_i, H_{t-1}\\left(\\boldsymbol{x}_i\\right)\\right)=\\frac{1}{2} \\sum_{i=1}^n\\left(y_i-H_{t-1}\\left(\\boldsymbol{x}_i\\right)\\right)^2\n",
    "$$\n",
    "とします．$H_{t-1}(x_i)$を一つの変数として偏微分を取ると，\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}\\left(H_{t-1}\\right)}{\\partial H_{t-1}\\left(\\boldsymbol{x}_i\\right)}=H_{t-1}\\left(\\boldsymbol{x}_i\\right)-y_i .\n",
    "$$\n",
    "なので，残差が出てきました！よって，残差を実現する$h_t$は\n",
    "$$\n",
    "h_t\\left(\\boldsymbol{x}_i\\right)=y_i-H_{t-1}\\left(\\boldsymbol{x}_i\\right)=-\\frac{\\partial L}{\\partial H_{t-1}\\left(\\boldsymbol{x}_i\\right)}\n",
    "$$\n",
    "を満たし，その場合は\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H_t\\left(\\boldsymbol{x}_i\\right) & =H_{t-1}\\left(\\boldsymbol{x}_i\\right)+\\alpha_t h_t\\left(\\boldsymbol{x}_i\\right) \\\\\n",
    "& =H_{t-1}\\left(\\boldsymbol{x}_i\\right)-\\alpha_t \\frac{\\partial L}{\\partial H_{t-1}\\left(\\boldsymbol{x}_i\\right)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "が成り立ちます．これは勾配法そのものですね．$h_t$は直接残差を取れるわけではないので，$h_t$を回帰しましょう：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\tilde{y}_i & =-\\left[\\frac{\\partial \\ell\\left(y_i, H\\left(\\boldsymbol{x}_i\\right)\\right)}{\\partial H\\left(\\boldsymbol{x}_i\\right)}\\right]_{H=H_{t-1}}=y_i-H_{t-1}\\left(\\boldsymbol{x}_i\\right) \\\\\n",
    "h_t & =\\underset{h \\in \\mathcal{H}}{\\operatorname{argmin}} \\frac{1}{2} \\sum_{i=1}^n\\left(\\tilde{y}_i-h\\left(\\boldsymbol{x}_i\\right)\\right)^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "とすれば，結局$h_t$の学習は，「学習データ全体についての勾配」を求めていることになります．\n",
    "\n",
    "まとめると，Gradient boostingは次のアルゴリズムを走らせます：\n",
    "\n",
    "* ステップ$t=1, 2, \\dots, T$に対して：\n",
    "  * $\\ell(y, H(\\boldsymbol{x}))$の関数$H$に対する勾配を計算する：\n",
    "$$\n",
    "\\tilde{y}_i=-\\left[\\frac{\\partial \\ell\\left(y_i, H\\left(\\boldsymbol{x}_i\\right)\\right)}{\\partial H\\left(\\boldsymbol{x}_i\\right)}\\right]_{H=H_{t-1}}\n",
    "$$\n",
    "  * 勾配を回帰：$h_t=\\underset{h \\in \\mathcal{H}}{\\operatorname{argmin}} \\sum^n\\left(\\tilde{y}_i-h\\left(\\boldsymbol{x}_i\\right)\\right)^2$\n",
    "  * 学習率を調整：$\\alpha_t=\\underset{\\alpha>0}{\\operatorname{argmin}} \\mathcal{L}\\left(H_{t-1}+\\alpha h_t\\right)$\n",
    "  * 次の学習器を生成：$H_t=H_{t-1}+\\alpha_t h_t$\n",
    "\n",
    "最後に\n",
    "$$\n",
    "H(\\boldsymbol{x})=\\sum_{t=1}^T \\alpha_t h_t(\\boldsymbol{x})\n",
    "$$\n",
    "を出力します．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**損失関数の例**\n",
    "\n",
    "* 二乗損失：$\\ell\\left(y_i, H\\left(\\boldsymbol{x}_i\\right)\\right)=\\frac{1}{2}\\left(y_i-H\\left(\\boldsymbol{x}_i\\right)\\right)^2, \\quad \\tilde{y}_i=y_i-H_{t-1}\\left(\\boldsymbol{x}_i\\right)$\n",
    "* 絶対損失：$\\ell\\left(y_i, H\\left(\\boldsymbol{x}_i\\right)\\right)=\\left|y_i-H\\left(\\boldsymbol{x}_i\\right)\\right|, \\quad \\tilde{y}_i=\\operatorname{sign}\\left(y_i-H_{t-1}\\left(\\boldsymbol{x}_i\\right)\\right)$\n",
    "* ロジスティック損失（二値分類）：$\\ell\\left(y_i, H\\left(\\boldsymbol{x}_i\\right)\\right) =\\log \\left(1+\\exp \\left(-y_i H\\left(\\boldsymbol{x}_i\\right)\\right)\\right)$として，$\\tilde{y}_i =\\frac{y_i}{1+\\exp \\left(y_i H_{t-1}\\left(\\boldsymbol{x}_i\\right)\\right)}$\n",
    "\n",
    "また，多クラス分類の場合はカテゴリ$k = 1, \\dots, K$について\n",
    "$$H_k(\\boldsymbol{x})=\\sum_{t=1}^T \\alpha_{t, k} h_{t, k}(\\boldsymbol{x})$$\n",
    "を構築し，損失関数を\n",
    "$$\n",
    "P_k(\\boldsymbol{x})=\\frac{\\exp \\left(H_k(\\boldsymbol{x})\\right)}{\\sum_{k^{\\prime}=1}^K \\exp \\left(H_{k^{\\prime}}(\\boldsymbol{x})\\right)}\n",
    "$$\n",
    "とします．つまり各クラス$k$に対して二値分類器を用意しているわけですね．マージン関数とは異なり，ソフトマックス損失を使っていることに注意しましょう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boostingの汎化誤差解析（二値分類）\n",
    "\n",
    "ここまでは回帰についてみてきました．\n",
    "以下の汎化誤差解析では基本的に二値分類を考えます（回帰はリプシッツ連続じゃなかったりしてめんどくさいです）．\n",
    "\n",
    "次の凸包を考えます：\n",
    "\n",
    "$$\n",
    "\\operatorname{conv}(\\mathcal{F})=\\left\\{\\sum_{k=1}^K \\alpha_k f_k \\mid K \\in \\mathbb{N}, \\sum_{k=1}^K \\alpha_k=1, \\alpha_k \\geq 0, f_k \\in \\mathcal{F}\\right\\}\n",
    "$$\n",
    "以降では，凸包のラデマッハ複雑度が$R_n(\\operatorname{conv}(\\mathcal{F}))=R_n(\\mathcal{F})$を満たすことを使います．\n",
    "ここで，\n",
    "$$\n",
    "R_n(\\operatorname{conv}(\\mathcal{F}))=\\mathbb{E}[\\sup _{F \\in \\operatorname{conv}(\\mathcal{F})} \\frac{1}{n} \\sum_{i=1}^n \\sigma_i \\underbrace{\\sum_{k=1}^K \\alpha_k f_k\\left(x_i\\right)}_{=F\\left(x_i\\right)}] .\n",
    "$$\n",
    "としました．$F(x_i)$は，与えられた$x_i$に対して凸包によって与えられる関数の値です．\n",
    "\n",
    "**証明**\n",
    "\n",
    "$$\n",
    "\\mathcal{A}=\\left\\{\\left(\\alpha_1, \\ldots, \\alpha_K\\right) \\mid K \\in \\mathbb{N}, \\alpha_k \\geq 0, \\frac{1}{K} \\sum_{k=1}^K \\alpha_k=1\\right\\}\n",
    "$$\n",
    "について，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\sup _{F \\in \\operatorname{conv}(\\mathcal{F})} \\frac{1}{n} \\sum_{i=1}^n \\sigma_i \\underbrace{\\sum_{k=1}^K \\alpha_k f_k\\left(x_i\\right)}_{=F\\left(x_i\\right)} \\\\\n",
    "& =\\sup _{\\left(\\alpha_1 \\ldots, \\alpha_K\\right) \\in \\mathcal{A}} \\sup _{f_1, \\ldots, f_K \\in \\mathcal{F}} \\frac{1}{n} \\sum_{i=1}^n \\sigma_i \\sum_{k=1}^K \\alpha_k f_k\\left(x_i\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "が成り立ちます．ここで$K$は固定してるわけではなく，適当な$K$を持ってきています（最終的に$K$に依存しないバウンドが出てくるので大丈夫です）．\n",
    "これに対して，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\sup _{f_1, \\ldots, f_K \\in \\mathcal{F}} \\sup _{\\left(\\alpha_1, \\ldots, \\alpha_K\\right) \\in \\mathcal{A}} \\frac{1}{n} \\sum_{i=1}^n \\sigma_i \\sum_{k=1}^K \\alpha_k f_k\\left(x_i\\right) \\\\\n",
    "= & \\sup _{f_1, \\ldots . f_K \\in \\mathcal{F}} \\sup _{\\left(\\alpha_1 \\ldots \\ldots \\alpha_K\\right) \\in \\mathcal{A}} \\frac{1}{n} \\sum_{k=1}^K \\alpha_k \\sum_{i=1}^n \\sigma_i f_k\\left(x_i\\right) \\\\\n",
    "= & \\sup _{f_1, \\ldots, f_K \\in \\mathcal{F}} \\max _k \\frac{1}{n} \\sum_{i=1}^n \\sigma_i f_k\\left(x_i\\right) \\\\\n",
    "= & \\sup _{f \\in \\mathcal{F}} \\frac{1}{n} \\sum_{i=1}^n \\sigma_i f\\left(x_i\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "が成り立ちます．\n",
    "３行目では$\\alpha_k$の和が１なので，$\\alpha_k$のsupを取るのは一番値がでかい$k$に対して$1$を割り当てるのと同じなことを使ってます\n",
    "\n",
    "---\n",
    "\n",
    "さて，弱学習器の集合を$\\mathcal{F}$としましょう．そして，Boostingの仮説関数を$H(\\boldsymbol{x})=\\sum_{t=1}^T \\alpha_t h_t(\\boldsymbol{x})$とします（ここで$h_t \\in \\mathcal{F}$）です．\n",
    "Boostingの仮説関数の集合を$\\mathcal{H}$とします．\n",
    "\n",
    "ここで，Boostingアルゴリズムでは$\\alpha_t \\geq 0$かつ$\\sum_{t=1}^T \\alpha_t=1$を考えていました．\n",
    "これを実現するために，適当に持ってきた$\\alpha_t$や$h_t$に対して，\n",
    "* $\\alpha_t$の値については，任意の$h \\in \\mathcal{F}$に対して$-h$も$\\mathcal{F}$とする\n",
    "* $\\sum^t_{t=1} \\alpha_t$を使って$H(x)$を正規化しても符号が変化しないので，正規化する（これは二値分類ではOKだが多値分類だと問題がでないか？）\n",
    "\n",
    "ことにします．これで$\\mathcal{F}$から適切な仮説が作れます．さて，次を満たす仮説集合を考えましょう．\n",
    "$$\n",
    "H(\\boldsymbol{x})=\\sum_{t=1}^T \\alpha h_t(\\boldsymbol{x}), h_t \\in \\mathcal{F}, \\alpha_t \\geq 0, \\sum_{t=1}^t \\alpha_t=1\n",
    "$$\n",
    "このとき次が成立します：\n",
    "\n",
    "---\n",
    "\n",
    "正数$\\rho > 0$に対して，確率$1-\\delta$以上で\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{P}[\\operatorname{sign}(H(\\boldsymbol{x})) \\neq y] \\leq & \\frac{1}{n} \\sum_{i=1}^n \\phi_\\rho\\left(y_i H\\left(\\boldsymbol{x}_i\\right)\\right) \\\\\n",
    "& +\\frac{2}{\\rho} R_n(\\mathcal{F})+\\sqrt{\\frac{\\log (1 / \\delta)}{2 n}} .\n",
    "\\end{aligned}\n",
    "$$\n",
    "が成立する．\n",
    "\n",
    "ここで損失関数を\n",
    "$$\n",
    "\\phi_{\\rho=1}\\left(y_i H\\left(\\boldsymbol{x}_i\\right)\\right) \\leq \\exp \\left(-y_i H\\left(\\boldsymbol{x}_i\\right)\\right)\n",
    "$$\n",
    "とすれば，Adaboostの汎化誤差解析になります（Gradient boostingは損失関数が決まってるわけではなく，特に指数損失の場合をAdaboostと呼ぶらしいです．指数損失のほうがヒンジ損失よりでかいので，Adaboostはあまり誤差を許容しません．なのでラベルミスなどにとても弱いです）．\n",
    "\n",
    "**証明**\n",
    "\n",
    "これはそんなに難しくありません．\n",
    "簡単な計算で，元の\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{P}[\\operatorname{sign}(H(\\boldsymbol{x})) \\neq y]= & \\mathbb{E}[\\mathbb{I}(\\operatorname{sign}(H(\\boldsymbol{x})) \\neq y)] \\\\\n",
    "= & \\mathbb{E}[\\mathbb{I}(y H(\\boldsymbol{x}) \\leq 0)] \\\\\n",
    "\\leq & \\mathbb{E}\\left[\\phi_\\rho(y H(\\boldsymbol{x}))\\right] \\\\\n",
    "\\leq & \\frac{1}{n} \\sum_{n=1}^n \\phi_\\rho\\left(y_i H\\left(\\boldsymbol{x}_i\\right)\\right) \\\\\n",
    "& +2 \\underbrace{R_n\\left(\\phi_\\rho \\circ \\mathcal{H}\\right)}_{=\\frac{1}{\\rho} R_n(\\mathcal{F})}+\\sqrt{\\frac{\\log (1 / \\delta)}{2 n}} .\n",
    "\\end{aligned}\n",
    "$$\n",
    "が成り立ちます．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 決定株（decision stamp）のラデマッハ複雑度\n",
    "\n",
    "他の重要な集団学習アルゴリズムに決定木アルゴリズムがあります．\n",
    "決定木は次の決定株アルゴリズムの集団学習とみなせます．先に決定株について見てみましょう：\n",
    "\n",
    "* 入力：$x \\in \\mathcal{X} \\subset \\mathbb{R}^d$\n",
    "* 決定株のパラメータ：$\\alpha \\in\\{-1,+1\\}, k \\in\\{1, \\ldots, d\\}, z \\in \\mathbb{R}$\n",
    "* 仮説関数：$h_{\\alpha, k, z}(x)=\\alpha \\cdot \\operatorname{sign}\\left(x_k-z\\right)$\n",
    "    * すなわち，入力$x$の各次元に対して二値分類（弱識別）を実行する関数です．パラメータ$z$によって符号を判別します．\n",
    "* 仮説空間：$\\mathcal{H}=\\left\\{h_{\\alpha, k, z}(x) \\mid \\alpha \\in\\{-1,+1\\}, k \\in\\{1, \\ldots, d\\}, z \\in \\mathbb{R}\\right\\}$\n",
    "\n",
    "このとき，決定株の仮説空間のラデマッハ複雑度は\n",
    "\n",
    "$$\n",
    "R_n(\\mathcal{H})=\\sqrt{\\frac{2}{n} \\log (2(n+1) d)}\n",
    "$$\n",
    "\n",
    "で与えられます．\n",
    "\n",
    "**証明**\n",
    "\n",
    "これはMassartの有限仮説の補題で証明できます．復習すると，有限集合$\\mathcal{A} \\subset \\mathbb{R}^n$に対して，$\\sup _{\\left(a_1, \\ldots, a_n\\right) \\in \\mathcal{A}} \\sum_{i=1}^n a_i^2 \\leq M^2$ならば，\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_\\sigma\\left[\\sup _{\\left(a_1, \\ldots, a_n\\right) \\in \\mathcal{A}} \\frac{1}{n} \\sum_{i=1}^n \\sigma_i a_i\\right] \\leq \\frac{\\sqrt{2 M^2 \\log | \\mathcal{A}}}{n} |\n",
    "$$\n",
    "が成り立ちます．\n",
    "\n",
    "ここで，決定株の前提を思い出しましょう．\n",
    "* $\\alpha \\in\\{-1,+1\\}$ 及び $k \\in\\{1, \\ldots, d\\}$は離散集合です\n",
    "* $z\\in \\mathbb{R}$は連続ですが，各座標$k=1, \\dots, d$について，$x_1, x_2, \\dots, x_d$を考えると，それぞれの値を超えるか超えないかの２種類しか仮説関数に反映されません．よって，仮説としての種類は実質$n+1$通りしかありません．\n",
    "* よって$\\mathcal{H}$は有限集合であり，$|\\mathcal{H}|=2 \\times d \\times(n+1)$なので，Massartの有限仮説の補題から，\n",
    "$$\n",
    "R_n(\\mathcal{H})=\\sqrt{\\frac{2}{n} \\log |\\mathcal{H}|}=\\sqrt{\\frac{2}{n} \\log (2(n+1) d)}\n",
    "$$\n",
    "が成り立ちます．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回帰木の汎化誤差解析\n",
    "\n",
    "回帰木の詳細はスキップします．\n",
    "次を準備として考えます：\n",
    "\n",
    "* 各葉$\\mathrm{leaf}_l$は値$w_l \\in \\mathbb{R}$を持つ\n",
    "* 入力$x \\in \\mathcal{X} \\subset \\mathbb{X}^d$を葉のいずれかに対応付けるノード分割関数を決定木を使って表現する．\n",
    "* 入力 $x$ が葉 $\\mathrm{leaf}_l$に対応付けられる場合は $1$，そうでない場合は $0$ であるような対応付けを $\\mathbb{1}_{x \\in \\text {leaf }_l}$, と表記する.\n",
    "* 回帰木$h: \\mathbb{R}^d \\to \\mathbb{R}$は\n",
    "$$\n",
    "h(x) := \\sum_{l \\in \\mathrm{leaves(h)}} w_l \\mathbb{1}_{x \\in \\mathrm{leaf}_l}\n",
    "$$\n",
    "として定義される．ここで$\\mathrm{leaves(h)}$は$h$の葉の集合．\n",
    "\n",
    "つまり，入力された$x$から葉$l$にたどり着いたら$w_l$がアクティブになり，回帰として出力されることになります．\n",
    "また，ここでのパラメータは葉と対応付け，そして$w_l$になります．\n",
    "このラデマッハ複雑度を導出しましょう．\n",
    "\n",
    "### 回帰木のラデマッハ複雑度\n",
    "\n",
    "* $\\widehat{\\boldsymbol{\\sigma}}$を次元数=葉の数のベクトルとして，その要素を\n",
    "$$\n",
    "[\\widehat{\\boldsymbol{\\sigma}}]_l=\\sum_{i=1}^n \\sigma_i \\mathbb{1}_{x_i \\in \\mathrm{leaf}_l}\n",
    "$$\n",
    "とします．\n",
    "* $\\|\\boldsymbol{w}\\|_q \\leq \\lambda, q \\geq 1$と仮定しましょう（アルゴリズムに実装する際は正則化をかければ良いです．）\n",
    "* $h(x)$を構成する決定木のノード数を$m$とします．\n",
    "* $h(x)$の集合を$\\mathcal{H}_{m, \\lambda, q}$とします．ここで，\n",
    "  \n",
    "$$\n",
    "\\mathcal{H}_{m, \\lambda, q}=\\left\\{h(x)=\\sum_{l \\in \\mathrm{leaves (h)}} w_l \\mathbb{1}_{x \\in \\mathrm{leaf}_l} : \\|w\\|_q \\leq \\lambda , q \\geq 1\\right \\}\n",
    "$$\n",
    "です．\n",
    " \n",
    "このとき，そのラデマッハ複雑度は\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{R}_{\\mathcal{S}_n}\\left(\\mathcal{H}_{m, \\lambda, q}\\right)&=\\frac{1}{n} \\mathbb{E}\\left[\\sup _{h \\in \\mathcal{H}_{m, \\lambda, q}}\\left[\\sum_{i=1}^n \\sigma_i h\\left(x_i\\right)\\right]\\right]\\\\\n",
    "&=\\frac{1}{n} \\underset{E}{\\mathbb{E}}\\left[\\sup _{h \\in \\mathcal{H}_{m, \\lambda, q}^{\\circ}}[\\hat{\\sigma} \\cdot \\boldsymbol{w}]\\right]\\\\\n",
    "& \\leq \\frac{1}{n} \\underset{\\sigma}{\\mathbb{E}}\\left[\\sup _{h \\in \\mathcal{H}_{m, \\lambda, q}}\\|\\hat{\\sigma}\\|_r\\|\\boldsymbol{w}\\|_q\\right] \\quad \\text { (Hölder's inequality) } \\\\\n",
    "& \\leq \\frac{\\lambda}{n} \\underset{\\sigma}{\\mathbb{E}}\\left[\\sup _{h \\in \\mathcal{H}_m}\\|\\hat{\\sigma}\\|_r\\right] \\quad\\left(\\|\\boldsymbol{w}\\|_q \\leq \\lambda\\right) \\\\\n",
    "& \\leq \\frac{\\lambda}{n} \\underset{\\sigma}{\\mathbb{E}}\\left[\\sup _{h \\in \\mathcal{H}_m}\\|\\hat{\\sigma}\\|_1\\right] \\quad\\left(r \\geq 1 \\Rightarrow\\|\\hat{\\sigma}\\|_r \\leq\\|\\hat{\\sigma}\\|_1\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "逆に言うと，Holderの不等式などを使いたいので適切な仮定を導入したわけですね．\n",
    "ここでさらに，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&=\\frac{\\lambda}{n} \\underset{\\sigma}{\\mathbb{E}}\\left[\\sup _{h \\in \\mathcal{H}_m} \\sum_{l \\in \\operatorname{leaves}(h)}\\left|\\sum_{i=1}^n \\sigma_i \\mathbb{1}_{\\left\\{x_i \\in \\text { leaf }_l\\right\\}}\\right|\\right]\\\\\n",
    "&\\leq\\frac{\\lambda}{n} \\underset{\\sigma}{\\mathbb{E}}\\left[\\sup _{h \\in \\mathcal{H}_m, s_l \\in \\{+1, -1\\}} \\sum_{l \\in \\operatorname{leaves}(h)}s_l\\sum_{i=1}^n \\sigma_i \\mathbb{1}_{\\left\\{x_i \\in \\text { leaf }_l\\right\\}}\\right]\\\\\n",
    "&=\\frac{\\lambda}{n} \\underset{\\sigma}{\\mathbb{E}}\\left[\\sup _{h \\in \\mathcal{H}_m, s_l \\in \\{+1, -1\\}} \\sum_{i=1}^n \\sigma_i \\sum_{l \\in \\operatorname{leaves}(h)}s_l\\mathbb{1}_{\\left\\{x_i \\in \\text { leaf }_l\\right\\}}\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "とできます．最終的にこれは決定木に帰着できました！そもそも$w_l$を使った回帰問題でのラデマッハ複雑度を考えるのが難しいので，決定木に直していると考えるとわかりやすいでしょう．\n",
    "結局，まとめると，\n",
    "\n",
    "決定木$b: \\mathbb{R}^d \\rightarrow\\{+1,-1\\}$\n",
    "$$\n",
    "b(x ; \\boldsymbol{s}, h):=\\sum_{l \\in \\mathrm{leaves}(h)} s_l \\mathbb{1}_{x \\in \\mathrm{leaf}_l}\n",
    "$$\n",
    "の集合を$\\mathcal{B}$とすると，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\widehat{R}_{\\mathcal{S}_n}\\left(\\mathcal{H}_{m, \\lambda, q}\\right) \\leq \\frac{\\lambda}{n} \\underset{\\underset{\\sigma}{\\mathbb{E}}}{\\mathbb{E}}\\left[\\sup _{\\substack{h \\in \\mathcal{H}_m \\\\\n",
    "s_l \\in\\{+1,-1\\}}} \\sum_{i=1}^n \\sigma_i \\sum_{l \\in \\text { leaves }(h)} s_l \\mathbb{1}_{\\left\\{x_i \\in \\text { lea } A_l\\right\\}}\\right] \\\\\n",
    "& =\\frac{\\lambda}{n} \\underset{\\sigma}{\\mathbb{E}}\\left[\\sup _{\\substack{h \\in \\mathcal{H}_m \\\\\n",
    "s_l \\in\\{+1,-1\\}}} \\sum_{i=1}^n \\sigma_i b\\left(x_i ; s, h\\right)\\right] . \\\\\n",
    "&\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "であり，つまり\n",
    "$$\n",
    "\\widehat{R}_n\\left(\\mathcal{H}_{m, \\lambda, q}\\right) \\leq \\frac{\\lambda}{n} \\underset{\\sigma}{\\mathbb{E}}\\left[\\sup _{b \\in \\mathcal{B}_m} \\sum_{i=1}^n \\sigma_i b\\left(x_i\\right)\\right]=\\lambda \\widehat{R}_{\\mathcal{S}_n}(\\mathcal{B})\n",
    "$$\n",
    "A\n",
    "として，**回帰木の複雑度が決定木の複雑度で抑えられます**．\n",
    "この決定木$b(\\cdot)$のノード数を$m$の二分木として，以下で解析していきましょう．\n",
    "このとき$\\mathcal{B}_m$は有限集合なので，Massartの有限仮説の補題から\n",
    "\n",
    "$$\n",
    "\\widehat{R}_n\\left(\\mathcal{B}_m\\right)=\\sqrt{\\frac{2}{n} \\log \\left|\\mathcal{B}_m\\right|}\n",
    "$$\n",
    "\n",
    "が成り立ちます．あとは$|\\mathcal{B}_m|$を求めるだけです．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二分木の仮説数\n",
    "\n",
    "これは簡単です．深さが$D$のとき，葉の数は$2^D$であり，ノード数は$m = 2^D - 1$なので，結局葉の数は$m+1$個です．ゴニョゴニョすると，結局\n",
    "$\\mathcal{B}_{m, 0}$（これは分類のしきい値を$0$にしたやつです）のサイズは\n",
    "$$\n",
    "\\left|\\mathcal{B}_{m, 0}\\right|=d^m \\cdot 2^{m+1}\n",
    "$$\n",
    "で与えられます．$\\theta$の値をノードごとに帰る場合は$\\theta$が連続になりそうですが，上でやった議論と同様にすると結局有限個の分類パターンしかありません．よって，\n",
    "\n",
    "$$\n",
    "\\left|\\mathcal{B}_m\\right|=(n+1)^m \\cdot d^m \\cdot 2^{m+1}\n",
    "$$\n",
    "\n",
    "が得られます．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
