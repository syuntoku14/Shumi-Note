{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多様な制約を一般化した強化学習のフレームワーク\n",
    "\n",
    "参考：\n",
    "* [Safe Exploration in Reinforcement Learning: A Generalized Formulation and Algorithms](https://arxiv.org/abs/2310.03225)\n",
    "* [A Survey of Constraint Formulations in Safe Reinforcement Learning](https://arxiv.org/abs/2402.02025)\n",
    "\n",
    "CMDPでは期待値の意味での制約を考えますが，それ以外にも制約の実現方法はあります．\n",
    "今回はいろいろなタイプのSafe RLを見てみましょう．\n",
    "\n",
    "表記：\n",
    "* 価値：$V_{r, h}^\\pi(s):=\\mathbb{E}_\\pi\\left[\\sum_{h^{\\prime}=h}^H \\gamma_r^{h^{\\prime}} r\\left(s_{h^{\\prime}}, a_{h^{\\prime}}\\right) \\mid s_h=s\\right]$\n",
    "* 初期状態での期待値：$V_c^\\pi(\\rho):=\\mathbb{E}_{s \\sim \\rho}\\left[V_{c, 0}^\\pi(s)\\right]$\n",
    "* $V_{\\max }:=\\frac{1-\\gamma_r^H}{1-\\gamma_r}$\n",
    "* ベルマン最適作用素：$\\mathcal{T}_h(Q)(s, a):=\\mathbb{E}\\left[r\\left(s_h, a_h\\right)+\\gamma_r V_Q\\left(s_{h+1}\\right) \\mid s_h=s, a_h=a\\right]$, where $V_Q(s):=\\max _{a \\in \\mathcal{A}} Q(s, a)$\n",
    "\n",
    "また，方策空間は決定的であることを仮定します（解けるのか？）\n",
    "\n",
    "## いろいろなSafe RL\n",
    "\n",
    "---\n",
    "\n",
    "**1. 期待値制約**\n",
    "\n",
    "$$\n",
    "\\max _\\pi V_r^\\pi(\\rho) \\quad \\text { s.t. } \\quad V_c^\\pi(\\rho) \\leq \\xi \\text {. }\n",
    "$$\n",
    "\n",
    "これは扱いやすいのでよく使われます．\n",
    "特にラグランジュ形式にすると報酬とコストが合体できるので解きやすいです．\n",
    "\n",
    "---\n",
    "\n",
    "**2. Safe RL with state constraints**\n",
    "\n",
    "* [Recovery RL: Safe Reinforcement Learning with Learned Recovery Zones](https://arxiv.org/abs/2010.15920)など\n",
    "\n",
    "$$\n",
    "\\max _\\pi V_r^\\pi(\\rho) \\quad \\text { s.t. } \\quad \\mathbb{E}_\\pi\\left[\\sum_{h=0}^H \\gamma_c^h \\mathbb{I}\\left(s_h \\in S_{\\text {unsafe }}\\right)\\right] \\leq \\xi\n",
    "$$\n",
    "\n",
    "危険な状態に突入してしまうことを避ける問題設定です．\n",
    "ロボティクスの応用などでよく出てくるっぽいですね．\n",
    "ちなみにこの問題は$c(s, a):=\\mathbb{I}\\left(s \\in S_{\\text {unsafe }}\\right)$とすれば，**1**に含まれます．\n",
    "\n",
    "---\n",
    "\n",
    "**3. Joint chance constraint**\n",
    "\n",
    "* [Safe Reinforcement Learning with Chance-constrained Model Predictive Control](https://arxiv.org/abs/2112.13941)\n",
    "\n",
    "$$\n",
    "\\max _\\pi V_r^\\pi(\\rho) \\quad \\text { s.t. } \\quad \\mathbb{P}_\\pi\\left[\\bigvee_{h=0}^H s_h \\in S_{\\text {unsafe }}\\right] \\leq \\xi,\n",
    "$$\n",
    "\n",
    "$h=0$から$H$までを総合して，危険な状態にいる確率が一定値以下であることを保証します．\n",
    "制御の問題でたまに出てくるみたいです．\n",
    "これは実際解きづらいので，次のゆるい問題を解くことがあります．\n",
    "\n",
    "$$\n",
    "\\mathbb{P}_\\pi\\left[\\bigvee_{h=1}^H s_h \\in S_{\\text {unsafe }}\\right] \\leq \\mathbb{E}_\\pi\\left[\\sum_{h=1}^H \\mathbb{I}\\left(s_h \\in S_{\\text {unsafe }}\\right)\\right]\n",
    "$$\n",
    "\n",
    "この意味で，**2**は**3**よりもゆるい問題です．\n",
    "\n",
    "---\n",
    "\n",
    "**4. Expected Instantaneous Safety Constraint\n",
    "with Time-variant Threshold**\n",
    "\n",
    "* [OptLayer - Practical Constrained Optimization for Deep Reinforcement Learning in the Real World](https://arxiv.org/abs/1709.07643)など\n",
    "\n",
    "$$\n",
    "\\max _\\pi V_r^\\pi(\\rho) \\quad \\text { s.t. } \\quad \\mathbb{E}_\\pi\\left[c\\left(s_h, a_h\\right)\\right] \\leq \\xi_h, \\forall h \\in[H]\n",
    "$$\n",
    "\n",
    "あんまり見ないですが，各ステップで制約を考える設定です．\n",
    "多分普通の問題よりは解きやすいのかも？\n",
    "また，\n",
    "$$\n",
    "\\eta_h:=\\gamma_c^{-h} \\cdot\\left(\\xi-\\sum_{h^{\\prime}=0}^{h-1} \\gamma_c^{h^{\\prime}} c\\left(s_{h^{\\prime}}, a_{h^{\\prime}}\\right)\\right), \\forall h \\in[H]\n",
    "$$\n",
    "を使って$\\mathbb{E}_\\pi\\left[c\\left(s_h, a_h\\right) \\leq \\eta_h\\right]$を考えると，これは**1**と等価っぽいです．\n",
    "\n",
    "**TODO: これは$\\eta_h$が方策に依存してしまうので，等価ではないのでは？各エピソードで方策が変わるとすると，これはNon-stationary MDPを解くことになるかもしれない．**\n",
    "\n",
    "---\n",
    "**5. Almost surely safe RL**\n",
    "\n",
    "* [Saute RL: Almost Surely Safe Reinforcement Learning Using State Augmentation](https://arxiv.org/abs/2202.06558)で導入されたっぽい\n",
    "\n",
    "$$\n",
    "\\max _\\pi V_r^\\pi(\\rho) \\quad \\text { subject to } \\quad \\mathbb{P}_\\pi \\left[\\sum_{h=1}^H \\gamma_c^h c\\left(s_h, a_h\\right) \\leq \\xi \\right]=1,\n",
    "$$\n",
    "\n",
    "これは期待値よりも強い制約を課す問題です．\n",
    "TODO: これNP困難になりそうだけど大丈夫か？Mean-variance MDPを考えるとこれNP困難になりそうな気がする．\n",
    "\n",
    "---\n",
    "\n",
    "**6. Almost Surely Instantaneous Safety\n",
    "Constraint with Time-invariant Threshold**\n",
    "\n",
    "* [Safe Exploration in Reinforcement Learning: A Generalized Formulation and Algorithms](https://arxiv.org/abs/2310.03225)\n",
    "* [End-to-End Safe Reinforcement Learning through Barrier Functions for Safety-Critical Continuous Control Tasks](https://arxiv.org/abs/1903.08792)\n",
    "\n",
    "$$\n",
    "\\max _\\pi V_r^\\pi(\\rho) \\quad \\text { s.t. } \\quad \\mathbb{P}_\\pi\\left[c\\left(s_h, a_h\\right) \\leq \\xi\\right]=1, \\forall h \\in[H]\n",
    "$$\n",
    "\n",
    "各ステップ全部で制約を満たすことを考え，しかもAlmost surelyであることを要求します．\n",
    "制御工学におけるバリア関数やリアプノフ関数とかで使われるっぽい？\n",
    "\n",
    "---\n",
    "\n",
    "**7. Almost Surely Instantaneous Safety Constraint with Time-variant Threshold**\n",
    "\n",
    "* [Safe Exploration in Reinforcement Learning: A Generalized Formulation and Algorithms](https://arxiv.org/abs/2310.03225)\n",
    "\n",
    "$$\n",
    "\\max _\\pi V_r^\\pi(\\rho) \\quad \\text { s.t. } \\quad \\mathbb{P}_\\pi\\left[c\\left(s_h, a_h\\right) \\leq \\xi_h\\right]=1, \\forall h \\in[H]\n",
    "$$\n",
    "\n",
    "これは複数の問題を一般化できる点で便利っぽい．\n",
    "* 明らかにこれは**6**の一般化\n",
    "* また，\n",
    "$$\n",
    "\\eta_h:=\\gamma_c^{-h} \\cdot\\left(\\xi-\\sum_{h^{\\prime}=0}^{h-1} \\gamma_c^{h^{\\prime}} c\\left(s_{h^{\\prime}}, a_{h^{\\prime}}\\right)\\right), \\forall h \\in[H]\n",
    "$$\n",
    "を使って$\\mathbb{P}_\\pi\\left[c\\left(s_h, a_h\\right) \\leq \\eta_h\\right]=1$を考えると，これは**5**と等価っぽいです．\n",
    "\n",
    "**TODO: これは$\\eta_h$が方策に依存してしまうので，等価ではないのでは？各エピソードで方策が変わるとすると，これはNon-stationary MDPを解くことになるかもしれない．**\n",
    "\n",
    "---\n",
    "\n",
    "**8. 他の問題**\n",
    "\n",
    "* [Policy Gradients with Variance Related Risk Criteria](https://arxiv.org/abs/1206.6404)\n",
    "\n",
    "$$\n",
    "\\max _\\pi V_r^\\pi(\\rho) \\quad \\text { s.t. } \\operatorname{Var}\\left[V_r^\\pi(\\rho)\\right]<\\xi \\text {. }\n",
    "$$\n",
    "\n",
    "分散が一定以下になるような制約を課す問題です．これはMean-variance MDPみたいにNP困難になりそうな気がする．\n",
    "\n",
    "後は自然言語を利用するアプローチもあるみたいです．\n",
    "\n",
    "TODO: Shieldingの話\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
