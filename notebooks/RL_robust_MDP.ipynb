{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ロバストMDP\n",
                "\n",
                "参考\n",
                "* [Robust dynamic programming](http://www.corc.ieor.columbia.edu/reports/techreports/tr-2002-07.pdf)\n",
                "* [バナッハの不動点定理](https://mathlandscape.com/banach-fixed-pt/)\n",
                "* [Robust Control of Markov Decision Processes with Uncertain Transition Matrices](https://people.eecs.berkeley.edu/~elghaoui/Pubs/RobMDP_OR2005.pdf)：完全双対性の証明など．こっちのほうがわかりやすいかも？\n",
                "* [First-order Policy Optimization for Robust Markov Decision Process](https://arxiv.org/abs/2209.10579)\n",
                "    * Remark 1.1でなぜ遷移のUncertainty setを考えるのか？みたいな話をしていて良い\n",
                "\n",
                "シミュレータを使ったアプリケーションではしばしば，シミュレータ上の遷移確率$P^o$で方策を学習させてから実世界にデプロイすることを考えます．\n",
                "しかし，実世界の遷移確率は$P^o$に対して必ずズレがあります．\n",
                "ロバストMDPでは，$P^o$からのズレを含めた遷移確率の集合を$\\mathcal{P}$として，$\\mathcal{P}$の中の最悪ケースに対してもある程度の性能を保証するように学習することを目標にします．\n",
                "\n",
                "\n",
                "## ロバストMDPによって性能に差が出る例\n",
                "\n",
                "参考：\n",
                "* [First-order Policy Optimization for Robust Markov Decision Process](https://arxiv.org/abs/2209.10579) 今回はこっちの例を使う\n",
                "* [Sample Complexity of Robust Reinforcement Learning with a Generative Model](https://arxiv.org/abs/2112.01506) こっちも読んでおこう\n",
                "\n",
                "ロバストMDPはいつ重要になるでしょうか？次のようなケースを考えてみましょう：\n",
                "\n",
                "![Robust-MDP](figs/SA-Robust-MDP.png)\n",
                "\n",
                "**(a)のMDP**\n",
                "\n",
                "* 状態$S_0$では２つの行動$\\{L, R\\}$があります．$R$だと報酬$+1$，$L$だと報酬$+0$が発生します．\n",
                "* また，状態$S_{k-1, L}$から$S_{k, L}$の部分で報酬$+(1+\\epsilon) \\gamma^{-k+1}$が発生します．ここで$\\epsilon \\ll 1$です．例えば$\\epsilon=0.01$など．\n",
                "    * つまり，$S_0$においてLを選択すると$1+\\epsilon$のリターン，右に行くと$+1$のリターンが発生します．よって左に行くのが最適解です．\n",
                "\n",
                "**(b)のMDP**\n",
                "\n",
                "* $\\mathbb{P}_u\\left(S_{m+1, L} \\mid S_{m, L}\\right)=p, \\mathbb{P}_u\\left(S_{m, R} \\mid S_{m, L}\\right)=1-p$とします．すなわち，(a)のMDPと遷移確率がほとんど変わりません\n",
                "* $S_{i, R}$に到達するとずっとその場に留まります．\n",
                "* このとき，左に行くとリターンは$(1+\\epsilon)p^{k-1}$であり，これは$k\\to \\infty$でリターンが$0$になります．よって，このMDPでは$S_0$で右に行くのが最適解です．\n",
                "\n",
                "(a)と(b)のMDPでは遷移が少し違うだけですが，その最適方策と得られるリターンが全然違います．\n",
                "また，左右等確率で選択するのは良い方策にはならず，良さそうな解は右を選択する方策です．\n",
                "右を選択する方策は$\\mathcal{M}$では$\\epsilon$-最適方策，そして$\\mathcal{M}_u$では最適方策になります．\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ロバスト動的計画法\n",
                "\n",
                "ロバストMDPの基本的な解き方の一つに，ロバスト動的計画法があります．\n",
                "\n",
                "**表記**\n",
                "\n",
                "* 更新の回数：$K$\n",
                "* エフェクティブホライゾン：$H:=1 / (1 - \\gamma)$\n",
                "* 遷移確率のUncertainty set： $ \\mathcal{P}=\\otimes \\mathcal{P}_{s, a}, \\text { where, } \\mathcal{P}_{s, a}=\\left\\{P_{s, a} \\in[0,1]^{|\\mathcal{S}|}: D\\left(P_{s, a}, P_{s, a}^o\\right) \\leq c_r, \\sum_{s^{\\prime} \\in \\mathcal{S}} P_{s, a}\\left(s^{\\prime}\\right)=1\\right\\}, $\n",
                "    * ここで$D$は分布間の距離です。今回は特にf-ダイバージェンス$D_f(P \\| Q)=\\sum_x Q(x) f\\left(\\frac{P(x)}{Q(x)}\\right)$を考えます。\n",
                "* 遷移確率$P$での価値関数：$V_{\\pi, P}(s)=\\mathbb{E}_{\\pi, P}\\left[\\sum_{t=0}^{\\infty} \\gamma^t r\\left(s_t, a_t\\right) \\mid s_0=s\\right]$\n",
                "* $P$における最適価値関数と最適方策：$V_P^*=\\max _\\pi V_{\\pi, P}, \\quad \\pi_P^*=\\underset{\\pi}{\\arg \\max } V_{\\pi, P}$\n",
                "* ロバストな価値関数と最適価値関数：$V^\\pi=\\inf _{P \\in \\mathcal{P}} V_{\\pi, P}, \\quad V^*=\\sup _\\pi \\inf _{P \\in \\mathcal{P}} V_{\\pi, P}$\n",
                "\n",
                "---\n",
                "\n",
                "**定理：マルコフ最適性**\n",
                "\n",
                "方策集合が全ての方策を含むとき、ロバストな最適価値関数 $V^*=\\sup_{\\pi} \\inf _{P \\in \\mathcal{P}} V_{\\pi, P}$を与える$\\pi$は決定的方策になる。\n",
                "\n",
                "これの証明は通常のマルコフ決定過程とほぼ同じらしい。Puterman(1994)を見よう。\n",
                "\n",
                "---\n",
                "\n",
                "---\n",
                "\n",
                "**完全双対性**\n",
                "\n",
                "$$\n",
                "V^*=\\sup _\\pi \\inf _{P \\in \\mathcal{P}} V_{\\pi, P}\n",
                "=\\inf _{P \\in \\mathcal{P}} \\sup _\\pi V_{\\pi, P}\n",
                "$$\n",
                "\n",
                "が成立．\n",
                "証明は[RL_robust_MDP_zero_duality.ipynb](RL_robust_MDP_zero_duality.ipynb)\n",
                "\n",
                "---\n",
                "\n",
                "---\n",
                "\n",
                "**ベルマン作用素**\n",
                "\n",
                "* 任意のベクトル$v$と集合$\\mathcal{B}$について、$\\sigma_{\\mathcal{B}}(v)=\\inf \\left\\{u^{\\top} v: u \\in \\mathcal{B}\\right\\}$とする。\n",
                "* ロバストベルマン最適作用素：$T(V)(s)=\\max _a(r(s, a)+ + \\gamma \\sigma_{\\mathcal{P}_{s, a}}(V))$\n",
                "\n",
                "ロバストベルマン最適作用素はcontractionになっている。つまり、任意の$U, V \\in \\mathcal{V}$について、\n",
                "\n",
                "$$\\|T U-T V\\| \\leq \\gamma\\|U-V\\|$$\n",
                "\n",
                "であり、また、$T$は唯一の解をもち、それは$V^*$である。\n",
                "\n",
                "前半部分をまず証明します。$TU \\geq TV$を仮定します。このとき、次を満たすように決定的方策$d$を選びます。\n",
                "\n",
                "$$r\\left(s, d(s)\\right)+\\gamma \\inf _{p \\in \\mathcal{P}(s, d(s))} \\mathbf{E}^p\\left[U\\left(s^{\\prime}\\right)\\right] \\geq T U(s)-\\epsilon$$\n",
                "\n",
                "続いて、次を満たすように$p_s \\in \\mathcal{P}(s, d(s))$を選びます。\n",
                "\n",
                "$$r\\left(s, d(s)\\right)+\\gamma \\mathbf{E}^{p_s}\\left[V\\left(s^{\\prime}\\right)\\right] \\leq r\\left(s, d(s)\\right)+\\lambda \\inf _{p \\in \\mathcal{P}(s, d(s))} \\mathbf{E}^p\\left[V\\left(s^{\\prime}\\right)\\right]+\\epsilon$$\n",
                "\n",
                "これを使って変形すると、\n",
                "\n",
                "$$\n",
                "\\begin{aligned}\n",
                "0 \\leq T U(s)- T V(s) & \\leq\\left(r\\left(s, d(s)\\right)+\\gamma \\inf _{p \\in \\mathcal{P}(s, d(s))} \\mathbf{E}^p\\left[U\\left(s^{\\prime}\\right)\\right]+\\epsilon\\right)- \\left(r\\left(s, d(s)\\right)+\\gamma\\inf _{p \\in \\mathcal{P}(s, d(s))} \\mathbf{E}^p\\left[V\\left(s^{\\prime}\\right)\\right]\\right), \\\\\n",
                "& \\leq \\left(r\\left(s, d(s)\\right)+\\gamma \\mathbf{E}^{p_s}\\left[U\\left(s^{\\prime}\\right)\\right]+\\epsilon\\right)-\\left(r\\left(s, d(s)\\right)+\\gamma \\mathbf{E}^{p_s}\\left[V\\left(s^{\\prime}\\right)\\right]-\\epsilon\\right), \\\\\n",
                "& =\\gamma \\mathbf{E}^{p_s}[U-V]+2 \\epsilon, \\\\\n",
                "& \\leq \\gamma \\mathbf{E}^{p_s}|U-V|+2 \\epsilon, \\\\\n",
                "& \\leq \\gamma\\|U-V\\|+2 \\epsilon .\n",
                "\\end{aligned}\n",
                "$$\n",
                "\n",
                "これでcontractionが示せました。唯一の収束点の方は省略しますが、[バナッハの不動点定理](https://mathlandscape.com/banach-fixed-pt/)から不動点の存在は示せます。詳細は[Robust dynamic programming](http://www.corc.ieor.columbia.edu/reports/techreports/tr-2002-07.pdf)参照。\n",
                "\n",
                "---\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### $\\sigma_{\\mathcal{P}_{s, a}}(V)$について\n",
                "\n",
                "ロバスト動的計画法では、$\\sigma_{\\mathcal{P}_{s, a}}(v)=\\inf \\left\\{p^{\\top} v: p \\in \\mathcal{P}_{s, a}\\right\\}$を解く必要があります。これは単純に制約付き最適化を実行すると非常に遅いですが、特定の制約の場合はラグランジュの未定乗数法を使えば効率的に解くことができます。　\n",
                "\n",
                "\n",
                "#### $D$がKLダイバージェンスのとき\n",
                "\n",
                "$D\\left(p_1 \\| p_2\\right)=\\sum_{s \\in \\mathcal{S}} p_1(s) \\log \\left(\\frac{p_1(s)}{p_2(s)}\\right)$の場合を考えましょう。\n",
                "\n",
                "---\n",
                "\n",
                "定義より$\\sigma_{\\mathcal{P}_{s, a}}(v)$は$t > 0$について\n",
                "$$\n",
                "\\begin{array}{ll}\n",
                "\\text { minimize }_p & p^{\\top} v\\\\\n",
                "\\text { subject to } & p \\in \\mathcal{P}=\\{p \\in [0, 1]^{|\\mathcal{S}|}: D(p \\| q) \\leq t, \\sum_{s\\in \\mathcal{S}}p(s)=1\\}\n",
                "\\end{array}\n",
                "$$\n",
                "でした（$t$は$c_\\tau$と、$q$は$P_{s, a}^o$と読み替えてください）。\n",
                "この解は\n",
                "\n",
                "$$\n",
                "-\\min _{\\lambda \\geq 0}t \\lambda+\\lambda \\log \\left(q^T\\exp \\left(-\\frac{v}{\\lambda}\\right)\\right)\n",
                "$$\n",
                "\n",
                "と等価です。\n",
                "また，その解$\\lambda^\\star$を使うと，最悪ケースの遷移は\n",
                "\n",
                "$$\n",
                "p^\\star(s) \\propto q(s) \\exp\\left(\\frac{-v(s)}{\\lambda^\\star}\\right)\n",
                "$$\n",
                "\n",
                "になります．\n",
                "\n",
                "---\n",
                "\n",
                "**証明**\n",
                "\n",
                "まず、制約付き最適化問題のラグランジアンは$\\lambda \\geq 0$と$\\mu \\in \\mathbb{R}$について、\n",
                "\n",
                "$$\\mathcal{L}=\\sum_{s \\in \\mathcal{S}} p(s) v(s)-\\lambda\\left(t-\\sum_{s \\in \\mathcal{S}} p(s) \\log \\left(\\frac{p(s)}{q(s)}\\right)\\right)-\\mu\\left(\\sum_{s \\in \\mathcal{S}} p(s)-1\\right)$$\n",
                "\n",
                "です。これを$p(s)$について微分すれば、\n",
                "\n",
                "$$\n",
                "\\lambda \\log \\left(\\frac{p(s)}{q(s)}\\right)+v(s)=\\mu-\\lambda\n",
                "$$\n",
                "\n",
                "を得ます。これを$p(s)$について解けば\n",
                "\n",
                "$$p(s)=q(s) \\exp \\left(-1+\\frac{\\mu-v(s)}{\\lambda}\\right)$$\n",
                "\n",
                "です。\n",
                "$\\sum_{s \\in \\mathcal{S}} p(s)=1$を使ってラグランジアンを変形すると\n",
                "\n",
                "$$\\mathcal{L}=\\sum_{s \\in \\mathcal{S}} p(s) \\left(v(s) + \\lambda \\log \\left(\\frac{p(s)}{q(s)}\\right) \\right)-\\lambda t\n",
                "=\\mu - \\lambda -\\lambda t\n",
                "$$\n",
                "\n",
                "です。\n",
                "また、$p(s)=q(s) \\exp \\left(-1+\\frac{\\mu-v(s)}{\\lambda}\\right)$と$\\sum_{s \\in \\mathcal{S}} p(s)=1$を合わせると、\n",
                "\n",
                "$$\\exp\\left(\\frac{\\mu-\\lambda}{\\lambda}\\right)\\sum_{s \\in \\mathcal{S}} q(s) \\exp \\left(\\frac{-v(s)}{\\lambda}\\right) = 1$$\n",
                "より、\n",
                "\n",
                "$$\\mu-\\lambda = -\\lambda \\log \\left(\\sum_{s \\in \\mathcal{S}} q(s) \\exp \\left(\\frac{-v(s)}{\\lambda}\\right)\\right)$$\n",
                "\n",
                "なので、$\\mathcal{L}(\\lambda)=-t \\lambda-\\lambda \\log \\left(q^T\\exp \\left(\\frac{v}{\\lambda}\\right)\\right)$\n",
                "を最大化させる$\\lambda \\geq 0$で元の問題が解けます。\n",
                "\n",
                "また，\n",
                "$$p(s)=q(s) \\exp \\left(-1+\\frac{\\mu}{\\lambda}\\right) \\exp\\left(\\frac{-v(s)}{\\lambda}\\right)$$\n",
                "ですが，\n",
                "上の式から\n",
                "\n",
                "$$\\exp\\left(-1 + \\frac{\\mu}{\\lambda}\\right)\\sum_{s \\in \\mathcal{S}} q(s) \\exp \\left(\\frac{-v(s)}{\\lambda}\\right) = 1$$\n",
                "\n",
                "なので，$\\exp\\left(-1 + \\frac{\\mu}{\\lambda}\\right)$は\n",
                "$q(s) \\exp\\left(\\frac{-v(s)}{\\lambda}\\right)$の正規化項です．よって，最適解は\n",
                "\n",
                "$$\n",
                "p(s) \\propto q(s) \\exp\\left(\\frac{-v(s)}{\\lambda}\\right)\n",
                "= \\exp\\left(\\log q(s) - v(s) / \\lambda\\right)\n",
                "$$\n",
                "になります．\n",
                "\n",
                "---\n",
                "\n",
                "ちなみにこの問題を解く計算量は$\\widetilde{\\mathcal{O}}(|\\mathcal{S}|)$になります。（\n",
                "[Robust dynamic programming](http://www.corc.ieor.columbia.edu/reports/techreports/tr-2002-07.pdf)参照）\n",
                "\n",
                "他の距離についても似たようなことができます。（[Robust dynamic programming](http://www.corc.ieor.columbia.edu/reports/techreports/tr-2002-07.pdf)参照）"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import jax.numpy as jnp\n",
                "import jax\n",
                "\n",
                "\n",
                "@jax.jit\n",
                "def compute_policy_matrix(policy: jnp.ndarray):\n",
                "    \"\"\"\n",
                "    Args:\n",
                "        policy (jnp.ndarray): (SxA) array\n",
                "\n",
                "    Returns:\n",
                "        policy_matrix (jnp.ndarray): (SxSA) array\n",
                "    \"\"\"\n",
                "    S, A = policy.shape\n",
                "    PI = policy.reshape(1, S, A)\n",
                "    PI = jnp.tile(PI, (S, 1, 1))\n",
                "    eyes = jnp.eye(S).reshape(S, S, 1)\n",
                "    PI = (eyes * PI).reshape(S, S*A)\n",
                "    return PI\n",
                "\n",
                "\n",
                "@jax.jit\n",
                "def compute_policy_Q(discount: float, policy: jnp.ndarray, rew: jnp.ndarray, P: jnp.ndarray):\n",
                "    \"\"\" Do policy evaluation with cost and transition kernel\n",
                "    Args:\n",
                "        discount (float): discount factor\n",
                "        policy (jnp.ndarray): (SxA) array\n",
                "        rew (jnp.ndarray): rew function. (SxA) array\n",
                "        P (jnp.ndarray): transition kernel. (SxAxS) array\n",
                "\n",
                "    Returns:\n",
                "        Q (jnp.ndarray): (SxA) array\n",
                "    \"\"\"\n",
                "    S, A = policy.shape\n",
                "\n",
                "    Pi = compute_policy_matrix(policy)\n",
                "    PPi = P.reshape(S*A, S) @ Pi\n",
                "    Q = jnp.linalg.inv(jnp.eye(S*A) - discount * PPi) @ rew.reshape(S*A)\n",
                "    return Q.reshape(S, A)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "from jaxopt import ProjectedGradient\n",
                "from functools import partial\n",
                "from jaxopt.projection import projection_non_negative\n",
                "\n",
                "\n",
                "# min_eps = jnp.finfo(jnp.float64).resolution\n",
                "min_eps = 0\n",
                "\n",
                "def worst_PV_loss(lam: float, KL_rad: float, Psa: jnp.ndarray, V: jnp.ndarray):\n",
                "    \"\"\"\n",
                "    min P(s, a) V such that {KL(P(s, a)|Po) <= rad} is equivalent to minimizing this loss function\n",
                "    See Lemma 4 in http://www.corc.ieor.columbia.edu/reports/techreports/tr-2002-07.pdf\n",
                "    \"\"\"\n",
                "    logsumexp = jax.nn.logsumexp(jnp.log(Psa+min_eps) - V / (lam + min_eps))\n",
                "    return KL_rad * lam + lam * logsumexp\n",
                "\n",
                "\n",
                "pg = ProjectedGradient(fun=worst_PV_loss, projection=projection_non_negative)\n",
                "\n",
                "@partial(jax.vmap, in_axes=(None, 0, None), out_axes=0)\n",
                "def compute_worst_P(KL_rad: float, Psa: jnp.ndarray, V: jnp.ndarray):\n",
                "    init_lam = 0.1\n",
                "    lam = pg.run(init_lam, None, KL_rad, Psa, V).params\n",
                "    worst_Psa = Psa * jnp.exp(-V / (lam + min_eps))\n",
                "    return worst_Psa / worst_Psa.sum()\n",
                "\n",
                "\n",
                "@partial(jax.vmap, in_axes=(None, 0, None), out_axes=0)\n",
                "def compute_worst_PV(KL_rad: float, Psa: jnp.ndarray, V: jnp.ndarray):\n",
                "    init_lam = 0.1\n",
                "    lam = pg.run(init_lam, None, KL_rad, Psa, V).params\n",
                "    logsumexp = jax.nn.logsumexp(jnp.log(Psa+min_eps) - V / (lam + min_eps))\n",
                "    worst_PV = KL_rad * lam + lam * logsumexp\n",
                "    return -worst_PV"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "Array([[4.6432018e-04, 6.1482191e-05, 4.7516823e-04],\n",
                            "       [4.4706464e-04, 3.5625696e-04, 1.3157725e-04],\n",
                            "       [2.1320581e-04, 3.6537647e-04, 1.6612113e-03],\n",
                            "       [4.8202276e-04, 3.0684471e-04, 8.8393688e-04],\n",
                            "       [1.0576546e-03, 2.3669004e-04, 3.3855438e-05]], dtype=float32)"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from jax.random import PRNGKey\n",
                "\n",
                "init_lam = 0.3\n",
                "S, A = 5, 3\n",
                "P = jax.random.uniform(PRNGKey(0), shape=(S*A, S))\n",
                "P = P / P.sum(axis=-1, keepdims=True)\n",
                "V = jax.random.uniform(PRNGKey(1), shape=(S,))\n",
                "\n",
                "worst_P = compute_worst_P(0.1, P, V).reshape(S, A, S)\n",
                "worst_PV = compute_worst_PV(0.1, P, V).reshape(S, A)\n",
                "\n",
                "(worst_P @ V - worst_PV)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "Array([[1.        , 1.        , 1.        ],\n",
                            "       [0.99999994, 1.        , 1.        ],\n",
                            "       [0.99999994, 1.        , 1.0000001 ],\n",
                            "       [1.        , 0.99999994, 1.        ],\n",
                            "       [0.9999999 , 1.        , 1.        ]], dtype=float32)"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "worst_P.sum(axis=-1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "policy = jnp.ones((S, A)) / A\n",
                "rew = jax.random.uniform(PRNGKey(2), shape=(S, A))\n",
                "Q = compute_policy_Q(0.99, policy, rew, P)\n",
                "worst_Q = compute_policy_Q(0.99, policy, rew, worst_P)\n",
                "\n",
                "Q - worst_Q"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.9.12 ('shumi-VTLwuKSy-py3.9')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.12"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "d6b7cac5e0d2ff733f340da4d53ae5ecfef7f7ad39623f5982b029a09306b36b"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
