{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 平均報酬におけるUCRL2アルゴリズム\n",
    "\n",
    "参考：\n",
    "* [REGAL: A Regularization based Algorithm for Reinforcement Learning in Weakly Communicating MDPs](https://arxiv.org/abs/1205.2661)\n",
    "\n",
    "Average rewardでSpanを外すアルゴリズムについて見ていきます"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "表記：\n",
    "\n",
    "* $T$ステップまでに得られた期待報酬：$R^{\\mathcal{G}}\\left(s_1, T\\right)=\\mathbb{E}\\left[\\sum_{t=1}^T r_t\\right]$\n",
    "* リグレット：$\\Delta^{\\mathcal{G}}\\left(s_1, T\\right)=\\lambda^{\\star} T-R^{\\mathcal{G}}\\left(s_1, T\\right)$\n",
    "    * $\\lambda^\\star$は最適な報酬．\n",
    "* Ergodic：全ての方策について，マルコフ連鎖が単一のRecurrent classになる（つまり，任意の状態から任意の状態に到達可能）．\n",
    "* Unichain：全ての方策について，マルコフ連鎖が単一のRecurrent classと，（空かもしれない）transientな状態集合になる．\n",
    "* Communicating：任意の$s_1, s_2 \\in \\mathcal{S}$について，$s_1$から$s_2$に遷移可能な**方策が存在する**．\n",
    "* Weakly communicating：$\\mathcal{S}$が２つに分割可能\n",
    "    1. Communicatingな状態集合\n",
    "    2. 任意の方策に対してTransientな状態集合\n",
    "\n",
    "ちなみに，こうした構造がない限り，リグレットの保証をつけることは不可能です．例えば，最適な報酬を得る状態に絶対にいけない状態にエージェントが配置されてしまった場合，無理です．\n",
    "今回はWeakly communicatingを考えます．\n",
    "\n",
    "* Weakly communicatingでは，最適ゲイン$\\lambda^\\star$は状態に非依存です．次の最適方程式を満たします：\n",
    "$$\n",
    "h^{\\star}+\\lambda^{\\star} \\mathbf{e}=\\max _{a \\in \\mathcal{A}}\\left(r(s, a)+P_{s, a}^{\\top} h^{\\star}\\right) = \\mathcal{T} h^{\\star}\n",
    "$$\n",
    "ここで，$h^\\star$はバイアスベクトルです．$h^\\star + c\\boldsymbol{e}$もバイアスベクトルになります．\n",
    "* スパン：$\\operatorname{sp}(h):=\\max _{s \\in \\mathcal{S}} h(s)-\\min _{s \\in \\mathcal{S}} h(s)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REGALアルゴリズム\n",
    "\n",
    "\n",
    "今回見るREGALアルゴリズムは$\\tilde{O}\\left(\\operatorname{sp}\\left(h^{\\star}(M)\\right) S \\sqrt{A T}\\right)$を達成します．\n",
    "\n",
    "---\n",
    "\n",
    "**補足：半径について**\n",
    "\n",
    "REGAL以前のアルゴリズムでは\n",
    "$$\n",
    "\\tilde{O}(D(M) S \\sqrt{A T})\n",
    "$$\n",
    "のリグレットが達成されてきました．ここで，$D(M)$はMDP$M$の直径です．\n",
    "$$\n",
    "D(M):=\\max _{s_1 \\neq s_2} \\min _\\pi T_{s_1 \\rightarrow s_2}^\\pi\n",
    "$$\n",
    "$T_{s_1 \\rightarrow s_2}^\\pi$は$\\pi$が$s_1$から$s_2$へ到達する期待ステップ数を表します．\n",
    "\n",
    "しばしば他の直径が使われることもあります：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "D_{\\text {worst }}(M) & :=\\max _\\pi \\max _{s_1 \\neq s_2} T_{s_1 \\rightarrow s_2}^\\pi \\\\\n",
    "D_{\\text {opt }}(M) & :=\\min _{\\pi \\in \\mathcal{O}} \\max _{s_1 \\neq s_2} T_{s_1 \\rightarrow s_2}^\\pi\n",
    "\\end{aligned}\n",
    "$$\n",
    "明らかに，$D \\leq D_\\text{opt} \\leq D_\\text{worst}$です．\n",
    "今回は次の片道直径を使います：\n",
    "\n",
    "$$\n",
    "D_{\\mathrm{ow}}(M):=\\max _s \\min _\\pi T_{s_1 \\rightarrow \\bar{s}}^\\pi\n",
    "$$\n",
    "\n",
    "ここで，$\\bar{s}=\\operatorname{argmax}_s h^{\\star}(s)$です．[RL_AverageReward_lemma.ipynb](RL_useful_lemma/RL_AverageReward_lemma.ipynb)で示しますが，$\\operatorname{sp}\\left(h^{\\star}\\right) \\leq D_{\\text {ow }} \\leq D$が成り立ちます．\n",
    "実はこの片道直径を使うと，今回のバウンドがタイトであることが示せます（Theorem 6）．\n",
    "\n",
    "---\n",
    "\n",
    "REGALアルゴリズムは，MDPのConfidence setから\n",
    "$$\n",
    "\\lambda^{\\star}(M)-C_k \\operatorname{sp}\\left(h^{\\star}(M)\\right)\n",
    "$$\n",
    "が最大になるようなMDP$M$を見つけます．つまり，最適ゲインを大きく，そしてスパンを小さめに抑えるMDPを選択します．\n",
    "\n",
    "各エピソード$k=1, 2, \\dots$において，\n",
    "1. $t_k$は現在の時刻です\n",
    "2. $\\mathcal{M}^k$を$\\left\\|P_{s, a}-\\hat{P}_{s, a}^t\\right\\|_1 \\leq \\sqrt{\\frac{12 S \\log (2 A t / \\delta)}{\\max \\{N(s, a, t), 1\\}}}$を満たすconfidence setとします\n",
    "3. $\\lambda^{\\star}(M)-C_k \\operatorname{sp}\\left(h^{\\star}(M)\\right)$を最大にするMDP$M^k$を選びます\n",
    "4. $\\pi^k$を$M^k$のゲイン最適方策とします\n",
    "5. どれかの$s, a$が$N_k(s, a)$回訪問されるまで$\\pi^k$を実行します．\n",
    "\n",
    "$v_k(s, a)=N_{k+1}(s, a)-N_k(s, a)$を$k$エピソード目に$(s, a)$が訪問された回数とします．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### リグレット解析\n",
    "\n",
    "* $\\ell_k=\\sum_{s, a} v_k(s, a)$：エピソード$k$の長さ\n",
    "* $\\lambda\\left(M^k\\right), h^{\\star}\\left(M^k\\right)$を$\\lambda_k$，$h_k^\\star$とします．\n",
    "* $\\tilde{P}_k$は$M^k$での遷移確率．$P_k$は$M$での遷移確率とします．\n",
    "* $h^\\star$と$h^\\star_k$はそれぞれ\n",
    "$$\n",
    "\\operatorname{sp}\\left(h^{\\star}\\right)=\\left\\|h^{\\star}\\right\\|_{\\infty} \\text { and } \\operatorname{sp}\\left(h_k^{\\star}\\right)=\\left\\|h_k^{\\star}\\right\\|_{\\infty}\n",
    "$$\n",
    "が満たされるように正規化されているとします．\n",
    "$$\\begin{aligned} & \\mathbf{v}_k(s)=v_k\\left(s, \\pi^k(s)\\right) \\\\ & \\mathbf{r}_k(s)=r\\left(s, \\pi^k(s)\\right)\\end{aligned}$$\n",
    "なるベクトルを導入します．このとき，\n",
    "$$\n",
    "h_k^{\\star}+\\lambda_k^{\\star} \\mathbf{e}=\\mathbf{r}_k+\\tilde{\\mathbf{P}}_k h_k^{\\star}\n",
    "$$\n",
    "です．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**エピソード数のバウンド**\n",
    "\n",
    "まず，次の定理が成立します：\n",
    "\n",
    "$T \\geq $SA$ステップ目までのエピソードの数$m$は,\n",
    "$$\n",
    "m \\leq S A \\log _2\\left(\\frac{8 T}{S A}\\right)\n",
    "$$\n",
    "でバウンドされる．\n",
    "\n",
    "証明は[RL_useful_lemma/RL_AverageReward_lemma.ipynb](RL_useful_lemma/RL_AverageReward_lemma.ipynb)参照．\n",
    "\n",
    "---\n",
    "\n",
    "**リグレットのバウンドの書き換え**\n",
    "\n",
    "$$\\Delta_k=\\sum_{s, a} v_k(s, a)\\left[\\lambda^{\\star}-r(s, a)\\right]$$\n",
    "を$k$エピソード内のリグレットとします．このとき，リグレットは\n",
    "$$\n",
    "\\sum_{k \\in G} \\Delta_k+\\sum_{k \\in B} \\Delta_k\n",
    "$$\n",
    "と同じです．ここで\n",
    "$$\n",
    "G=\\left\\{k: M \\in \\mathcal{M}^k\\right\\} \\text { and } B=\\{1, \\ldots, m\\}-G\n",
    "$$\n",
    "としました．つまり，$G$はGood event, $B$はBAD eventです．\n",
    "\n",
    "このとき，次が成立します（つまりBAD eventはあまり起きません）：\n",
    "\n",
    "確率 $1- \\delta$以上で，\n",
    "$$\n",
    "\\sum_{k \\in R} \\Delta_k \\leq \\sqrt{T}\n",
    "$$\n",
    "\n",
    "TODO:証明\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
