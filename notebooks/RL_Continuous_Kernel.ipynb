{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# カーネルを使った強化学習\n",
    "\n",
    "参考：\n",
    "* [Kernel-Based Reinforcement Learning: A Finite-Time Analysis](https://arxiv.org/abs/2004.05599)\n",
    "\n",
    "状態行動空間が連続のMDPではどのようにRLすればよいでしょうか？\n",
    "今回はカーネルを使った強化学習を見てみます．\n",
    "\n",
    "表記：\n",
    "* $(x, a)$に定義された測度$P$と任意の関数$f$について，$P f(x, a)=P(\\cdot \\mid x, a) f=\\int f(y) \\mathrm{d} P(y \\mid x, a)$\n",
    "* $\\mathcal{X}$と$\\mathcal{A}$上に定義された測度：$\\rho:(\\mathcal{X} \\times \\mathcal{A})^2 \\rightarrow \\mathbb{R}_{\\geq 0}$\n",
    "* $\\mathcal{X}$上の測度空間：$\\left(\\mathcal{X}, \\mathcal{T}_{\\mathcal{X}}\\right)$．$\\mathcal{T}_{\\mathcal{X}}$はσ代数です．\n",
    "* 報酬関数：$\\left\\{r_h\\right\\}_{h \\in[H]}$は$\\mathcal{X} \\times \\mathcal{A}$ to $[0,1]$の集合\n",
    "* 遷移カーネル：$P=\\left\\{P_h\\right\\}_{h \\in[H]}$．ここで，$P_h$は$(\\mathcal{X} \\times \\mathcal{A}) \\times \\mathcal{T}_{\\mathcal{X}}$ to $\\mathbb{R}_{\\geq 0}$なるカーネル．\n",
    "* ベルマン方程式：$Q_h^*(x, a) \\stackrel{\\text { def }}{=} r_h(x, a)+\\int_{\\mathcal{X}} V_{h+1}^*(y) \\mathrm{d} P_h(y \\mid x, a)$\n",
    "* リグレット：$\\mathcal{R}(K) \\stackrel{\\text { def }}{=} \\sum_{k=1}^K\\left(V_1^*\\left(x_1^k\\right)-V_1^{\\pi_k}\\left(x_1^k\\right)\\right)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仮定：\n",
    "\n",
    "---\n",
    "\n",
    "* $(\\mathcal{X}\\times \\mathcal{A})\\times (\\mathcal{X}\\times \\mathcal{A})$上の測度$\\rho$は既知であるとする．\n",
    "* 任意の$\\left(x, x^{\\prime}, a, a^{\\prime}\\right)$について，\n",
    "$$\n",
    "\\rho\\left[(x, a),\\left(x^{\\prime}, a^{\\prime}\\right)\\right]=\\rho_{\\mathcal{X}}\\left(x, x^{\\prime}\\right)+\\rho_{\\mathcal{A}}\\left(a, a^{\\prime}\\right)\n",
    "$$\n",
    "が成り立つような測度$\\rho_{\\mathcal{X}}$と$\\rho_{\\mathcal{A}}$が存在する．\n",
    "\n",
    "---\n",
    "\n",
    "報酬と遷移は次の意味でリプシッツ連続ですL\n",
    "\n",
    "$$\n",
    "\\left|r_h(x, a)-r_h\\left(x^{\\prime}, a^{\\prime}\\right)\\right| \\leq \\lambda_r \\rho\\left[(x, a),\\left(x^{\\prime}, a^{\\prime}\\right)\\right]\n",
    "$$\n",
    "\n",
    "および\n",
    "\n",
    "$$\n",
    "W_1\\left(P_h(\\cdot \\mid x, a), P_h\\left(\\cdot \\mid x^{\\prime}, a^{\\prime}\\right)\\right) \\leq \\lambda_p \\rho\\left[(x, a),\\left(x^{\\prime}, a^{\\prime}\\right)\\right]\n",
    "$$\n",
    "\n",
    "ここで，$W_1$は1-Wasserstein距離です．つまり，$W_1(\\mu, \\nu) \\stackrel{\\text { def }}{=}\\sup _{f: \\operatorname{Lip}(f) \\leq 1} \\int_{\\mathcal{X}} f(y)(\\mathrm{d} \\mu(y)-\\mathrm{d} \\nu(y))$\n",
    "\n",
    "---\n",
    "\n",
    "このようなリプシッツ連続な報酬と遷移のもとでは最適Q値がリプシッツ連続になります．\n",
    "\n",
    "---\n",
    "\n",
    "**補題**\n",
    "\n",
    "$L_h \\stackrel{\\text { def }}{=} \\sum_{h^{\\prime}=h}^H \\lambda_r \\lambda_p^{H-h^{\\prime}}$\n",
    "とする．このとき，\n",
    "\n",
    "$$\n",
    "\\left|Q_h^*(x, a)-Q_h^*\\left(x^{\\prime}, a^{\\prime}\\right)\\right| \\leq L_h \\rho\\left[(x, a),\\left(x^{\\prime}, a^{\\prime}\\right)\\right]\n",
    "$$\n",
    "\n",
    "が成立します．証明は簡単なので省略．\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## カーネルUCBVI\n",
    "\n",
    "$u, v \\in \\mathcal{X} \\times \\mathcal{A}$とします．関数$g: \\mathbb{R}_{\\geq 0} \\rightarrow[0,1]$について，カーネル関数を\n",
    "\n",
    "$$\\psi_\\sigma(u, v) \\stackrel{\\text { def }}{=} g(\\rho[u, v] / \\sigma)$$\n",
    "\n",
    "として定めます．\n",
    "\n",
    "---\n",
    "\n",
    "**仮定**\n",
    "\n",
    "関数$g: \\mathbb{R}_{\\geq 0} \\rightarrow[0,1]$は微分可能，non-increasing，$g(4) > 0$を満たし，また，次を満たす定数$C^g_1, C^g_2$が存在します：\n",
    "\n",
    "$$\n",
    "g(z) \\leq C_1^g \\exp \\left(-z^2 / 2\\right) \\text { and } \\sup _z\\left|g^{\\prime}(z)\\right| \\leq C_2^g\n",
    "$$\n",
    "\n",
    "例えば$g(z)=\\exp \\left(-z^2 / 2\\right)$のようなガウスカーネルが仮定を満たしています．\n",
    "\n",
    "---\n",
    "\n",
    "カーネルUCBVIではカーネル関数を使って過去のデータを重み付けし，報酬や遷移確率を推定します．\n",
    "\n",
    "$(x, a)$ and $(s, h) \\in[K] \\times[H]$に対して，重み関数を\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& w_h^s(x, a) \\stackrel{\\text { def }}{=} \\psi_\\sigma\\left((x, a),\\left(x_h^s, a_h^s\\right)\\right) \\quad \\text { and } \\\\\n",
    "& \\widetilde{w}_h^s(x, a) \\stackrel{\\text { def }}{=} \\frac{w_h^s(x, a)}{\\beta+\\sum_{l=1}^{k-1} w_h^l(x, a)},\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "とします．ここで$\\beta > 0$は正則化項です．この重みを使って，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\widehat{r}_h^k(x, a) \\stackrel{\\text { def }}{=} \\sum_{s=1}^{k-1} \\widetilde{w}_h^s(x, a) r_h^s, \\\\\n",
    "& \\widehat{P}_h^k(y \\mid x, a) \\stackrel{\\text { def }}{=} \\sum_{s=1}^{k-1} \\widetilde{w}_h^s(x, a) \\delta_{x_{h+1}^s}(y) .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "のように報酬と遷移を計算します．\n",
    "そして，推定した報酬と遷移を使って\n",
    "\n",
    "$$\n",
    "\\widetilde{Q}_h^k(x, a)=\\widehat{r}_h^k(x, a)+\\widehat{P}_h^k V_{h+1}^k(x, a)+\\mathrm{B}_h^k(x, a)\n",
    "$$\n",
    "\n",
    "のようにQ値を計算します．ここで$B$は探索用のボーナスです．\n",
    "これを使って，\n",
    "\n",
    "$$\n",
    "Q_h^k(x, a) \\stackrel{\\text { def }}{=} \\min _{s \\in[k-1]}\\left(\\widetilde{Q}_h^k\\left(x_h^s, a_h^s\\right)+L_h \\rho\\left[(x, a),\\left(x_h^s, a_h^s\\right)\\right]\\right)\n",
    "$$\n",
    "\n",
    "のようにしてQ値の間隔を補完します．\n",
    "\n",
    "---\n",
    "\n",
    "**ボーナスについて**\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{C}_h^k(x, a) \\stackrel{\\text { def }}{=} \\beta+\\sum_{s=1}^{k-1} w_h^s(x, a)\n",
    "$$\n",
    "\n",
    "を訪問回数の一般化とします．これを使って，ボーナスを\n",
    "\n",
    "$$\n",
    "\\mathrm{B}_h^k(x, a) \\approx \\frac{H}{\\sqrt{\\mathbf{C}_h^k(x, a)}}+\\frac{\\beta H}{\\mathbf{C}_h^k(x, a)}+L_1 \\sigma\n",
    "$$\n",
    "\n",
    "として定義します．\n",
    "\n",
    "以上をまとめて，Kernel-UCBVIは次の処理を行います：\n",
    "\n",
    "![KUCBVI](figs/Kernel-UCBVI-OptimisticQ.png)\n",
    "![KUCBVI](figs/Kernel-UCBVI.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "元論文はrlberryのTwinRoomで実験してますが，似たような簡単な環境を作ります．\n",
    "\n",
    "**実装についてのコメント**\n",
    "論文の実装では結局状態空間を離散化してる（representative stateってやつ？）[https://github.com/omardrwch/kernel_ucbvi_experiments/blob/main/algorithms/rs_kernelucbvi.py](https://github.com/omardrwch/kernel_ucbvi_experiments/blob/main/algorithms/rs_kernelucbvi.py)\n",
    "\n",
    "結局やってることはテーブルMDPと変わらないかも．\n",
    "一旦実装は保留しよう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import NamedTuple, Optional, Callable\n",
    "\n",
    "# [0, 0] ~ [1, 1]の部屋を動き回る環境を考えます\n",
    "# [-0.1, -0.1] ~ [0.1, 0.1]の行動を考えます\n",
    "S_low = np.array([0, 0])\n",
    "S_high = np.array([1, 1])\n",
    "A_low = np.array([-0.1, -0.1])\n",
    "A_high = np.array([0.1, 0.1])\n",
    "\n",
    "\n",
    "# 右上の方にいくと報酬が発生します\n",
    "def rew_fn(s, a):\n",
    "    if 0.8 <= s[0] <= 0.9 and 0.8 <= s[1] <= 0.9:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "# 指定された行動のベクトルの方向に移動します\n",
    "def tran_fn(s, a):\n",
    "    noise = np.random.randn(2) * 0.01\n",
    "    next_s = s + a + noise\n",
    "    return next_s\n",
    "\n",
    "# 初期状態です．固定します．\n",
    "init_s = np.array([0.1, 0.1])\n",
    "\n",
    "# 状態集合, 行動集合, 割引率, 報酬行列, 遷移確率行列が準備できたのでMDPのクラスを作ります\n",
    "\n",
    "class MDP(NamedTuple):\n",
    "    S_low: np.ndarray\n",
    "    S_high: np.ndarray\n",
    "    A_low: np.ndarray\n",
    "    A_high: np.ndarray\n",
    "    H: int  # ホライゾン\n",
    "    rew_fn: Callable\n",
    "    tran_fn: Callable\n",
    "    init_s: np.ndarray  # 初期分布\n",
    "\n",
    "\n",
    "H = 30\n",
    "mdp = MDP(S_low, S_high, A_low, A_high, H, rew_fn, tran_fn, init_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 7735.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def rho_s(s1, s2):\n",
    "    return np.linalg.norm(s1 - s2)\n",
    "\n",
    "\n",
    "def rho_a(a1, a2):\n",
    "    return np.linalg.norm(a1 - a2)\n",
    "\n",
    "\n",
    "def rho(sa1, sa2):\n",
    "    s1, a1 = sa1\n",
    "    s2, a2 = sa2\n",
    "    return rho_s(s1, s2) + rho_a(a1, a2)\n",
    "\n",
    "\n",
    "def kernel(sa1, sa2, sigma=0.1):\n",
    "    # gaussianカーネルです\n",
    "    z = rho(sa1, sa2) / sigma\n",
    "    return np.exp(-z**2 / 2)\n",
    "\n",
    "\n",
    "def compute_greedy_action(k, Ds, Da, Dns, Drew):\n",
    "    for h in range(H):\n",
    "        for m in range(k):\n",
    "            Q\n",
    "\n",
    "def KernelUCBVI(mdp: MDP, K: int):\n",
    "    returns = []\n",
    "    for k in tqdm(range(K)):\n",
    "        s = mdp.init_s\n",
    "        Ds = np.zeros(shape=(K, mdp.H, 2))  # state data\n",
    "        Da = np.zeros(shape=(K, mdp.H, 2))  # action data\n",
    "        Dns = np.zeros(shape=(K, mdp.H, 2))  # next state data\n",
    "        Drew = np.zeros(shape=(K, mdp.H, 1))  # reward data\n",
    "\n",
    "        # Q = optimistic_Q(...)\n",
    "        ret = 0\n",
    "        for h in range(H):\n",
    "            a = compute_greedy_action(s, k, Ds, Da, Dns, Drew)\n",
    "            a = np.random.rand(2)\n",
    "            next_s = mdp.tran_fn(s, a)\n",
    "            rew = mdp.rew_fn(s, a)\n",
    "            Ds[k, h] = s\n",
    "            Da[k, h] = a\n",
    "            Dns[k, h] = next_s\n",
    "            Drew[k, h] = rew\n",
    "            s = next_s\n",
    "            ret += rew\n",
    "\n",
    "        returns.append(ret)\n",
    "\n",
    "    return returns\n",
    "\n",
    "K = 1000\n",
    "returns = KernelUCBVI(mdp, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リグレット解析\n",
    "\n",
    "\n",
    "このアルゴリズムについて，次のリグレットバウンドが成立します：\n",
    "\n",
    "$$\n",
    "\\mathcal{R}(K) \\leq \\widetilde{\\mathcal{O}}\\left(H^2 \\sqrt{\\left|\\mathcal{C}_\\sigma\\right| K}+L_1 K H \\sigma+H^3\\left|\\mathcal{C}_\\sigma\\right|\\left|\\widetilde{\\mathcal{C}}_\\sigma\\right|\\right)\n",
    "$$\n",
    "\n",
    "ここで，$\\left|\\mathcal{C}_\\sigma\\right|$ と $\\left|\\widetilde{\\mathcal{C}}_\\sigma\\right|$は$\\left(\\mathcal{X}\\times \\mathcal{A}, \\rho\\right)$と$\\left(\\mathcal{X}, \\rho_{\\mathcal{X}}\\right)$の$\\sigma$-coveringです．\n",
    "また，$L_1$は最適Q値のリプシッツ定数です．\n",
    "\n",
    "---\n",
    "\n",
    "**証明**\n",
    "\n",
    "$\\widehat{P}_h^k(\\cdot \\mid x, a)$は今回はディラック測度の重み付き和担ってるので，$P_h(\\cdot\\mid x, a)$と$\\widehat{P}_h^k(\\cdot \\mid x, a)$の距離をバウンドするのは難しいです．そこで，次の変形をします：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\left|\\left(\\widehat{P}_h^k-P_h\\right) V_{h+1}^*(x, a)\\right| \\\\\n",
    "& =\\left|\\sum_{s=1}^{k-1} \\widetilde{w}_h^s(x, a) V_{h+1}^*\\left(x_{h+1}^s\\right)-P_h V_{h+1}^*(x, a)\\right| \\\\\n",
    "& \\leq \\underbrace{\\left|\\sum_{s=1}^{k-1} \\widetilde{w}_h^s(x, a)\\left(V_{h+1}^*\\left(x_{h+1}^s\\right)-P_h V_{h+1}^*\\left(x_h^s, a_h^s\\right)\\right)\\right|}_{(\\mathbf{A})} \\\\\n",
    "& +\\underbrace{\\lambda_p L_{h+1} \\sum_{s=1}^{k-1} \\widetilde{w}_h^s(x, a) \\rho\\left[(x, a),\\left(x_h^s, a_h^s\\right)\\right]}_{(\\mathbf{B})}+\\underbrace{\\frac{\\beta\\left\\|V_{h+1}^*\\right\\|_{\\infty}}{\\mathbf{C}_h^k(x, a)}}_{(\\mathbf{C})} .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Aの部分はマルチンゲール差分列であり，特殊なHoeffdingでバウンドできます.\n",
    "BはLipschitz性を利用したことによるバイアスです．\n",
    "Cは$\\beta$のせいで出てくるバウンドです．\n",
    "ゴニョゴニョすると，結局\n",
    "\n",
    "$$\n",
    "\\left|\\left(\\widehat{P}_h^k-P_h\\right) V_{h+1}^*(x, a)\\right| \\lesssim \\frac{H}{\\sqrt{\\mathbf{C}_h^k(x, a)}}+\\frac{\\beta H}{\\mathbf{C}_h^k(x, a)}+L_1 \\sigma .\n",
    "$$\n",
    "\n",
    "が出てきます．あとは通常のOptimismを使った証明とだいたい同じです（多分）．"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
