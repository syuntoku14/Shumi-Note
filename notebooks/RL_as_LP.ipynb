{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 強化学習と線型計画法\n",
    "\n",
    "* 参考文献: [強化学習 (機械学習プロフェッショナルシリーズ) , 66ページ付近　](https://www.amazon.co.jp/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%97%E3%83%AD%E3%83%95%E3%82%A7%E3%83%83%E3%82%B7%E3%83%A7%E3%83%8A%E3%83%AB%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA-%E6%A3%AE%E6%9D%91-%E5%93%B2%E9%83%8E/dp/4065155916/ref=asc_df_4065155916/?tag=jpgo-22&linkCode=df0&hvadid=310429813636&hvpos=&hvnetw=g&hvrand=6867034787001615408&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=1009224&hvtargid=pla-675730625220&psc=1&th=1&psc=1)\n",
    "* [Reinforcement Learning via Fenchel-Rockafellar Duality](https://arxiv.org/abs/2001.01866)\n",
    "\n",
    "強化学習が扱う最適方策の導出（プランニング問題）は線形計画問題としても定式化できます。\n",
    "線形計画問題の双対問題を考えることで、通常よりも効率の良い解法が見つかる場合があります（ロバストMDPの解析などでも出てきます）。\n",
    "\n",
    "**表記**（[RL_utils.ipynb](RL_utils.ipynb)参照）\n",
    "\n",
    "MDPを次で定義します。\n",
    "\n",
    "1. 有限状態集合: $S=\\{1, \\dots, |S|\\}$\n",
    "2. 有限行動集合: $A=\\{1, \\dots, |A|\\}$\n",
    "3. 遷移確率行列: $P\\in \\mathbb{R}^{SA\\times S}$\n",
    "4. 報酬行列: $r\\in \\mathbb{R}^{S\\times A}$\n",
    "5. 割引率: $\\gamma \\in [0, 1)$\n",
    "6. 初期状態: $\\mu \\in \\mathbb{R}^{S}$\n",
    "\n",
    "* 内積の記法: $f_1, f_2 \\in \\mathbb{R}^{S\\times A}$に対して、$\\langle f_1, f_2 \\rangle = (\\sum_{a\\in A} f_1(s, a)f_2(s, a))_s \\in \\mathbb{R}^S$とします。これは方策についての和を省略するときなどに便利です。例えば$\\langle \\pi, q_\\pi\\rangle = v_\\pi$です。\n",
    "* 方策行列（$\\Pi^\\pi \\in \\mathbb{R}^{S\\times SA}$）：$\\langle \\pi, q\\rangle$を行列で書きたいときに便利。\n",
    "    * $\\Pi^\\pi(s,(s, a))=\\pi(a \\mid s)$ \n",
    "    * $\\Pi^\\pi q^\\pi = \\langle \\pi, q^\\pi \\rangle = v^\\pi$が成立。\n",
    "* 遷移確率行列（$P^\\pi \\in \\mathbb{R}^{SA\\times SA}$）: 次の状態についての方策の情報を追加したやつ。\n",
    "    * $P^\\pi = P \\Pi^\\pi$\n",
    "    * Q値を使ったベルマン期待作用素とかで便利。$q^\\pi = r + \\gamma P^\\pi q^\\pi$が成立。\n",
    "    * $(I - \\gamma P^\\pi)^{-1}r = q^\\pi$が成立する。\n",
    "* 割引訪問頻度（$d^\\pi_\\mu \\in \\mathbb{R}^{SA}$）：S, Aについての割引累積訪問頻度\n",
    "    * ${d}^\\pi_\\mu (s, a) = \\pi(a|s) \\sum_{s_0} \\mu(s_0) \\sum_{t=0}^\\infty \\mathrm{Pr}\\left(S_t=s|S_0=s_0, M(\\pi)\\right)$がで定義される。\n",
    "    * $d^\\pi_\\mu = \\mu \\Pi^\\pi (I - \\gamma P^\\pi)^{-1} = \\mu (I - \\gamma \\bar{P}^\\pi)^{-1} \\Pi^\\pi$が成立。\n",
    "    * $d^\\pi_\\mu = \\mu \\Pi^\\pi + \\gamma d^\\pi_\\mu P^\\pi$が成立。動的計画法のように解ける。\n",
    "* 初期状態からの方策の価値：$\\rho(\\pi)= \\mu \\Pi^\\pi Q^\\pi=d^\\pi_\\mu r$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 線型計画法による最適価値関数の計算\n",
    "\n",
    "参考：\n",
    "* [On the convex formulations of robust Markov decision processes](https://arxiv.org/abs/2209.10187)\n",
    "\n",
    "線型計画法による最適価値関数の計算のためには，次の定理が便利です．\n",
    "\n",
    "---\n",
    "\n",
    "**Contraction Lemma**\n",
    "\n",
    "$F: \\mathbb{R}^S \\to \\mathbb{R}^S$を単調な縮小作用素（Contraction operator）とし，\n",
    "$g: \\mathbb{R}^S \\to \\mathbb{R}$を非減少な関数とします．\n",
    "また，$v^\\star$を$F$の不動点とします．\n",
    "このとき，\n",
    "\n",
    "$$\n",
    "g\\left(\\boldsymbol{v}^{\\star}\\right)=\\min \\{g(\\boldsymbol{v}) \\mid \\boldsymbol{v} \\geq F(\\boldsymbol{v})\\}=\\max \\{g(\\boldsymbol{v}) \\mid \\boldsymbol{v} \\leq F(\\boldsymbol{v})\\}\n",
    "$$\n",
    "\n",
    "が成り立ちます．\n",
    "また，$g$が単調増加であれば，$v^\\star$は唯一存在します．\n",
    "\n",
    "\n",
    "証明しましょう．\n",
    "\n",
    "**$g\\left(\\boldsymbol{v}^{\\star}\\right)=\\min \\{g(\\boldsymbol{v}) \\mid \\boldsymbol{v} \\geq F(\\boldsymbol{v})\\}$の証明**\n",
    "\n",
    "$v \\geq F(x)$なる$v$を考えます．このとき，$F$は単調なので，$v\\geq F^k(v)$です．\n",
    "しかし，$F^k(v)\\to v^\\star$なので，任意の$v\\in \\mathbb{R}^S$について，$v\\geq F(v) \\Rightarrow v \\geq v^\\star$です．\n",
    "また，$g$は非減少なので，$g(v)\\geq g(v^\\star)$です．\n",
    "ここで，$v^\\star \\geq F(v^\\star)=v^\\star$であるから，\n",
    "$g\\left(\\boldsymbol{v}^{\\star}\\right)=\\min \\{g(\\boldsymbol{v}) \\mid \\boldsymbol{v} \\geq F(\\boldsymbol{v})\\}$です．\n",
    "\n",
    "また，$g$が単調増加であるときを考えましょう．\n",
    "$v'\\leq F(v')$かつ$g(v')=g(v^\\star)$である$v'\\in \\mathbb{R}^S$を考えます．\n",
    "$v'\\leq F(v')$であるから，$v'\\leq v^\\star$です．\n",
    "ここで，$v'< v^\\star$である$v'$が存在するならば，$g(v')<g(v^\\star)$になってしまいます．よって，$v'=v^\\star$です．\n",
    "\n",
    "$\\max$の方は同様にして証明できます．\n",
    "\n",
    "---\n",
    "\n",
    "線形計画問題の導出のために、価値関数についての上界と下界の定理を見てみましょう。\n",
    "\n",
    "---\n",
    "\n",
    "**補題：価値関数の上界と下界**\n",
    "\n",
    "最適価値関数の上界と下界には次の関係が成立します\n",
    "\n",
    "* 上界：$v \\geq (B v)$のとき、$v \\geq v^*$\n",
    "* 下界：$v \\leq (B v)$のとき、$v \\leq v^*$\n",
    "\n",
    "証明は簡単です。ベルマン作用素の単調性から、\n",
    "\n",
    "$$v \\geq (B v)  \\geq (B^k v)  \\geq \\dots \\geq v^*$$\n",
    "\n",
    "が成り立ちます。下界も同様です。\n",
    "また、これは期待ベルマン作用素についても成立します。\n",
    "\n",
    "* 上界：$v \\geq (B_\\pi v) $のとき、$v \\geq v^\\pi$\n",
    "* 下界：$v \\leq (B_\\pi v) $のとき、$v \\leq v^\\pi$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "ここで，Contraction Lemmaの$F$としてベルマン（最適）作用素を考えると，$F$の要件である単調な縮小作用素であることを満たします．\n",
    "また，$g$として割引累積報酬和$\\mu \\Pi^\\pi q$を考えれば，これは単調増加な関数です．よって，次の主問題はベルマン（最適）作用素の不動点を返します．\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\rho(\\pi)=\\min _q & \\;\\mu \\Pi^\\pi q \\\\\n",
    "\\text { s.t. } & \\;q \\geq r+\\gamma P^\\pi q\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**別の解釈**\n",
    "\n",
    "方策反復法などで出てきますが，任意の方策$\\pi$について，次が成立します．（青本の2.9など参照）\n",
    "\n",
    "$$v^\\pi \\leq B v^\\pi$$\n",
    "\n",
    "つまり，$\\{v^\\pi \\mid \\pi \\in \\Pi\\} \\subseteq \\{v \\mid v \\leq B v\\} $\n",
    "\n",
    "が成立します．一方で，任意の$v \\in \\{v \\mid v \\leq B v\\}$について$v \\leq v^*$であり，また，$v^* \\in \\{v^\\pi \\mid \\pi \\in \\Pi\\}$なので，後者の集合の内部でのMaximumを考えると，最適価値関数が獲得できます．\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方策評価の主問題と双対問題\n",
    "\n",
    "参考：[Reinforcement Learning via Fenchel-Rockafellar Duality](https://arxiv.org/abs/2001.01866)\n",
    "\n",
    "初期状態についての方策の評価を考えます。主問題として次の形式を考えましょう。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\rho(\\pi)=\\min _q & \\;\\mu \\Pi^\\pi q \\\\\n",
    "\\text { s.t. } & \\;q \\geq r+\\gamma P^\\pi q\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "変形して、\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\rho(\\pi)=\\min _q & \\;\\mu \\Pi^\\pi q \\\\\n",
    "\\text { s.t. } & \\; (I - \\gamma P^\\pi) q - r \\geq 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "ここで、[CVX_convex_functions.ipynb](CVX_coFenchel-Rockas.ipynb)の線形計画問題における変換を思い出しましょう。\n",
    "線形計画問題については次のような変形が成り立ちます：\n",
    "\n",
    "---\n",
    "\n",
    "**ラグランジアンによる変形**\n",
    "\n",
    "上の方策評価の問題は、$b= \\mu\\Pi^\\pi$、$A = (I - \\gamma P^\\pi)$、$c=r$とおけば、\n",
    "$$\n",
    "\\min _y \\langle b, y \\rangle \\text { s.t. } A y \\geq c\n",
    "$$\n",
    "と等価です。よって、ラグランジアンによる変形を考えると、この問題は\n",
    "\n",
    "$$\n",
    "\\max _{x \\geq 0}\\langle c, x\\rangle \\quad \\text { s.t. }\\quad A_* x=b\n",
    "$$\n",
    "\n",
    "と等価です。\n",
    "\n",
    "---\n",
    "\n",
    "よって、上の方策評価の問題は\n",
    "\n",
    "$$\n",
    "\\rho(\\pi) = \n",
    "\\max _{d \\geq 0}\\langle r, d\\rangle \\quad \\text { s.t. }\\quad d(I - \\gamma P^\\pi)=\\mu\\Pi^\\pi\n",
    "$$\n",
    "であり、変形して\n",
    "\n",
    "$$\n",
    "\\rho(\\pi) = \n",
    "\\max _{d \\geq 0}\\langle r, d\\rangle \\quad \\text { s.t. }\\quad d = \\mu\\Pi^\\pi + \\gamma d P^\\pi\n",
    "$$\n",
    "\n",
    "が成立します。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最適価値関数の主問題と双対問題\n",
    "\n",
    "* 参考文献: [強化学習 (機械学習プロフェッショナルシリーズ) , 66ページ付近　](https://www.amazon.co.jp/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%97%E3%83%AD%E3%83%95%E3%82%A7%E3%83%83%E3%82%B7%E3%83%A7%E3%83%8A%E3%83%AB%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA-%E6%A3%AE%E6%9D%91-%E5%93%B2%E9%83%8E/dp/4065155916/ref=asc_df_4065155916/?tag=jpgo-22&linkCode=df0&hvadid=310429813636&hvpos=&hvnetw=g&hvrand=6867034787001615408&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=1009224&hvtargid=pla-675730625220&psc=1&th=1&psc=1)\n",
    "\n",
    "---\n",
    "\n",
    "**主問題**\n",
    "\n",
    "上の補題から、$v^*$は$v \\geq v^*$を満たす関数$v$の下界になります。\n",
    "これをつかうと、$v^*$を求める問題は、不等式制約$v \\geq (B v)$を満たす$v$のうち最小になる$v$によって近似することができます。\n",
    "関数$v$の大小を測る指標として$v$の重み付き和$w^Tv$を導入すると、次の線形計画問題が考えられます：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\mathrm{minimize}_{v \\in \\mathbb{R}^{S}} \\; w^T v\\\\\n",
    "&\\text{ subject to } v \\geq \\Pi\\left(r + \\gamma (P v) \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\Pi$にはmax作用素が入ってしまっていますが、次のように変形して、不等式を$\\forall s, a \\in {S}\\times {A}$で評価すれば大丈夫です。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\mathrm{minimize}_{v \\in \\mathbb{R}^{S}} \\; w^T v\\\\\n",
    "&\\text{ subject to } (\\tilde{I} - \\gamma P)v \\geq r\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "ここで$\\tilde{I}:=[I_{|S|}, I_{|S|}, \\dots] \\in \\mathbb{R}^{{SA}\\times S}$としました。\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "**双対問題**\n",
    "\n",
    "主問題を双対化してみましょう。\n",
    "つまり、 目的関数を制約条件に、そして制約条件を目的変数に逆転させることで、最適化問題を最定式化します。\n",
    "\n",
    "一応線形計画問題の強双対性については触れておきます\n",
    "\n",
    "* 線形計画問題が有界な最適解$x^*$をもつならば、双対問題も有界の最適解$\\nu^*$をもち、互いの目的案数の最適値は一致する。$c^Tx^* = b^T \\nu^*$\n",
    "* 主問題の目的関数が非有界ならば、双対問題は実行不可能である。つまり、任意の$M\\in \\mathbb{R}$について、主問題の目的関数の値$f(x)$を$M$より大きくできるような実行可能解$x$が存在する場合、双対問題は実行不可能である。\n",
    "\n",
    "つまり、プランニングでは双対問題を変わりに解いても、主問題と同じ解が得られます。\n",
    "\n",
    "主問題をラグランジュの未定乗数法を使って双対化すると\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\mathrm{maximize}_{x \\in \\mathbb{R}^{SA}} \\; x^T r \\\\\n",
    "&\\text{ subject to } x^T(\\tilde{I} - \\gamma P) = w, \\; x \\geq 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "になります。\n",
    "\n",
    "---\n",
    "\n",
    "双対問題の特性を調べるために、割引訪問頻度を導入します。\n",
    "\n",
    "* 割引訪問頻度（$d^\\pi_\\mu \\in \\mathbb{R}^{SA}$）：S, Aについての割引累積訪問頻度\n",
    "    * $d^\\pi_\\mu = \\mu \\Pi^\\pi (I - \\gamma P^\\pi)^{-1} = \\mu (I - \\gamma \\bar{P}^\\pi)^{-1}$\n",
    "    * ${d}^\\pi_\\mu (s, a) = \\pi(a|s) \\sum_{s_0} \\mu(s_0) \\sum_{t=0}^\\infty \\mathrm{Pr}\\left(S_t=s|S_0=s_0, M(\\pi)\\right)$が成立する。\n",
    "\n",
    "これを使うと、$(d^\\pi_w)^T r = w^T v^\\pi$ であることが簡単にわかります。\n",
    "\n",
    "ちなみに$\\pi, \\pi^\\prime\\in \\Pi$が$\\pi(a|s)=\\pi^\\prime(a|s)$のとき、かつそのときに限り、$d^\\pi_w(s, a)=d^{\\pi^\\prime}_w(s, a)$になります。（証明は教科書の補題2.11参照）\n",
    "つまり、$\\pi$と割引訪問頻度$d^\\pi_w$は一対一対応しています。\n",
    "\n",
    "さらに、$d^\\pi_w$については次の命題が成立します。\n",
    "\n",
    "---\n",
    "\n",
    "* 任意の方策$\\pi\\in \\Pi$の割引訪問頻度は双対問題の実行可能解$x$になる。\n",
    "\n",
    "まずこれを示します。定義から明らかに$d^\\pi_w \\geq 0$です。また、\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\sum_{s, a} \\gamma P(s^\\prime|s, a)d^\\pi_w(s, a)\\\\\n",
    "&=\\sum_{s_0} w(s_0) \\sum_{s, a} \\gamma P(s^\\prime|s, a)\\sum^\\infty_{t=0} \\gamma^t \\mathrm{Pr}(S_t=s, A_t=a| S_0=s_0, M(\\pi))\\\\\n",
    "&=\\sum_{s_0} w(s_0) \\sum^\\infty_{t=0} \\gamma^{t+1} \\mathrm{Pr}(S_{t+1}=s^\\prime| S_0=s_0, M(\\pi))\\\\\n",
    "&=\\sum_{s_0} w(s_0) \\sum^\\infty_{t=1} \\gamma^{t} \\mathrm{Pr}(S_{t}=s^\\prime| S_0=s_0, M(\\pi))\\\\\n",
    "&=\\sum_{s_0} w(s_0) \\left(\\sum^\\infty_{t=1} \\gamma^{t} \\mathrm{Pr}(S_{t}=s^\\prime| S_0=s_0, M(\\pi)) - \\mathrm{Pr}(S_{0}=s^\\prime| S_0=s_0, M(\\pi)) \\right)\\\\\n",
    "&=\\sum_{s_0} w(s_0) \\sum^\\infty_{t=1} \\gamma^{t} \\mathrm{Pr}(S_{t}=s^\\prime| S_0=s_0, M(\\pi)) - w(s^\\prime)\\\\\n",
    "&=\\sum_{a^\\prime} d^\\pi_w (s^\\prime, a^\\prime) - w(s^\\prime)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "なので、割引訪問頻度は実行可能解になってます。\n",
    "\n",
    "（たぶん\n",
    "$d^\\pi_w(s, a) = \\sum_{s_0 \\in \\mathcal{S}}w(s_0) \\sum^\\infty_{t=0} \\gamma^t \\mathrm{Pr}(S_t=s, A_t=a| S_0=s_0, M(\\pi))$\n",
    "は$(I - \\gamma P)^{-1}$に$w(s_0)$の要素が入ってるので、$(I - \\gamma P)^T x$の$x$に代入すると$(I - \\gamma P)$がキャンセルされて$w$だけ残るはず。\n",
    "行列をいい感じに書き直したらもっとキレイにかけるかも。\n",
    "[RL_utils.ipynb](RL_utils.ipynb)の行列を使えばキレイに書けそう！\n",
    "）\n",
    "\n",
    "また、次の命題も成立します。\n",
    "\n",
    "* ある関数$x\\in \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}$が双対問題の実行可能解のとき、確率的方策$\\pi_x\\propto x$の割引訪問頻度$d^{\\pi_x}_w$は関数$x$と一致する。つまり、$d^{\\pi_x}_w=x$\n",
    "\n",
    "こっちは省略しますが、行列をいじるとすぐに出てきます。\n",
    "これはつまり、実行可能解から方策に変換するやり方と、その方策の割引訪問頻度から実行可能解を復元するやり方を説明しています。\n",
    "\n",
    "---\n",
    "\n",
    "**主問題と双対問題の関係**\n",
    "\n",
    "上の命題を踏まえて、次の作用素を考えましょう。\n",
    "\n",
    "* 実行可能解から方策に変換する作用素：$\\mathcal{P}$\n",
    "* 方策から割引訪問頻度に変換する作用素：$\\mathcal{D}$\n",
    "\n",
    "このとき、$\\mathcal{P}$と$\\mathcal{D}$は全単射であり、一対一対応します。\n",
    "つまり、\n",
    "\n",
    "$$\\mathcal{P}(x) = \\mathcal{D}^{-1}(x) \\text{ かつ } \\mathcal{D}(\\pi) = \\mathcal{P}^{-1}(\\pi)$$\n",
    "が成立します。\n",
    "\n",
    "以上より、$\\sum_{s\\in\\mathcal{S}}\\sum_{a\\in \\mathcal{A}}x(s, a) r(s, a)$は$\\sum_{s\\in\\mathcal{S}} w(s) v^{\\pi_x}(s)$と同じです。\n",
    "よって双対問題は最大の価値関数を探す問題と同じなんですね。\n",
    "さらに、重み関数$w$が０より大きければ、その選択にかかわらず、主問題の最適解は最適価値関数と一致します。\n",
    "\n",
    "以上の話をPythonで実験してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "S = 10  # 状態集合のサイズ\n",
    "A = 3  # 行動集合のサイズ\n",
    "S_set = np.arange(S)  # 状態集合\n",
    "A_set = np.arange(A)  # 行動集合\n",
    "gamma = 0.8  # 割引率\n",
    "\n",
    "# 報酬行列を適当に作ります\n",
    "rew = np.random.rand(S, A)\n",
    "assert rew.shape == (S, A)\n",
    "\n",
    "# 遷移確率行列を適当に作ります\n",
    "P = np.random.rand(S*A, S)\n",
    "P = P / np.sum(P, axis=-1, keepdims=True)  # 正規化して確率にします\n",
    "P = P.reshape(S, A, S)\n",
    "np.testing.assert_almost_equal(P.sum(axis=-1), 1)  # ちゃんと確率行列になっているか確認します\n",
    "\n",
    "\n",
    "# 状態集合, 行動集合, 割引率, 報酬行列, 遷移確率行列が準備できたのでMDPのクラスを作ります\n",
    "\n",
    "class MDP(NamedTuple):\n",
    "    S_set: np.array  # 状態集合\n",
    "    A_set: np.array  # 行動集合\n",
    "    gamma: float  # 割引率\n",
    "    horizon: int  # エフェクティブホライゾン\n",
    "    rew: np.array  # 報酬行列\n",
    "    P: np.array  # 遷移確率行列\n",
    "\n",
    "    @property\n",
    "    def S(self) -> int:  # 状態空間のサイズ\n",
    "        return len(self.S_set)\n",
    "\n",
    "    @property\n",
    "    def A(self) -> int:  # 行動空間のサイズ\n",
    "        return len(self.A_set)\n",
    "\n",
    "\n",
    "horizon = int(1 / (1 - gamma))\n",
    "mdp = MDP(S_set, A_set, gamma, horizon, rew, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from functools import partial\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# 動的計画法で最適価値関数を計算します。\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"S\", \"A\"))\n",
    "def _compute_optimal_V(mdp: MDP, S: int, A: int):\n",
    "\n",
    "    def backup(optimal_Q):\n",
    "        max_Q = optimal_Q.max(axis=1)\n",
    "        next_v = mdp.P @ max_Q\n",
    "        assert next_v.shape == (S, A)\n",
    "        return mdp.rew + mdp.gamma * next_v\n",
    "    \n",
    "    optimal_Q = jnp.zeros((S, A))\n",
    "    body_fn = lambda i, Q: backup(Q)\n",
    "    Q = jax.lax.fori_loop(0, mdp.horizon + 1000, body_fn, optimal_Q)\n",
    "    return Q.max(axis=-1)\n",
    "\n",
    "compute_optimal_V = lambda mdp: _compute_optimal_V(mdp, mdp.S, mdp.A)\n",
    "\n",
    "optimal_V_by_DP = compute_optimal_V(mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPとLPの解の差： 4.7683716e-07\n",
      "DPとLPの目的関数の差 0.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import linprog\n",
    "# 重みはランダムに設定してみます\n",
    "w = np.random.rand(S)\n",
    "\n",
    "P = mdp.P.reshape(S*A, S)\n",
    "I = np.eye(S).reshape(S, 1, S)\n",
    "tI = np.tile(I, [1, A, 1]).reshape(S*A, S)\n",
    "\n",
    "assert P.shape == tI.shape\n",
    "\n",
    "# 主問題を解きます\n",
    "# Ax < b の不等式用です\n",
    "I_gP = - (tI - mdp.gamma * P)\n",
    "r = - mdp.rew.reshape(-1)\n",
    "lin_res = linprog(w, A_ub=I_gP, b_ub=r)\n",
    "\n",
    "optimal_V_by_LP = lin_res.x\n",
    "DP_fn = optimal_V_by_DP @ w\n",
    "\n",
    "print(\"DPとLPの解の差：\", np.abs(optimal_V_by_LP - optimal_V_by_DP).max())\n",
    "print(\"DPとLPの目的関数の差\", (DP_fn - lin_res.fun))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意（Contraction Lemma）の実装"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contraction Lemmaより，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\mathrm{minimize}_{v \\in \\mathbb{R}^{S}} \\; w^T v\\\\\n",
    "&\\text{ subject to } v \\geq \\Pi\\left(r + \\gamma (P v) \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "と\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\mathrm{maximize}_{v \\in \\mathbb{R}^{S}} \\; w^T v\\\\\n",
    "&\\text{ subject to } v \\leq \\Pi\\left(r + \\gamma (P v) \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "は解は同じになるはずです．\n",
    "しかし，ここで少し落とし穴があります．\n",
    "主問題が含む$\\max$作用素$\\Pi$を実現するために，実装上は全ての$(s, a)$での不等式制約を導入していました．\n",
    "これを不等号を入れ替えたときに同様に実装すると，$\\max$が$\\min$作用素に変換されてしまいます．\n",
    "なので単純に実装すると下のように失敗します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPとLPの解の差： 3.0303154\n",
      "DPとLPの目的関数の差 59.206844\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import linprog\n",
    "# 重みはランダムに設定してみます\n",
    "w = np.ones(S)\n",
    "\n",
    "P = mdp.P.reshape(S*A, S)\n",
    "I = np.eye(S).reshape(S, 1, S)\n",
    "tI = np.tile(I, [1, A, 1]).reshape(S*A, S)\n",
    "\n",
    "assert P.shape == tI.shape\n",
    "\n",
    "# 主問題を解きます\n",
    "# Ax < b の不等式用です\n",
    "I_gP = (tI - mdp.gamma * P)\n",
    "r = mdp.rew.reshape(-1)\n",
    "lin_res = linprog(-w, A_ub=I_gP, b_ub=r, bounds=(0, None))\n",
    "\n",
    "optimal_V_by_LP = lin_res.x\n",
    "LP_fn = lin_res.fun\n",
    "DP_fn = optimal_V_by_DP @ w\n",
    "\n",
    "print(\"DPとLPの解の差：\", np.abs(optimal_V_by_LP - optimal_V_by_DP).max())\n",
    "print(\"DPとLPの目的関数の差\", (DP_fn - LP_fn))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このようにContraction Lemmaで変形した線型計画法は一致しませんが，もともとの動的計画法と主問題の結果はほぼ一致していました。双対問題はどうでしょうか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LPとDUALの目的関数の差 2.4868995751603507e-14\n"
     ]
    }
   ],
   "source": [
    "# 双対問題を解きます\n",
    "# Ax = b の等式用です\n",
    "r = mdp.rew.reshape(-1)\n",
    "I_gP = (tI - mdp.gamma * P).T\n",
    "dual_res = linprog(-r, A_eq=I_gP, b_eq=w, bounds=(0, None))  # Maximumなのでマイナスをつけます\n",
    "\n",
    "print(\"LPとDUALの目的関数の差\", (lin_res.fun + dual_res.fun))  # Maximumなので和で計算します"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DPとLPとDUALはほぼ同じ答えにたどり着いていますね。よかったよかった。\n",
    "\n",
    "TODO: 今回の表記は結構めんどうなので、もっと単純にかける気がする。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('shumi-VTLwuKSy-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6b7cac5e0d2ff733f340da4d53ae5ecfef7f7ad39623f5982b029a09306b36b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
