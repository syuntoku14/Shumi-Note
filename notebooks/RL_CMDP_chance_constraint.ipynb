{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chance Constrained MDPのNP困難性など\n",
    "\n",
    "参考：\n",
    "* [Probabilistic Goal Markov Decision Processes](https://www.ijcai.org/Proceedings/11/Papers/341.pdf)\n",
    "\n",
    "表記：\n",
    "* ホライゾン：$T$\n",
    "* $p$：遷移確率\n",
    "* $g$：報酬の分布の集合．$g_t(r \\mid s, a)$は時刻$t$での報酬が$r$になる確率を表す．\n",
    "* 収益：$X_\\pi$\n",
    "\n",
    "決定的方策：\n",
    "* 履歴依存な決定的方策の集合：$\\Pi^h$\n",
    "* 現在の状態（と時刻）に依存した決定的な方策の集合：$\\Pi^{t, s}$\n",
    "* 現在の状態と時刻，そして今までの報酬の総和に依存した決定的な方策の集合：$\\Pi^{t, s, x}$\n",
    "\n",
    "\n",
    "確率的方策：\n",
    "* 履歴依存な確率的方策の集合：$\\Pi^{h, u}$\n",
    "* 現在の状態（と時刻）に依存した確率的な方策の集合：$\\Pi^{t, s, u}$\n",
    "* 現在の状態と時刻，そして今までの報酬の総和に依存した確率的な方策の集合：$\\Pi^{t, s, x, u}$\n",
    "\n",
    "今回は次の問題を考えます：\n",
    "* 決定問題（$D(\\Pi)$）：目標値$V \\in \\mathbb{R}$に対して，その目標を達成する確率が$\\alpha \\in (0, 1)$以上の方策$\\pi \\in \\Pi$は存在するか？ $\\operatorname{Pr}\\left(X_\\pi \\geq V\\right) \\geq \\alpha$\n",
    "* Probabilistic Goal：目標値$V \\in \\mathbb{R}$に対して，その目標を達成する確率を最大にする方策$\\pi$を探す $\\operatorname{Pr}\\left(X_\\pi \\geq V\\right)$\n",
    "\n",
    "方策集合について，次の定義を導入します：\n",
    "* $\\Pi$は$\\Pi'$に劣る：$D(\\Pi)$が真ならば$D(\\Pi')$も真．しかし，逆は成立しない場合の状況．\n",
    "* $\\Pi$は$\\Pi'$と等価である：$D(\\Pi)$が真ならば$D(\\Pi')$も真．逆も成立する状況．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 決定問題における方策集合の関係\n",
    "\n",
    "---\n",
    "\n",
    "**決定問題において決定的方策は十分（$\\Pi^{h, u}$と$\\Pi^{h}$は等価）**\n",
    "\n",
    "$MDP$，$\\alpha \\in [0, 1], V$について，もし\n",
    "\n",
    "$$\n",
    "\\operatorname{Pr}\\left(X_{\\pi_u} \\geq V\\right) \\geq \\alpha,\n",
    "$$\n",
    "\n",
    "を満たす方策$\\pi_u \\in \\Pi^{h, u}\\left(\\right.$ respectively $\\Pi^{t, s, x, u}$ and $\\left.\\Pi^{t, s, u}\\right)$が存在するならば，\n",
    "\n",
    "$$\n",
    "\\operatorname{Pr}\\left(X_\\pi \\geq V\\right) \\geq \\alpha .\n",
    "$$\n",
    "\n",
    "を満たす決定的な方策$\\pi \\in \\Pi^h$ (respectively $\\Pi^{t, s, x}$ and $\\left.\\Pi^{t, s}\\right)$も存在する．\n",
    "\n",
    "**証明**：若干テクニカル．\n",
    "確率的方策を「履歴＋確率変数$U$から行動を写像する関数」とみなします．\n",
    "そして，$\\pi_u \\sim \\pi$を，確率的方策の行動が決定的な方策の行動と一致するイベントとします．つまり，$\\pi_u\\left(H_t, U_{0: t}\\right)=\\pi\\left(H_t\\right)$．\n",
    "このとき，タワールールから，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{Pr}\\left(X_{\\pi_u} \\geq V\\right) & =\\sum_{\\pi \\in \\Pi^h} \\operatorname{Pr}\\left(X_{\\pi_u} \\geq V \\mid \\pi_u \\sim \\pi\\right) \\operatorname{Pr}\\left(\\pi_u \\sim \\pi\\right) \\\\\n",
    "& =\\sum_{\\pi \\in \\Pi^h} \\operatorname{Pr}\\left(X_\\pi \\geq V\\right) \\operatorname{Pr}\\left(\\pi_u \\sim \\pi\\right) .\n",
    "\\end{aligned}\n",
    "$$\n",
    "です．そして，$\\sum_{\\pi \\in \\Pi^h} \\operatorname{Pr}\\left(\\pi_u \\sim \\pi\\right)=1$なので，\n",
    "$$\n",
    "\\max _{\\pi \\in \\Pi^h} \\operatorname{Pr}\\left(X_\\pi \\geq V\\right) \\geq \\operatorname{Pr}\\left(X_{\\pi_u} \\geq V\\right)\n",
    "$$\n",
    "が成り立ちます．\n",
    "\n",
    "---\n",
    "\n",
    "**決定問題における報酬の履歴の重要性（$\\Pi^{t, s}$は$\\Pi^{t, s, x}$に劣る）**\n",
    "\n",
    "次のMDPを考えましょう：\n",
    "\n",
    "![](figs/inferior-MDP-chance.png)\n",
    "\n",
    "行動$a, b$があり，$a$は上に進む経路，$b$は下に進む経路とします．\n",
    "このMDPでは，固定された行動$a, b$について，$\\operatorname{Pr}(X \\geq 0)=0.5$です．\n",
    "\n",
    "一方で，$s_1$時点での報酬が$+1$ならば$a$を取り，そうでないならば$b$を取る行動を考えると，$\\operatorname{Pr}\\left(X_\\pi \\geq 0\\right)=0.75$になります．\n",
    "\n",
    "後で見ますが，累積報酬以上の情報を付与しても，方策の性能は上がりません．\n",
    "よって，$\\Pi^{t, s, x}$ と $\\Pi^h$ は等価です．\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NP困難性\n",
    "\n",
    "実は，Probabilistic Goalを解くのは一般にNP困難になります．\n",
    "\n",
    "---\n",
    "\n",
    "**Theorem 2** \n",
    "$\\Pi^h, \\Pi^{h, u}, \\Pi^{t, s}, \\Pi^{t, s, u}, \\Pi^{t, s, x}$ or $\\Pi^{t, s, x, u}$の場合について，$D(\\Pi)$問題はNP-困難です．\n",
    "\n",
    "**証明**\n",
    "\n",
    "$\\Pi^{t, s}$がナップサック問題を含むことを示すことでNP困難性を示します．\n",
    "ナップサック問題は次の問題です：\n",
    "* $n$個のアイテム\n",
    "    * アイテム$i$には価値$v_i$と重さ$w_i$が割り当てられています．それぞれ非負の整数\n",
    "* ナップサック問題は次と同じです：正の整数$W$と$V$について，次を満たす$I \\subseteq[1: n]$が存在するか判定する問題\n",
    "\n",
    "$$\n",
    "\\sum_{i \\in I} w_i \\leq W ; \\quad \\sum_{i \\in I} v_i \\geq V\n",
    "$$\n",
    "\n",
    "さて，Probabilistic Goalが解けるとナップサック問題が解けちゃうことを示しましょう．\n",
    "次のMDPを作ります：\n",
    "![](figs/NP-hard-chance.png)\n",
    "\n",
    "* $T=n+2$\n",
    "* 状態集合：$\\mathcal{S}=\\left\\{s_1, \\cdots, s_n, s^{b a d}, t\\right\\}$\n",
    "  * $t$は終端状態です．\n",
    "* 行動集合：\n",
    "  * $a$を取ると，報酬$0$が発生して$s_{i+1}$に遷移します\n",
    "  * $b$を取ると，報酬$v_i$が発生して，確率$1/2^{w_i}$で$s_{i+1}$に遷移します．それ以外の確率で$s^{bad}$に行きます．\n",
    "* $s^{bad}$では報酬$-L \\triangleq-2 \\sum_{i=1}^n v_i$が発生し，終端状態に移行します．\n",
    "\n",
    "さて，決定問題$D\\left(\\Pi^{t, s}\\right)$を考えましょう．$\\alpha = 1/2^W$とします．\n",
    "\n",
    "**$D\\left(\\Pi^{t, s}\\right)$がpositiveのとき:**\n",
    "このとき，$\\operatorname{Pr}\\left(X_\\pi \\geq V\\right) \\geq \\alpha$を満たす方策が存在します．\n",
    "$I'$を，$\\pi$が$s_i$で$b$を取る$i \\in [1:n]$の集合とします．\n",
    "$b$以外の行動は報酬0なので，このとき，$\\sum_{i \\in I^{\\prime}} v_i \\geq V$です．\n",
    "\n",
    "また，$s^{bad}$では大きな負の報酬が発生するので，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{Pr}\\left(X_\\pi \\geq V\\right) & \\leq \\operatorname{Pr}\\left(s^{b a d} \\text { is never reached. }\\right) \\\\\n",
    "& =\\Pi_{i \\in I^{\\prime}} \\frac{1}{2^{w_i}}=\\frac{1}{2^{\\sum_{i \\in I^{\\prime}} w_i}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "です．よって，\n",
    "$$\n",
    "\\frac{1}{2^{\\sum_{i \\in I^{\\prime}} w_i}} \\geq 1 / 2^W \\quad \\Rightarrow \\quad \\sum_{i \\in I^{\\prime}} w_i \\leq W\n",
    "$$\n",
    "が成り立ちます．故に，Probabilistic GoalがPositiveなら，ナップサック問題もPositiveです．\n",
    "\n",
    "**ナップサック問題がPositiveのとき:** つまり，次を満たす$I \\subset [1 : n]$が存在します：\n",
    "$$\\sum_{i \\in I} w_i \\leq W ; \\quad \\sum_{i \\in I} v_i \\geq V$$\n",
    "\n",
    "次の方策$\\pi'$を考えましょう．$i \\in I$では$b$を取り，それ以外では$a$を取ります．このとき，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\operatorname{Pr}\\left(s^{b a d} \\text { is never reached. }\\right) \\\\\n",
    "& =\\prod_{i \\in I} \\frac{1}{2^{w_i}}=\\frac{1}{2^{\\sum_{i \\in I} w_i}} \\geq 1 / 2^W=o\n",
    "\\end{aligned}\n",
    "$$\n",
    "です．このとき，$s^{bad}$が到達されないので，$\\sum_{i \\in I} v_i \\geq V$です．よって，$\\operatorname{Pr}\\left(X_{\\pi^{\\prime}} \\geq V\\right) \\geq \\alpha$であり，$D(\\Pi^{t, s})$はpositiveです．\n",
    "\n",
    "$D\\left(\\Pi^{h, u}\\right), D\\left(\\Pi^{t, s, u}\\right), D\\left(\\Pi^{t, s, x, u}\\right)$がNP困難であることは等価性を示せばいけます．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 擬多項式アルゴリズム\n",
    "\n",
    "アルゴリズムの入力のBit長を無視して，その入力値について多項式時間で処理されるアルゴリズムのことを，擬多項式時間アルゴリズムと呼びます．\n",
    "ナップサック問題は擬多項式時間アルゴリズムであり，実際，$O(nW)$の計算時間を使えば，動的計画法で解くことができます．\n",
    "\n",
    "今回のProbabilistic Goalも，擬多項式時間アルゴリズムが存在します．\n",
    "報酬が整数値を取るとしましょう．そして，その最大値を$K=\\max _{r \\in \\mathcal{R}}|r|$とします．\n",
    "\n",
    "このとき，$T, |S|, |A|, K$の多項式時間で$D\\left(\\Pi^{h, u}\\right)$を求めるアルゴリズムが存在します．また，Goal MDPも解けます．\n",
    "\n",
    "**証明**\n",
    "\n",
    "Probabilistic Goal MDPの最適解を\n",
    "$$\n",
    "\\pi^*=\\arg \\max _{\\pi \\in \\Pi^h} \\operatorname{Pr}\\left(X_\\pi \\geq V\\right)\n",
    "$$\n",
    "としましょう．これが，状態数$2TK|\\mathcal{S}|$のMDPを解くことで得られることを示せば十分です．\n",
    "\n",
    "各状態$s$を$(s, C)$で拡張したAugmented MDP (AMDP) を考えます．ここで，$C$は$[-T K, T K]$内の整数です．\n",
    "そして，次のような遷移を考えます：\n",
    "$$\n",
    "\\operatorname{Pr}\\left(\\left(s^{\\prime}, C^{\\prime}\\right) \\mid a,(s, C)\\right)=p_t\\left(s^{\\prime} \\mid a, s\\right) \\times g_t\\left(C^{\\prime}-C \\mid s, a\\right)\n",
    "$$\n",
    "つまり，$(s', C')$の遷移は，$s\\to s'$への遷移と，$C'-C$の報酬発生が同時に生じたときに起こります．\n",
    "そして，AMDPの報酬は，時刻$T$で$C \\geq V$を満たすような$(s, C)$でだけ$+1$が発生します．\n",
    "\n",
    "明らかに，このAMDPで価値$+1$を達成する方策は，確率$1$で$C$以上の価値を出してます．\n",
    "収益の値が達成する確率に対応しています．\n",
    "\n",
    "## Chance Constrained MDP\n",
    "\n",
    "上の議論を応用すると，Chance Constrained MDPを解くこともできます：\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\text { Maximize: } & \\mathbb{E}\\left(X_\\pi\\right) \\\\\n",
    "\\text { Subject to: } & \\operatorname{Pr}\\left(X_\\pi \\geq V\\right) \\geq \\alpha\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "これを$T, |\\mathcal{S}|, |\\mathcal{A}|, K$の多項式計算量で解くアルゴリズムが存在します．\n",
    "\n",
    "**証明**\n",
    "\n",
    "上と同様に，状態空間 $(s, C)$ のAMDPを作りましょう．\n",
    "ただし，即時報酬を$(r^1, r^2)$ なるペアとして定義し，これらは最後のステージでのみ非ゼロの値を取るようにします．\n",
    "\n",
    "最後のステージで，状態が $(s, C')$ である場合，第一の報酬成分は $C'$ として，第二の報酬成分は $\\mathbb{1}(C' > v)$とします．\n",
    "したがって、このAMDPについて，累積報酬のペアを考えると，その第一成分と第二成分の期待値は，それぞれ元のMDPにおける期待報酬と目標到達確率に等しくなります．\n",
    "よって，以上のAMDPは次の問題を解くCMDPの方策を求めることと同値になります．\n",
    "\n",
    "$$\n",
    "\\max \\mathbb{E}(X^1_{\\pi})\\quad \\text{such that }\\quad \\mathbb{E}(X^2_{\\pi}) \\geq \\alpha,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
