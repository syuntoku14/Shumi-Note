{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 強化学習と線型計画法（有限ホライゾン）\n",
    "\n",
    "参考文献\n",
    "\n",
    "* [Exploration-Exploitation in Constrained MDPs](https://arxiv.org/abs/2003.02189)の2.3章\n",
    "\n",
    "強化学習が扱う最適方策の導出（プランニング問題）は線形計画問題としても定式化できます。\n",
    "有限ホライゾンの場合でも、無限ホライゾン([RL_as_LP.ipynb](RL_as_LP.ipynb))と似たような形式で線形計画問題に落とし込むことができます。\n",
    "\n",
    "**表記**（[RL_utils.ipynb](RL_utils.ipynb)参照）\n",
    "\n",
    "MDPを次で定義します。\n",
    "\n",
    "1. 有限状態集合: $S=\\{1, \\dots, |S|\\}$\n",
    "2. 有限行動集合: $A=\\{1, \\dots, |A|\\}$\n",
    "3. $h$ステップ目の遷移確率行列: $P_h\\in \\mathbb{R}^{SA\\times S}$\n",
    "4. $h$ステップ目の報酬行列: $r_h\\in \\mathbb{R}^{S\\times A}$\n",
    "5. ホライゾン: $H$\n",
    "6. 初期状態: $\\mu \\in \\mathbb{R}^{S}$\n",
    "\n",
    "また、次の占有率を導入しておきます。　\n",
    "* 占有率：$d_h^\\pi(s, a ; p):=\\mathbb{E}\\left[\\mathbb{1}\\left\\{s_h=s, a_h=a\\right\\} \\mid s_1=s_1, p, \\pi\\right]=\\operatorname{Pr}\\left\\{s_h=s, a_h=a \\mid s_1=s_1, p, \\pi\\right\\}$\n",
    "* 価値関数：$V_1^\\pi\\left(s_1\\right)=\\sum_{h, s, a} d_h^\\pi(s, a) r_h(s, a):=r^T d^\\pi(p)$\n",
    "\n",
    "\n",
    "占有率は任意の$h \\in[H] \\backslash\\{1\\}$について次を満たします。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sum_a d_h^\\pi(s, a) & =\\sum_{s^{\\prime}, a^{\\prime}} p_{h-1}\\left(s \\mid s^{\\prime}, a^{\\prime}\\right) d_{h-1}^\\pi\\left(s^{\\prime}, a^{\\prime}\\right) & & \\forall s \\in \\mathcal{S} \\\\\n",
    "d_h^\\pi(s, a) & \\geq 0 & & \\forall s, a\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$h=1$については\n",
    "$$d_1^\\pi(s, a)=\\pi_1(a \\mid s) \\cdot \\mu(s) \\quad \\forall s, a$$\n",
    "\n",
    "です。\n",
    "\n",
    "これを使うと、有限ホライゾンの最適方策は\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\max_d \\sum_{s, a, h} d_h(s, a) r_h(s, a) &&\\\\\n",
    "\\text { s.t. } &\\sum_a d_h(s, a) =\\sum_{s^{\\prime}, a^{\\prime}} p_{h-1}\\left(s \\mid s^{\\prime}, a^{\\prime}\\right) d_{h-1}\\left(s^{\\prime}, a^{\\prime}\\right) & & \\forall h \\in[H] \\backslash\\{1\\} \\\\\n",
    "& \\sum_a d_1(s, a) =\\mu(s) & & \\forall s \\in \\mathcal{S}\\\\\n",
    "& d_h(s, a) \\geq 0 & &\\forall(s, a, h) \\in \\mathcal{S} \\times \\mathcal{A} \\times[H]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "を解き、\n",
    "\n",
    "$$\n",
    "\\pi_h^d(a \\mid s)=\\frac{d_h(s, a)}{\\sum_b d_h(s, b)}, \\quad \\forall(s, a, h) \\in \\mathcal{S} \\times \\mathcal{A} \\times[H]\n",
    "$$\n",
    "\n",
    "とすれば求まります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状態数： 10\n",
      "行動数： 3\n",
      "ホライゾン： 30\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from typing import NamedTuple, Optional\n",
    "from jax.random import PRNGKey\n",
    "\n",
    "key = PRNGKey(0)\n",
    "\n",
    "S = 10  # 状態集合のサイズ\n",
    "A = 3  # 行動集合のサイズ\n",
    "S_set = jnp.arange(S)  # 状態集合\n",
    "A_set = jnp.arange(A)  # 行動集合\n",
    "H = 30  # ホライゾン\n",
    "\n",
    "# 報酬行列を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "rew = jax.random.uniform(key=key, shape=(H, S, A))\n",
    "assert rew.shape == (H, S, A)\n",
    "\n",
    "\n",
    "# 遷移確率行列を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "P = jax.random.uniform(key=key, shape=(H, S*A, S))\n",
    "P = P / jnp.sum(P, axis=-1, keepdims=True)  # 正規化して確率にします\n",
    "P = P.reshape(H, S, A, S)\n",
    "np.testing.assert_allclose(P.sum(axis=-1), 1, atol=1e-6)  # ちゃんと確率行列になっているか確認します\n",
    "\n",
    "\n",
    "# 初期状態分布を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "init_dist = jax.random.uniform(key, shape=(S,))\n",
    "init_dist = init_dist / jnp.sum(init_dist)\n",
    "np.testing.assert_allclose(init_dist.sum(axis=-1), 1, atol=1e-6)  # ちゃんと確率行列になっているか確認します\n",
    "\n",
    "\n",
    "# 状態集合, 行動集合, 割引率, 報酬行列, 遷移確率行列が準備できたのでMDPのクラスを作ります\n",
    "\n",
    "class MDP(NamedTuple):\n",
    "    S_set: jnp.array  # 状態集合\n",
    "    A_set: jnp.array  # 行動集合\n",
    "    H: int  # ホライゾン\n",
    "    rew: jnp.array  # 報酬行列\n",
    "    P: jnp.array  # 遷移確率行列\n",
    "    init_dist: jnp.array  # 初期分布\n",
    "    optimal_Q: Optional[jnp.ndarray] = None  # 最適Q値\n",
    "\n",
    "    @property\n",
    "    def S(self) -> int:  # 状態空間のサイズ\n",
    "        return len(self.S_set)\n",
    "\n",
    "    @property\n",
    "    def A(self) -> int:  # 行動空間のサイズ\n",
    "        return len(self.A_set)\n",
    "\n",
    "\n",
    "mdp = MDP(S_set, A_set, H, rew, P, init_dist)\n",
    "\n",
    "print(\"状態数：\", mdp.S)\n",
    "print(\"行動数：\", mdp.A)\n",
    "print(\"ホライゾン：\", mdp.H)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （準備）動的計画法\n",
    "\n",
    "**表記**\n",
    "\n",
    "* ステップ$h$の方策行列（$\\Pi_h^\\pi \\in \\mathbb{R}^{S\\times SA}$）：$\\langle \\pi_h, q\\rangle$を行列で書きたいときに便利。\n",
    "    * $\\Pi_h^\\pi(s,(s, a))=\\pi_h(a \\mid s)$ \n",
    "    * $\\Pi_h^\\pi q_h^\\pi = \\langle \\pi, q_h^\\pi \\rangle = v_h^\\pi$が成立。\n",
    "* ステップ$h$の遷移確率行列１（$P_h^\\pi \\in \\mathbb{R}^{SA\\times SA}$）: 次の状態についての方策の情報を追加したやつ。\n",
    "    * $P_h^\\pi = P_h \\Pi_h^\\pi$\n",
    "    * Q値を使ったベルマン期待作用素とかで便利。$q_h^\\pi = r_h + P_h^\\pi q^\\pi$が成立。\n",
    "* ステップ$h$の遷移確率行列２（$\\bar{P}_h^\\pi \\in \\mathbb{R}^{S\\times S}$）: 方策$\\pi$のもとでの状態遷移の行列。\n",
    "    * $\\bar{P}_h^\\pi = \\Pi_h^\\pi P_h$\n",
    "    * V値を使ったベルマン期待作用素とかで便利。$v_h^\\pi = \\Pi^\\pi r_h + \\gamma \\bar{P}_h^\\pi v^\\pi$。\n",
    "* ステップ$h$の訪問頻度（$d^\\pi_{h,\\mu} \\in \\mathbb{R}^{SA}$）：S, Aについての累積訪問頻度\n",
    "    * ${d}^\\pi_{h,\\mu} (s, a) = \\pi(a|s) \\sum_{s_0} \\mu(s_0) \\sum_{t=0}^h \\mathrm{Pr}\\left(S_t=s|S_0=s_0, M(\\pi)\\right)$\n",
    "\n",
    "\n",
    "**実装した関数**\n",
    "\n",
    "* ``compute_greedy_policy``: Q関数 ($H\\times S \\times A \\to \\mathcal{R}$) の貪欲方策を返します\n",
    "* ``compute_optimal_Q``: MDPの最適Q関数 $q_* : H\\times S \\times A \\to \\mathcal{R}$ を返します。\n",
    "* ``compute_policy_Q``: 方策 $\\pi$ のQ関数 $q_\\pi : H\\times S \\times A \\to \\mathcal{R}$ を返します。\n",
    "* ``compute_policy_matrix``: 方策$\\pi$の行列${\\Pi}^{\\pi} : H \\times S \\times SA$を返します。\n",
    "* ``compute_policy_visit``: 方策 $\\pi$ の割引訪問頻度${d}^\\pi_{\\mu} : {H\\times S \\times A}$ を返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最適価値関数と最適方策の価値関数の差 0.0\n",
      "0ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 1.9073486e-06\n",
      "1ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 1.9073486e-06\n",
      "2ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 1.9073486e-06\n",
      "3ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 0.0\n",
      "4ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 1.9073486e-06\n",
      "5ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 1.9073486e-06\n",
      "6ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 5.722046e-06\n",
      "7ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 9.536743e-06\n",
      "8ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 9.536743e-06\n",
      "9ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 3.8146973e-06\n",
      "10ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 3.8146973e-06\n",
      "11ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 9.536743e-07\n",
      "12ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 9.536743e-07\n",
      "13ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 9.536743e-07\n",
      "14ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 0.0\n",
      "15ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 0.0\n",
      "16ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 9.536743e-07\n",
      "17ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 9.536743e-07\n",
      "18ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 2.861023e-06\n",
      "19ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 1.9073486e-06\n",
      "20ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 1.4305115e-06\n",
      "21ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 9.536743e-07\n",
      "22ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 9.536743e-07\n",
      "23ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 0.0\n",
      "24ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 4.7683716e-07\n",
      "25ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 2.3841858e-07\n",
      "26ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 2.3841858e-07\n",
      "27ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 2.3841858e-07\n",
      "28ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 0.0\n",
      "29ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差 0.0\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import jax\n",
    "import chex\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_greedy_policy(Q: jnp.ndarray):\n",
    "    \"\"\"Q関数の貪欲方策を返します\n",
    "\n",
    "    Args:\n",
    "        Q (jnp.ndarray): (HxSxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        greedy_policy (jnp.ndarray): (HxSxA)の行列\n",
    "    \"\"\"\n",
    "    greedy_policy = jnp.zeros_like(Q)\n",
    "    H, S, A = Q.shape\n",
    "    \n",
    "    def body_fn(i, greedy_policy):\n",
    "        greedy_policy = greedy_policy.at[i, jnp.arange(S), Q[i].argmax(axis=-1)].set(1)\n",
    "        return greedy_policy\n",
    "\n",
    "    greedy_policy = jax.lax.fori_loop(0, H, body_fn, greedy_policy)\n",
    "    chex.assert_shape(greedy_policy, (H, S, A))\n",
    "    return greedy_policy\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"H\", \"S\", \"A\"))\n",
    "def _compute_optimal_Q(mdp: MDP, H: int, S: int, A: int):\n",
    "    \"\"\"ベルマン最適作用素をホライゾン回走らせて最適価値関数を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "\n",
    "    Returns:\n",
    "        optimal_Q (jnp.ndarray): (HxSxA)の行列\n",
    "    \"\"\"\n",
    "\n",
    "    def backup(i, optimal_Q):\n",
    "        h = H - i - 1\n",
    "        max_Q = optimal_Q[h+1].max(axis=1)\n",
    "        next_v = mdp.P[h] @ max_Q\n",
    "        chex.assert_shape(next_v, (S, A))\n",
    "        optimal_Q = optimal_Q.at[h].set(mdp.rew[h] + next_v)\n",
    "        return optimal_Q\n",
    "    \n",
    "    optimal_Q = jnp.zeros((H+1, S, A))\n",
    "    optimal_Q = jax.lax.fori_loop(0, mdp.H, backup, optimal_Q)\n",
    "    return optimal_Q[:-1]\n",
    "\n",
    "compute_optimal_Q = lambda mdp: _compute_optimal_Q(mdp, mdp.H, mdp.S, mdp.A)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_Q(mdp: MDP, policy: jnp.ndarray):\n",
    "    \"\"\"ベルマン期待作用素をホライゾン回走らせて価値関数を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "        policy (np.ndarray): (HxSxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        optimal_Q (jnp.ndarray): (HxSxA)の行列\n",
    "    \"\"\"\n",
    "    H, S, A = policy.shape\n",
    "\n",
    "    def backup(i, policy_Q):\n",
    "        h = H - i - 1\n",
    "        max_Q = (policy[h+1] * policy_Q[h+1]).sum(axis=1)\n",
    "        next_v = mdp.P[h] @ max_Q\n",
    "        chex.assert_shape(next_v, (S, A))\n",
    "        policy_Q = policy_Q.at[h].set(mdp.rew[h] + next_v)\n",
    "        return policy_Q\n",
    "    \n",
    "    policy_Q = jnp.zeros((H+1, S, A))\n",
    "    policy_Q = jax.lax.fori_loop(0, mdp.H, backup, policy_Q)\n",
    "    return policy_Q[:-1]\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_matrix(policy: jnp.ndarray):\n",
    "    \"\"\"\n",
    "    上で定義した方策行列を計算します。方策についての内積が取りたいときに便利です。\n",
    "    Args:\n",
    "        policy (jnp.ndarray): (HxSxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        policy_matrix (jnp.ndarray): (HxSxSA)の行列\n",
    "    \"\"\"\n",
    "    H, S, A = policy.shape\n",
    "    PI = policy.reshape(H, 1, S, A)\n",
    "    PI = jnp.tile(PI, (1, S, 1, 1))\n",
    "    eyes = jnp.tile(jnp.eye(S).reshape(1, S, S, 1), (H, 1, 1, 1))\n",
    "    PI = (eyes * PI).reshape(H, S, S*A)\n",
    "    return PI\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_visit(mdp: MDP, policy: jnp.ndarray, init_dist: jnp.ndarray):\n",
    "    \"\"\"MDPと方策について、訪問頻度を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "        policy (jnp.ndarray): (HxSxA)の行列\n",
    "        init_dist (jnp.ndarray): (S) 初期状態の分布\n",
    "\n",
    "    Returns:\n",
    "        visit (jnp.ndarray): (HxSxA)のベクトル\n",
    "    \"\"\"\n",
    "    H, S, A = policy.shape\n",
    "    Pi = compute_policy_matrix(policy)\n",
    "    P = mdp.P.reshape(H, S*A, S)\n",
    "\n",
    "    def body_fn(h, visit):\n",
    "        next_visit = visit[h] @ P[h] @ Pi[h+1]\n",
    "        visit = visit.at[h+1].set(next_visit)\n",
    "        return visit\n",
    "    \n",
    "    visit = jnp.zeros((H+1, S*A))\n",
    "    visit = visit.at[0].set((init_dist @ Pi[0]))\n",
    "    visit = jax.lax.fori_loop(0, mdp.H, body_fn, visit)\n",
    "    visit = visit[:-1].reshape(H, S, A)\n",
    "    return visit\n",
    "\n",
    "\n",
    "# 動的計画法による最適価値関数\n",
    "optimal_Q_DP = compute_optimal_Q(mdp)\n",
    "optimal_V_DP = optimal_Q_DP.max(axis=-1)\n",
    "optimal_policy = compute_greedy_policy(optimal_Q_DP)\n",
    "optimal_policy_Q_DP = compute_policy_Q(mdp, optimal_policy)\n",
    "mdp = mdp._replace(optimal_Q=optimal_Q_DP)\n",
    "print(\"最適価値関数と最適方策の価値関数の差\", jnp.abs(optimal_Q_DP - optimal_policy_Q_DP).max())\n",
    "\n",
    "# 訪問頻度によるリターンの計算\n",
    "policy_visit = compute_policy_visit(mdp, optimal_policy, mdp.init_dist)\n",
    "np.testing.assert_allclose(policy_visit.sum(axis=(1, 2)), 1.0, atol=1e-6)\n",
    "np.testing.assert_allclose(policy_visit[0].sum(axis=-1), mdp.init_dist, atol=1e-6)\n",
    "for h in range(H):\n",
    "    return_by_visit = (policy_visit * mdp.rew)[h:].sum()\n",
    "    return_by_DP = (optimal_Q_DP[h] * policy_visit[h]).sum()\n",
    "    print(f\"{h}ステップ目の訪問頻度によるリターンと動的計画法によるリターンの差\", np.abs(return_by_visit - return_by_DP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CBC MILP Solver \n",
      "Version: 2.10.3 \n",
      "Build Date: Dec 15 2019 \n",
      "\n",
      "command line - /home/toshinori/shumi-note/.venv/lib/python3.9/site-packages/pulp/solverdir/cbc/linux/64/cbc /tmp/d7e19f5f14da4d2684e66562025be7db-pulp.mps max timeMode elapsed branch printingOptions all solution /tmp/d7e19f5f14da4d2684e66562025be7db-pulp.sol (default strategy 1)\n",
      "At line 2 NAME          MODEL\n",
      "At line 3 ROWS\n",
      "At line 305 COLUMNS\n",
      "At line 10806 RHS\n",
      "At line 11107 BOUNDS\n",
      "At line 11108 ENDATA\n",
      "Problem MODEL has 300 rows, 900 columns and 9600 elements\n",
      "Coin0008I MODEL read with 0 errors\n",
      "Option for timeMode changed from cpu to elapsed\n",
      "Presolve 190 (-110) rows, 550 (-350) columns and 5950 (-3650) elements\n",
      "0  Obj -0 Primal inf 1.9339191 (10) Dual inf 1046.5361 (550)\n",
      "0  Obj -0 Primal inf 1.9339191 (10) Dual inf 1.3796436e+12 (550)\n",
      "31  Obj -0 Primal inf 1.9339191 (10) Dual inf 5.542992e+12 (445)\n",
      "62  Obj -0 Primal inf 1.9339191 (10) Dual inf 7.0748679e+12 (370)\n",
      "93  Obj -0 Primal inf 1.9339191 (10) Dual inf 8.3266145e+12 (315)\n",
      "124  Obj -0 Primal inf 1.9339191 (10) Dual inf 8.3284794e+12 (252)\n",
      "173  Obj -0 Primal inf 1.9339191 (10) Dual inf 3.6353634e+12 (162)\n",
      "224  Obj 22.022114 Dual inf 22.945284 (57)\n",
      "255  Obj 22.857271 Dual inf 1.3440467 (10)\n",
      "265  Obj 22.890361\n",
      "Optimal - objective value 22.890361\n",
      "After Postsolve, objective 22.890361, infeasibilities - dual 0 (0), primal 0 (0)\n",
      "Optimal objective 22.89036147 - 265 iterations time 0.002, Presolve 0.00\n",
      "Option for printingOptions changed from normal to all\n",
      "Total time (CPU seconds):       0.01   (Wallclock seconds):       0.01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pulp\n",
    "from itertools import product\n",
    "prob = pulp.LpProblem(name=\"CMDP\", sense=pulp.LpMaximize)\n",
    "hsa_indices = [(h, s, a) for h, s, a in product(range(H), range(S), range(A))]\n",
    "sa_indices = [(s, a) for s, a in product(range(S), range(A))]\n",
    "d = pulp.LpVariable.dicts(\"d\", hsa_indices, lowBound=0, cat=\"Continuous\")\n",
    "\n",
    "# 目的関数\n",
    "prob += pulp.lpSum([d[hsa] * mdp.rew[hsa[0], hsa[1], hsa[2]] for hsa in hsa_indices])\n",
    "\n",
    "# 初期状態についての制約\n",
    "for s in range(S):\n",
    "    d_0sa = [d[(0, s, a)] for a in range(A)]\n",
    "    prob += pulp.lpSum(d_0sa) == mdp.init_dist[s].item()\n",
    "\n",
    "# 各ステップについての制約\n",
    "for h in range(1, H):\n",
    "    for ns in range(S):\n",
    "        d_hns = pulp.lpSum([d[(h, ns, na)] for na in range(A)])\n",
    "        d_phns = pulp.lpSum([d[(h-1, sa[0], sa[1])] * mdp.P[h-1, sa[0], sa[1], ns] for sa in sa_indices])\n",
    "        prob += d_hns == d_phns\n",
    "\n",
    "\n",
    "sol = prob.solve()\n",
    "d_arr = jnp.array([pulp.value(d[h, s, a]) for (h, s, a) in hsa_indices])\n",
    "d_arr = d_arr.reshape(H, S, A)\n",
    "\n",
    "np.testing.assert_allclose(d_arr.sum(axis=(1, 2)), 1.0, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最適価値関数とLPによる価値関数の差 0.0\n"
     ]
    }
   ],
   "source": [
    "policy = d_arr / d_arr.sum(axis=-1, keepdims=True)\n",
    "Q_LP = compute_policy_Q(mdp, policy)\n",
    "\n",
    "print(\"最適価値関数とLPによる価値関数の差\", jnp.abs(optimal_Q_DP - Q_LP).max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PULPを使ってもいいですが，かなり遅いです．なんとか通常の行列形式に書き換えてみましょう．\n",
    "\n",
    "元の問題は\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\max_d \\sum_{s, a, h} d_h(s, a) r_h(s, a) &&\\\\\n",
    "\\text { s.t. } &\\sum_a d_h(s, a) - \\sum_{s^{\\prime}, a^{\\prime}} p_{h-1}\\left(s \\mid s^{\\prime}, a^{\\prime}\\right) d_{h-1}\\left(s^{\\prime}, a^{\\prime}\\right) = 0& & \\forall h \\in[H] \\backslash\\{1\\} \\\\\n",
    "& \\sum_a d_1(s, a) =\\mu(s) & & \\forall s \\in \\mathcal{S}\\\\\n",
    "& d_h(s, a) \\geq 0 & &\\forall(s, a, h) \\in \\mathcal{S} \\times \\mathcal{A} \\times[H]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "でした．この制約の部分を\n",
    "\n",
    "$$\n",
    "B d = b\n",
    "$$\n",
    "\n",
    "の形に直します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "d = d_arr\n",
    "d = d.reshape(H * S * A)\n",
    "\n",
    "B = np.zeros((H, S, A, H, S, A))\n",
    "\n",
    "# 初期状態についての制約\n",
    "for s, a in product(range(S), range(A)):\n",
    "    B[0, s, a, 0, s] = 1\n",
    "\n",
    "\n",
    "# 遷移についての制約\n",
    "for h, s, a in product(range(1, H), range(S), range(A)):\n",
    "    B[h, s, a, h, s] = 1  # sum_a d(h, s, a) を実現します\n",
    "    B[h, s, a, h-1] = -mdp.P[h-1, :, :, s]  # sum_a d(h, s, a) を実現します\n",
    "\n",
    "\n",
    "B = B.reshape((H*S*A, H*S*A))\n",
    "mu = np.repeat(mdp.init_dist[:, None], A, axis=1).reshape(-1)\n",
    "b = np.hstack((mu, np.zeros((H-1)*S*A)))\n",
    "\n",
    "np.testing.assert_almost_equal(B @ d, b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで行列形式での制約ができました．これを使って，次の問題をscipyで解きます．\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\max d^T r \\;\\; \\text { s.t. }  B d = b \\; \\text{ and }\\; d \\geq 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最適価値関数とLPによる価値関数の差 0.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import linprog\n",
    "\n",
    "r = - mdp.rew.reshape(-1)\n",
    "lin_res = linprog(r, A_eq=B, b_eq=b, bounds=(0, None))\n",
    "\n",
    "d_arr_matrix = lin_res.x.reshape(H, S, A)\n",
    "policy = d_arr_matrix / d_arr_matrix.sum(axis=-1, keepdims=True)\n",
    "Q_LP_matrix = compute_policy_Q(mdp, policy)\n",
    "\n",
    "print(\"最適価値関数と行列形式のLPによる価値関数の差\", jnp.abs(optimal_Q_DP - Q_LP_matrix).max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('shumi-VTLwuKSy-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6b7cac5e0d2ff733f340da4d53ae5ecfef7f7ad39623f5982b029a09306b36b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
