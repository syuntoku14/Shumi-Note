{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 価値反復法が失敗するケース\n",
    "\n",
    "参考：\n",
    "* [Csaba講義](https://rltheory.github.io/lecture-notes/planning-in-mdps/lec4/)\n",
    "\n",
    "* 状態の数：3\n",
    "* 行動の数：2\n",
    "* 決定的遷移\n",
    "* 任意の方策に対してその価値が$[0, 1/(1-\\gamma)]$に収まる\n",
    "\n",
    "ようなMDPのクラスを考えましょう．このとき，価値反復法で最適方策を見つけるためには，無限回のイテレーション複雑度が必要になる最悪のMDPが存在します．\n",
    "\n",
    "ここで，「イテレーション複雑度」は「$k$ステップ以降で方策$\\pi_k$が最適になるような$k$の値のことです．\n",
    "\n",
    "**証明**\n",
    "\n",
    "次のMDPを考えましょう．\n",
    "\n",
    "![vi_mdp](figs/vi_mdp.png)\n",
    "\n",
    "* ここで，$R$は収益のことではなく，$[0, \\gamma / (1-\\gamma)]$ から選択されるパラメータです．\n",
    "* $s_1$での最適方策は$a_0$であり，$\\gamma / (1-\\gamma)$の収益になります\n",
    "\n",
    "さて，$v_0=0$でスタートする価値反復法を考えてみましょう．このとき，$s_1$では$a_1$が選択されます．\n",
    "$R$が$\\gamma / (1-\\gamma)$に近づくにつれて，価値反復法が無限に$a_1$を選択してしまうことを示しましょう．\n",
    "\n",
    "まず，$v_k(s_0)=0$および$v_k\\left(s_2\\right)=\\frac{\\gamma}{1-\\gamma}\\left(1-\\gamma^k\\right)$が任意の$k \\geq 0$で成立します．このとき，\n",
    "$\\pi_k(s_1)=a_1$は$R > v_k(s_2)$である限り成立します．\n",
    "\n",
    "よって，もし価値反復法に$k_0$以上のイテレーションを経過させたいのであれば，\n",
    "\n",
    "$$\n",
    "R=\\frac{v^*\\left(s_2\\right)+v_{k_0}\\left(s_2\\right)}{2}<\\gamma /(1-\\gamma)\n",
    "$$\n",
    "\n",
    "を選択すれば良いです．\n",
    "\n",
    "---\n",
    "\n",
    "この結果は，「厳密な最適方策を計算する」ことに関しては，価値反復法は方策反復法に劣ることを示してます．\n",
    "しかし，価値反復法は$\\log(1/\\delta)$回の更新で$\\delta$-suboptimal方策を計算することができます．\n",
    "\n",
    "* [Csaba講義](https://rltheory.github.io/lecture-notes/planning-in-mdps/lec4/)でも言ってますが，基本的には「近似最適方策を計算できるなら，別に良くない？ってスタンス」でものごとを考えたほうが良さそうです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
