{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low Inherent Bellman Errorと強化学習\n",
    "\n",
    "参考\n",
    "* [Learning Near Optimal Policies with Low Inherent Bellman Error](https://arxiv.org/abs/2003.00153)\n",
    "\n",
    "テーブル形式MDPでは様々な良い強化学習の結果が得られていますが，実用上はなんらかの関数近似を入れる必要が出てきます．\n",
    "一番便利なのは線形関数近似ですが，最適価値が線形に表されるとしても，Fitted Q iterationは発散することがあります（[RL_General_fitted_Q_iteration.ipynb](RL_General_fitted_Q_iteration.ipynb)でやりましたね）．\n",
    "\n",
    "どのようなケースならば収束や有限サンプルでの性能が保証されるでしょうか？\n",
    "すべての$Q^\\pi$が線形に表現できる場合については結果が出ています（[Least-Squares Policy Iteration](https://www.jmlr.org/papers/v4/lagoudakis03a.html)など）．\n",
    "この線形に近似できる条件を**LSPI条件**，と呼ぶことにします．\n",
    "\n",
    "一方で，近似誤差が$\\left\\|\\widehat{Q}^\\pi-Q^\\pi\\right\\| \\leqslant \\epsilon$であり，$\\epsilon \\gtrsim 1 / \\sqrt{d}$のとき，サンプル効率は$d$について指数的になることが知られています（[Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?](https://arxiv.org/abs/1910.03016)）．\n",
    "\n",
    "上で紹介したものは方策反復ベースのアルゴリズムについてですが，価値反復についても結果が得られています．\n",
    "それを紹介するために，inherent Bellman errorを導入しましょう．\n",
    "\n",
    "---\n",
    "\n",
    "準備\n",
    "\n",
    "* ベルマン最適作用素：$\\mathcal{T}_t\\left(Q_{t+1}\\right)(s, a)=r_t(s, a)+\\mathbb{E}_{s^{\\prime} \\sim p_t(s, a)} \\max _{a^{\\prime}} Q_{t+1}\\left(s^{\\prime}, a^{\\prime}\\right)$\n",
    "* ベクトルの集合：$\\mathcal{B}_t \\stackrel{\\text { def }}{=}\\left\\{\\theta_t \\in \\mathbb{R}^{d_t}|| \\phi_t(s, a)^{\\top} \\theta_t \\mid \\leqslant D, \\forall(s, a)\\right\\}$\n",
    "* 価値関数：$Q_t\\left(\\theta_t\\right)(s, a)=\\phi_t(s, a)^{\\top} \\theta_t, \\quad V_t\\left(\\theta_t\\right)=\\max _a \\phi_t(s, a)^{\\top} \\theta_t$\n",
    "* $\\mathcal{Q}_t \\stackrel{\\text { def }}{=}\\left\\{Q_t\\left(\\theta_t\\right) \\mid \\theta_t \\in \\mathcal{B}_t\\right\\}, \\mathcal{V}_t \\stackrel{\\text { def }}{=}\\left\\{V_t\\left(\\theta_t\\right) \\mid \\theta_t \\in \\mathcal{B}_t\\right\\}$\n",
    "\n",
    "このとき，（線形な特徴ベクトル$\\phi$についての）Inherent Bellman errorは次で定義されます．\n",
    "\n",
    "次を最大にする$t \\in [H]$における値であり，$\\mathcal{I}$で表記されます．\n",
    "\n",
    "$$\n",
    "\\sup _{\\theta_{t+1} \\in \\mathcal{B}_{t+1}} \\inf _{\\theta_t \\in \\mathcal{B}_t} \\sup _{(s, a) \\in \\mathcal{S} \\times \\mathcal{A}} \\left| \\phi_t(s, a)^{\\top} \\theta_t - \\left(\\mathcal{T}_t Q_{t+1}\\left(\\theta_{t+1}\\right)\\right)(s, a) \\right|.\n",
    "$$\n",
    "\n",
    "ちなみにボーナスの追加などを考慮して，クリップした値について考える場合もあります（[Optimism in Reinforcement Learning with Generalized Linear Function Approximation](https://arxiv.org/abs/1912.04136)など．）\n",
    "クリップしたほうが強い仮定になります．\n",
    "\n",
    "$\\mathcal{I}=0$の条件をLSVI条件と呼ぶことにします（多分Linear Bellman Completenessとも呼ばれる？）．\n",
    "LSVI条件が成立しているとき，次のように報酬が線形になり，また，次状態の価値も線形に表現することができます．\n",
    "\n",
    "$$\n",
    "\\inf _{\\theta_t^R \\in \\mathcal{B}_t} \\sup _{(s, a) \\in \\mathcal{S} \\times \\mathcal{A}}\\left|r_t(s, a)-\\phi_t(s, a)^{\\top} \\theta_t^R\\right|=0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sup _{\\theta_{t+1} \\in \\mathcal{B}_{t+1}} \\inf _{\\theta_t^P \\in \\mathcal{B}_t} \\sup _{(s, a) \\in \\mathcal{S} \\times \\mathcal{A}} | \\mathbb{E}_{s^{\\prime} \\sim p_t(s, a)} V_{t+1}\\left(\\theta_{t+1}\\right)\\left(s^{\\prime}\\right) \n",
    "-\\phi_t(s, a)^{\\top} \\theta_t^P | =0 .\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**LSPIとLSVI条件の違い**\n",
    "\n",
    "* [Linear MDP](RL_General_linearMDP.ipynb.ipynb)は$\\mathcal{I}=0$満たしますが，$\\mathcal{I}=0$はlinear MDPとは限りません．その意味で，Linear MDPを一般化しています．\n",
    "* また，同様の意味で，LSPI条件はLinear MDPを一般化しています．\n",
    "* LSVI条件が満たされているとき，Bellman rankは特徴ベクトルの次元と一致します．つまり，Bellman rankはLSVI条件を一般化しています．\n",
    "* LSVI条件とLSPI条件は別物です．実際，次が成立します．\n",
    "\n",
    "$\\mathcal{I}=0$だが，\n",
    "$$\n",
    "\\exists \\pi, \\exists t \\in[H], \\nexists \\theta_t^\\pi \\in \\mathbb{R}^{d_t} \\quad \\text { s.t. } \\quad Q_t^\\pi=\\phi_t(s, a)^{\\top} \\theta_t .\n",
    "$$\n",
    "のMDPが存在する．また，\n",
    "$$\n",
    "\\forall \\pi, \\forall t \\in[H], \\exists \\theta_t^\\pi \\quad \\text { that satisfies } \\quad Q_t^\\pi(s, a)=\\phi_t(s, a)^{\\top} \\theta_t^\\pi\n",
    "$$\n",
    "だが，$\\mathcal{I} > 0$のMDPが存在する．\n",
    "\n",
    "**$\\Rightarrow$の証明**\n",
    "\n",
    "まず最初のStatementについて証明しましょう．\n",
    "\n",
    "次のMDPを考えます．\n",
    "* 状態空間が２つのnon-communicatingな集合$A=s_1^A, \\ldots, s_H^A$と$B=s_1^B, \\ldots, s_H^B$からなる．\n",
    "* 初期状態は$s_1^A$もしくは$s_1^B$とする．\n",
    "* $s^A_H$と$s^B_H$以外の状態では行動は１つしかない．\n",
    "* $i \\in [H-1]$について，$s^A_i$から$s^A_{i+1}$への遷移は決定的．$B$も同様．\n",
    "* $s^A_H$と$s^B_H$の両方では，２つの行動が存在する．$(s^A_H, 0)$および$(s^B_H, 0)$は報酬$0$，$(s^A_H, 1)$および$(s^B_H, 1)$は報酬$1$を出す．また，どちらもその時点でエピソード終了．\n",
    "* パラメータについて\n",
    "  * 任意の$<H$な状態について，$\\phi_t(\\cdot, \\cdot)=1$\n",
    "  * $H$では，$\\phi_H\\left(s_H^A, 0\\right)=\\phi_H\\left(s_H^B, 0\\right)=0$ and $\\phi_H\\left(s_H^A, 1\\right)=\\phi_H\\left(s_H^B, 1\\right)=1$とする\n",
    "\n",
    "\n",
    "このとき，$\\mathcal{I}=0$になります．\n",
    "任意の$t\\in [H-2]$について，$\\theta_t=\\theta_{t+1}$とすれば，\n",
    "$$\n",
    "\\bar{Q}_t\\left(s_t^A, \\cdot\\right)=\\bar{Q}_t\\left(s_t^B, \\cdot\\right)=\\bar{V}_{t+1}\\left(s_{t+1}^B\\right)=\\bar{V}_{t+1}\\left(s_{t+1}^A\\right)\n",
    "$$\n",
    "が成り立ちます（行動が一つしかないので）．$t=H-1$では$\\theta_t=\\max\\{0, \\theta_H\\}$とすれば良いです．\n",
    "\n",
    "つまり，任意の$\\theta_{t+1}$に対して$Q_{t+1}$を考えると，$Q_t(\\cdot, \\cdot)=\\mathcal{T}_t Q_{t+1}(\\cdot, \\cdot)$であるような$\\theta_t$が存在します（$\\theta_t=\\theta_{t+1}$にするだけ．）\n",
    "また，$t=H$はLinear banditと同じなので，$\\mathcal{I}=0$です．\n",
    "\n",
    "ここで，AとBの最後のステップで異なる行動をとる方策を考えましょう．つまり，\n",
    "$$\n",
    "\\pi_H^x\\left(s_H^A\\right)=1 \\neq 0=\\pi_H^x\\left(s_H^B\\right)\n",
    "$$\n",
    "とします．\n",
    "すると，この方策はAとBで収益が異なるので，\n",
    "$$\n",
    "Q_t^{\\pi^x}\\left(s_t^A, \\cdot\\right) \\neq Q_t^{\\pi^x}\\left(s_t^B,, \\cdot\\right)\n",
    "$$\n",
    "です．しかし，どんな$\\theta_t$を持ってきても，\n",
    "$$\n",
    "Q_t^{\\pi^x}(s_t^A, 0) = \\theta_t =\\theta_t \\phi_t(s_t^A, 0) = \n",
    "\\theta_t \\phi_t(s_t^B, 0) = \\theta_t\n",
    "= Q_t^{\\pi^x}(s_t^B, 0)\n",
    "$$\n",
    "が成立するので，これは上と矛盾します．よって，このMDPでは任意の$\\pi$について$Q^\\pi$が線形に実現可能であるわけではないです．\n",
    "\n",
    "\n",
    "**$\\Leftarrow$の証明**\n",
    "\n",
    "次のChain MDPを考えます．\n",
    "* 状態空間：$s_1, \\dots, s_H$．ここで$s_1$は初期状態．\n",
    "* 任意の行動は次の状態に遷移する：$s_i\\to s_{i+1}$．また，報酬は発生しない．\n",
    "* それぞれの状態では特徴ベクトル$\\phi_t(\\cdot,-1)=-1$ and $\\phi_t(\\cdot,+1)=+1$が割り当てられている．\n",
    "\n",
    "<!-- どの報酬もゼロなので，このMDPでは\n",
    "$$\n",
    "Q_t^\\pi(s, a)=\\phi_t(s, a)^{\\top} \\theta_t^\\pi \\text { with } \\theta_t^\\pi=0\n",
    "$$\n",
    "が成立することに注意しましょう． -->\n",
    "\n",
    "ここで，例えば$\\theta_{t+1}=1$について考えてみましょう．このとき，\n",
    "$$\n",
    "V_{t+1}\\left(\\theta_{t+1}\\right)\\left(s_{t+1}\\right)=\\max _a \\phi_{t+1}\\left(s_{t+1}, a\\right)^{\\top} \\theta_{t+1}=1\n",
    "$$\n",
    "が成立します．報酬ゼロなので，\n",
    "$$\n",
    "\\left(\\mathcal{T}_t V_{t+1}\\left(\\theta_{t+1}\\right)\\right)\\left(s_t, \\cdot\\right)=V_{t+1}\\left(\\theta_{t+1}\\right)\\left(s_{t+1}\\right)=1\n",
    "$$\n",
    "も成立します．\n",
    "\n",
    "一方で，線形な関数近似器の場合，$Q_t(\\theta_t)(s_t, +1)=-Q_t(\\theta_t)(s_t, -1)$が必ず成立することに注意しましょう．\n",
    "どちらの行動を選んでも同じ状態に遷移するので，$\\mathcal{I}=0$であるためには，\n",
    "$$\n",
    "Q_t(\\theta_t)(s_t, +1)=Q_t(\\theta_t)(s_t, -1)\n",
    "=\n",
    "\\left(\\mathcal{T}_t V_{t+1}\\left(\\theta_{t+1}\\right)\\right)\\left(s_t, \\cdot\\right)=V_{t+1}\\left(\\theta_{t+1}\\right)\\left(s_{t+1}\\right)=1\n",
    "$$\n",
    "でなければなりませんが，これは$Q_t(\\theta_t)(s_t, +1)=-Q_t(\\theta_t)(s_t, -1)$であることに矛盾します．\n",
    "\n",
    "---\n",
    "\n",
    "**コメント**：ベルマン最適方程式についてのCompletenessが成り立つならベルマン期待方程式についても成り立つ気がしたけど，それは間違いぽい．もしそれが言えてしまうと，$\\mathcal{I}=0$なら$Q^\\pi$-realizableになってしまうはず．これは$\\Rightarrow$の証明と矛盾する．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "このLSVI条件について，\n",
    "* G-optimal design で学習するアプローチ：[LSVIアルゴリズム](https://rltheorybook.github.io/)\n",
    "* オンラインの設定：[ELEANORアルゴリズム](https://arxiv.org/abs/2003.00153)\n",
    "\n",
    "などが存在しています．ELEANORアルゴリズムはMinimax最適です．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
