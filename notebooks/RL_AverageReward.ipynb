{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 平均報酬強化学習\n",
    "\n",
    "参考：\n",
    "* [Average-reward model-free reinforcement learning: a systematic review and literature mapping](https://arxiv.org/abs/2010.08920)\n",
    "* [Markov Decision Processes: Discrete Stochastic Dynamic Programming](https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316887)\n",
    "* [Average reward reinforcement learning: Foundations, Algorithms, and Empirical Results](https://link.springer.com/content/pdf/10.1007/BF00114727.pdf)\n",
    "\n",
    "今回は平均報酬強化学習について学びます．また，最初に無限ホライゾンの問題設定について少し学びます．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDPの構造\n",
    "\n",
    "参考：\n",
    "* [Average reward reinforcement learning: Foundations, Algorithms, and Empirical Results](https://link.springer.com/content/pdf/10.1007/BF00114727.pdf)\n",
    "\n",
    "後で見る割引問題では気にしなくても良いですが（多分擬似的なホライゾンができるため？），平均報酬ではMDPの構造を気にしないと困ることがあります．\n",
    "ここでは，いくつか重要な用語や定義についてまとめます．\n",
    "\n",
    "---\n",
    "\n",
    "次を満たすとき，状態$x$と$y$は方策$\\pi$について**communicate**である，と言います．\n",
    "* 方策$\\pi$において，$x\\to y$および$y \\to x$の確率が非ゼロ．\n",
    "\n",
    "逆に，次を満たすとき，状態$x$は方策$\\pi$について**recurrent**である，と言います．\n",
    "* $\\pi$によって$x$から出発してそこに戻る確率が１\n",
    "\n",
    "つまり，recurrentな状態は$\\pi$において永遠に訪問されます．\n",
    "\n",
    "---\n",
    "\n",
    "次を満たすとき，状態$x$は**transient**である，と言います．\n",
    "* 任意の方策について，$x$から出発してそこに戻る確率が$<1$である．\n",
    "\n",
    "つまり，transientな状態は有限な回数しかそこに戻ってこれません．\n",
    "\n",
    "---\n",
    "\n",
    "次を満たす状態の集合$X$は，$\\pi$において**ergodic**もしくは**recurrent**である，と言います．\n",
    "* $X$の中の全ての状態はrecurrentであり，また，お互いにcommunicateである．\n",
    "\n",
    "もし全ての状態によってergodicな状態集合が形成されれば，そのマルコフ連鎖は**irreducible**（既約）である，と言います．\n",
    "\n",
    "---\n",
    "\n",
    "$p_{s, s}^n(\\pi)$を，方策$\\pi$が$s$から始まって$n$ステップで再び$s$に至る確率とします．\n",
    "次を状態$s$における方策$\\pi$の**period**と呼びます．\n",
    "* $p^n_{s, s}(\\pi) > 0$であるような全ての$n$の最大公約数\n",
    "\n",
    "---\n",
    "\n",
    "次を満たすとき，そのMDPは**ergodic**もしくは**recurrent**である，と言います．\n",
    "* 任意の（決定的かつ定常な）方策が，単一のrecurrentな状態集合をもつ\n",
    "\n",
    "次を満たすとき，そのMDPは**unichain**である，と言います．\n",
    "* 任意の方策が，単一のrecurrentな状態集合と(空でも良い)transientな状態集合をもつ\n",
    "\n",
    "次を満たすとき，そのMDPは**communicating**である，と言います．\n",
    "* 全ての状態のペアがcommunicateになるような定常な方策が存在する\n",
    "\n",
    "次を満たすとき，そのMDPは**multichain**である，と言います．\n",
    "* ２つ以上のrecurrentな状態集合をもつような定常な方策が存在する．\n",
    "\n",
    "---\n",
    "\n",
    "recurrentやtransientな状態の例については論文のFigure 2などを参照してください．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 無限ホライゾンの目的関数について\n",
    "\n",
    "* [Markov Decision Processes: Discrete Stochastic Dynamic Programming](https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316887)の５章\n",
    "\n",
    "無限ホライゾンでは，大きく３種類の目的関数を考えることができます．\n",
    "\n",
    "1. 期待総報酬：$v^\\pi(s) \\equiv \\lim _{N \\rightarrow \\infty} E_s^\\pi\\left\\{\\sum_{t=1}^N r\\left(X_t, Y_t\\right)\\right\\}=\\lim _{N \\rightarrow \\infty} v_{N+1}^\\pi(s)$\n",
    "    * これは$+\\infty$もしくは$-\\infty$になることがあります．\n",
    "2. 期待割引総報酬：$v_\\lambda^\\pi(s) \\equiv \\lim _{N \\rightarrow \\infty} E_s^\\pi\\left\\{\\sum_{t=1}^N \\lambda^{t-1} r\\left(X_t, Y_t\\right)\\right\\}$\n",
    "    * これは$\\sup \\sup |r(s, a)|=M<\\infty$ $s \\in S \\quad a \\in A_s$である限り，極限が存在します．\n",
    "3. 期待平均報酬（ゲインとも呼ばれます）：$g^\\pi(s) \\equiv \\lim _{N \\rightarrow \\infty} \\frac{1}{N} E_s^\\pi\\left\\{\\sum_{t=1}^N r\\left(X_t, Y_t\\right)\\right\\}=\\lim _{N \\rightarrow \\infty} \\frac{1}{N} v_{N+1}^\\pi(s)$\n",
    "    * これは極限が存在しない場合があります．しかし，次のlimsupとliminfは必ず存在します．\n",
    "    * $g^\\pi(s) \\equiv \\liminf _{N \\rightarrow \\infty} \\frac{1}{N} v_{N+1}^\\pi(s), \\quad g_{+}^\\pi(s) \\equiv \\limsup _{N \\rightarrow \\infty} \\frac{1}{N} v_{N+1}^\\pi(s)$\n",
    "    * UnichainなMDPでは，任意の方策について，ゲインが状態に非依存であることに注意しましょう．つまり，$\\rho^\\pi(x)=\\rho^\\pi(y)=\\rho^\\pi$です．\n",
    "        * recurrentな状態は有限回しか訪問されないので，平均を取ると影響が消えます\n",
    "        * recurrentな状態は永遠に訪問されるので，状態間で平均報酬が変わりません．\n",
    "\n",
    "期待平均報酬が存在しない例を見てみましょう．\n",
    "$S=\\{1, 2, \\dots\\}$とし，各$s \\in \\mathcal{S}$に対して，$p\\left(s+1 \\mid s, a_s\\right)=1, p\\left(j \\mid s, a_s\\right)=0$ for $j \\neq s+1$および$r\\left(s, a_s\\right)=(-1)^{s+1} s$になるような$a_s$が一つだけ存在するとします．\n",
    "\n",
    "![no-limit](figs/no-limit-MDP.png)\n",
    "\n",
    "このとき，方策は一つしか存在しません．それを$\\pi$としましょう．また，$r_N(s)=0$としましょう．また，$v_1^\\pi(1)=0, v_2^\\pi(1)=1$とします．このとき，任意の$N > 1$で\n",
    "$$\n",
    "v_N^\\pi(1)=\\left\\{\\begin{array}{ll}\n",
    "k & N=2 k \\\\\n",
    "-k & N=2 k+1,\n",
    "\\end{array} k=1,2, \\ldots\\right.\n",
    "$$\n",
    "なので，\n",
    "$$\n",
    "\\lim \\sup _{N \\rightarrow \\infty} v_N^\\pi(1)=+\\infty \\quad \\text { and } \\quad \\liminf _{N \\rightarrow \\infty} v_N^\\pi(1)=-\\infty\n",
    "$$\n",
    "であるため，$s=1$における期待総報酬は存在しません．また，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&g_{+}^\\pi(1)=\\limsup _{N \\rightarrow \\infty} N^{-1} v_{N+1}^\\pi(1)=\\frac{1}{2}\\\\\n",
    "&g_{-}^\\pi(1)=\\liminf _{N \\rightarrow \\infty} N^{-1} v_{N+1}^\\pi(1)=-\\frac{1}{2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "なので，$s=1$におけるゲインも存在しません．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 期待総報酬問題について\n",
    "\n",
    "この問題では$v^\\pi(s)=\\lim _{N \\rightarrow \\infty} v_N^\\pi(s)$を達成する$\\pi$について扱います．\n",
    "しかし，多くの場合，この$v^\\pi(s)$は発散することがあります．\n",
    "そこで，次の問題だけ考えることでこれを回避します：\n",
    "1. 任意の方策で$\\lim _{N \\rightarrow \\infty} v_N^\\pi(s)$が存在する問題\n",
    "2. 極限が存在し，かつそれが有限であり，また，任意の方策に対して$v^{\\pi^*}(s) \\geq \\limsup _{N \\rightarrow \\infty} v_N^\\pi(s)$を満たすような方策$\\pi^\\star$が最低でも一つ存在する問題\n",
    "3. TODO: Use optimality criteria which are sensitive to the rate at which $v^\\pi_N(s)$ diverges.\n",
    "\n",
    "**1が満足される条件**\n",
    "\n",
    "$$v_{+}^\\pi(s) \\equiv E_s^*\\left\\{\\sum_{t=1}^{\\infty} \\max\\{r\\left(X_t, Y_t\\right), 0\\}\\right\\}$$\n",
    "$$v_{-}^\\pi(s) \\equiv E_s^*\\left\\{\\sum_{t=1}^{\\infty} \\max\\{-r\\left(X_t, Y_t\\right), 0\\}\\right\\}$$\n",
    "\n",
    "とします．そして，\n",
    "* 任意の$\\pi \\in \\Pi^{\\mathrm{HR}}$について，$v_{+}^\\pi(s)$ か $v_{-}^\\pi(s)$が有限\n",
    "\n",
    "であれば，\n",
    "\n",
    "$$\n",
    "v^\\pi(s)=v_{+}^\\pi(s)-v_{-}^\\pi(s)\n",
    "$$\n",
    "\n",
    "となり，極限が存在します．\n",
    "また，特に次のケースを区別して考えます\n",
    "\n",
    "1. (Positive bounded model) 任意の$s \\in S$について，$r(s, a) \\geq 0$な$a \\in A_s$が存在かつ$v^\\pi_+(s)$が任意の$\\pi \\in \\Pi^{HR}$で有限\n",
    "    * この仮定のもとでは，任意の方策は非正の報酬の状態に吸収されます（そうでないと価値が有限になりません）\n",
    "    * 問題の例：\n",
    "        * [最適停止問題](https://ja.wikipedia.org/wiki/%E6%9C%80%E9%81%A9%E5%81%9C%E6%AD%A2%E5%95%8F%E9%A1%8C)\n",
    "        * 何らかの状態に到達する確率を最大化したい問題（ギャンブルなど）\n",
    "        * 何らかの状態に到達するまでの時間を可能な限り伸ばしたい問題（コンピュータゲームなど）\n",
    "2. (Negative model) 任意の$s \\in S$と$a \\in A_s$について，$r(s, a) \\leq 0$かつ，何らかの$\\pi \\in \\Pi^{HR}$について，$v^\\pi(s) > -\\infty$\n",
    "    * 仮定を満たす方策は報酬が０の状態に吸収されます（そうでないと価値が有限になりません）\n",
    "    * 問題の例：\n",
    "        * 何らかの状態に到達しない確率を最大化したい問題\n",
    "        * 何らかの状態に到達するまでの時間を最小化する問題\n",
    "        * 最適停止問題\n",
    "3. (Convergent model) 任意の$s \\in S$と任意の$\\pi \\in \\Pi^{HR}$について，$v_{+}^\\pi(s)$ and $v_{-}^\\pi(s)$が有限\n",
    "\n",
    "ちなみに，Positive bounded modelとNegative modelは符号を**逆転しただけではありません**．\n",
    "実際，Positive bounded modelでは報酬を最大化する一方で，Negative modelでは総報酬をゼロに近づけることが目標です．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 期待割引総報酬問題について\n",
    "\n",
    "割引問題は経済のモデルなどで便利です．また，かなり充実した理論が存在し，様々な解法が存在します．\n",
    "よく知られた話ですが，割引率はランダムなホライゾン$\\nu$とみなすこともできます．\n",
    "これは例えば経済モデルにおける工場の破産や生産設備の故障，生物系のモデルにおける生き物の死亡などをモデル化するときに便利です．\n",
    "\n",
    "$$\n",
    "v_\\nu^\\pi(s) \\equiv E_s^\\pi\\left[E_\\nu\\left\\{\\sum_{t=1}^\\nu r\\left(X_t, Y_t\\right)\\right\\}\\right]\n",
    "$$\n",
    "\n",
    "を，ランダムなホライゾン$\\nu$に対しての期待価値とします．ここで，$\\nu$が帰化分布に従う場合を考えてみましょう．つまり，\n",
    "\n",
    "$$\n",
    "P\\{\\nu=n\\}=(1-\\lambda) \\lambda^{n-1}, \\quad n=1,2, \\ldots\n",
    "$$\n",
    "\n",
    "です．幾何分布は上で紹介したような「故障のモデル」などを表すときに便利です．\n",
    "証明はMarkov decision processesのProposition 5.3.1に任せますが，$v_\\nu^\\pi(s)=v_\\lambda^\\pi(s)$ for all $s \\in S$が成立します．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最適性の評価指標\n",
    "\n",
    "* [Markov Decision Processes: Discrete Stochastic Dynamic Programming](https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316887)の５章\n",
    "* [Average reward reinforcement learning: Foundations, Algorithms, and Empirical Results](https://link.springer.com/content/pdf/10.1007/BF00114727.pdf)\n",
    "\n",
    "上で見た目的関数に対応する最適性の指標について見てみましょう．\n",
    "\n",
    "---\n",
    "\n",
    "次を満たす$\\pi^\\star$は**総報酬最適**である，といいます\n",
    "\n",
    "$$\n",
    "v^{\\pi^*}(s) \\geq v^\\pi(s) \\quad \\text { for each } s \\in S \\text { and all } \\pi \\in \\Pi^{\\mathrm{HR}}\n",
    "$$\n",
    "\n",
    "この指標は\"positive bounded\", \"negative\"，そして\"convergent model\"に対して適用できます．\n",
    "\n",
    "また，次を**MDPの価値**，と呼びます\n",
    "\n",
    "$$\n",
    "v^*(s) \\equiv \\sup _{\\pi \\in I 1^{\\mathrm{HR}}} v^\\pi(s)\n",
    "$$\n",
    "\n",
    "$v^{\\pi^*}(s)=v^*(s) \\quad$ for all $s \\in S$であるならば，最適方策$\\pi^* \\in \\Pi^K(K=$ HR, HD, MR, or MD $)$が存在します．\n",
    "\n",
    "---\n",
    "\n",
    "次を満たす方策$\\pi^\\star$は**割引最適**である，といいます．\n",
    "割引率$0 \\leq \\lambda < 1$について，\n",
    "\n",
    "$$\n",
    "v_\\lambda^{\\pi^*}(s) \\geq v_\\lambda^\\pi(s) \\quad \\text { for each } s \\in S \\text { and all } \\pi \\in \\Pi^{\\mathrm{HR}}\n",
    "$$\n",
    "\n",
    "また，割引モデルに置いて，MDPの価値は\n",
    "\n",
    "$$\n",
    "v_\\lambda^*(s) \\equiv \\sup _{\\pi \\in \\Pi^{\\mathrm{HR}}} v_\\lambda^\\pi(s)\n",
    "$$\n",
    "\n",
    "によって定義され，上と同様に，次を満たすときに割引最適な方策が存在します．\n",
    "\n",
    "$$\n",
    "v_\\lambda^{\\pi *}(s)=v_\\lambda^*(s) \\quad \\text { for all } s \\in S\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "次を満たす方策$\\pi^\\star$は**平均(ゲイン)最適**である，といいます．\n",
    "\n",
    "$$\n",
    "g^{\\pi^*}(s) \\geq g^\\pi(s) \\quad \\text { for each } s \\in S \\text { and all } \\pi \\in \\Pi^{\\mathrm{HR}}\n",
    "$$\n",
    "\n",
    "MDPの価値と最適方策の存在性は今までと同様です．\n",
    "\n",
    "平均報酬の場合は，最適方策が存在しない場合があります（上で見た例など）．\n",
    "そのような場合は，次を満たす方策のことを最適方策とします．\n",
    "\n",
    "$$\n",
    "g_{-\\pi^*}^{\\pi^*}(s)=\\liminf _{N \\rightarrow \\infty} \\frac{1}{N} v_{N+1}^{\\pi^*}(s) \\geq \\limsup _{N \\rightarrow \\infty} \\frac{1}{N} v_{N+1}^\\pi(s)=g_{+}^\\pi(s)\n",
    "$$\n",
    "\n",
    "また，次を満たす方策をそれぞれ上極限，下極限最適方策，と呼びます．\n",
    "\n",
    "$$\n",
    "g_{+}^{\\pi^*}(s) \\geq g_{+}^\\pi(s)\n",
    "\\quad\n",
    "g_{-}^{\\pi^*}(s) \\geq g_{-}^\\pi(s)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "TODO: Overtaking optimal方策について言及しよう．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## バイアスについて\n",
    "\n",
    "平均(ゲイン)最適方策の評価では，その途中経過は無視して，最終的に収束したときの報酬についてだけ考えます（途中経過は$\\to \\infty$で消えます）．\n",
    "これは問題によってはあまり嬉しくありません．例えば迷路の問題を考えて見ましょう．ゴールすると$+1$，それ以外は$-1$の報酬とします．\n",
    "このとき，ゴールに至るようなどんな方策も$+1$の平均報酬になり，最適方策の区別がつかなくなります．\n",
    "一方で迷路のような問題ではなるべく早くゴールすることが望ましいです．\n",
    "\n",
    "そこで，**バイアス最適**の概念が役に立ちます．\n",
    "\n",
    "---\n",
    "\n",
    "次の関数をバイアス関数（もしくはrelative関数）と言います．\n",
    "\n",
    "$$v_b^\\pi(s) = \\lim _{N \\rightarrow \\infty} \\frac{1}{N} E_s^\\pi\\left\\{\\sum_{t=1}^N r\\left(X_t, Y_t\\right) - g^\\pi(s) \\right\\}$$\n",
    "\n",
    "また，次も成立します．\n",
    "\n",
    "$$v_b^\\pi(s) - v_b^\\pi(x) = \\lim _{N \\rightarrow \\infty} \\frac{1}{N} E_s^\\pi\\left\\{\\sum_{t=1}^N r\\left(X_t, Y_t\\right)\\right\\} - \n",
    "\\frac{1}{N} E_x^\\pi\\left\\{\\sum_{t=1}^N r\\left(X_t, Y_t\\right)\n",
    "\\right\\}$$\n",
    "\n",
    "次を満たすような$\\pi^\\star$を**バイアス最適**（T-optimalとも呼ばれます）である，といいます\n",
    "* $v_b^{\\pi^\\star}(s) \\geq v_b^\\pi(s)$が全ての$\\pi$と$s$\n",
    "\n",
    "---\n",
    "\n",
    "バイアス関数と割引価値は，[ローラン展開](https://manabitimes.jp/math/2627)を使うことで変形ができます．\n",
    "\n",
    "まず，割引率$\\lambda$の代わりに，$\\lambda=(1+\\rho)^{-1}$ or $\\rho=(1-\\lambda) \\lambda^{-1}$なる変数$\\rho$を使います．\n",
    "つまり，$1+\\rho$が$\\lambda$の逆数です．\n",
    "このとき，\n",
    "\n",
    "$$\n",
    "v_\\lambda=(I-\\lambda P)^{-1} r=(1+\\rho)(\\rho I+[P-I])^{-1} r\n",
    "$$\n",
    "\n",
    "が明らかに成立します．\n",
    "ここで，ローラン展開を利用すると，つぎが成立します．\n",
    "\n",
    "$S$が有限とする．$\\nu$を$I-P$のうち，絶対値が最小の非ゼロの固有値とする．このとき，任意の$0 < \\rho < |\\nu|$について，\n",
    "\n",
    "$$\n",
    "v_\\lambda=(1+\\rho)\\left[\\rho^{-1} y_{-1}+\\sum_{n=0}^{\\infty} \\rho^n y_n\\right]\n",
    "$$\n",
    "\n",
    "が成立します．ここで，\n",
    "$$\n",
    "y_{-1}=P^* r=g, y_0=H_P r=b, \\text { and } y_n=(-1)^n H_P^{n+1} r \\text { for } n=1,2, \\ldots\n",
    "$$\n",
    "です．$H_P$は$H_P=C-\\lim _{N \\rightarrow \\infty} \\sum_{n=0}^{N-1}\\left(P^N-P^*\\right)$で定義され，deviation matrixと呼ばれます．\n",
    "\n",
    "TODO: 証明．\n",
    "\n",
    "上の補題から，$g$と$b$をゲインとバイアスとすると，$S$が有限かつ報酬が有限ならば，\n",
    "$$\n",
    "v_\\lambda=(1-\\lambda)^{-1} g+b+f(\\lambda)\n",
    "$$\n",
    "が成立します．ここで$f(\\lambda)$は$\\lambda \\to 1$でゼロに収束するベクトルです．\n",
    "\n",
    "TODO: 証明\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ここから下は[Average-reward model-free reinforcement learning: a systematic review and literature mapping](https://arxiv.org/abs/2010.08920)を参照\n",
    "\n",
    "\n",
    "## 定常分布について\n",
    "\n",
    "次を定義します：\n",
    "* 方策から誘導されるマルコフ連鎖：$\\boldsymbol{P}_\\pi \\in[0,1]^{|\\mathcal{S}| \\times|\\mathcal{S}|}$\n",
    "    * その$t$回繰り返したもの：$\\boldsymbol{P}_\\pi^t\\left[s_0, s\\right]=\\operatorname{Pr}\\left\\{S_t=s \\mid s_0, \\pi\\right\\}=: p_\\pi^t\\left(s \\mid s_0\\right)$\n",
    "\n",
    "このとき，このマルコフ連鎖の収束先は\n",
    "$$p_\\pi^{\\star}\\left(s \\mid s_0\\right)=\\lim _{t_{\\max } \\rightarrow \\infty} \\frac{1}{t_{\\max }} \\sum_{t=0}^{t_{\\max }-1} p_\\pi^t\\left(s \\mid s_0\\right)=\\lim _{t_{\\max } \\rightarrow \\infty} p_\\pi^{t_{\\max }}\\left(s \\mid s_0\\right), \\quad \\forall s \\in \\mathcal{S}$$\n",
    "\n",
    "を満たします．この最初のlimitは有限なMDPならば必ず存在し，２つ目のLimitはMDPがaperiodicであれば存在します（TODO: 証明）\n",
    "\n",
    "また，$p_\\pi^\\star(s|s_0)$をベクトル化したものを$\\boldsymbol{p}_\\pi^{\\star} \\in[0,1]^{|\\mathcal{S}|}$とします．\n",
    "これは\n",
    "$$\\left(\\boldsymbol{p}_\\pi^{\\star}\\right)^{\\top} \\boldsymbol{P}_\\pi=\\left(\\boldsymbol{p}_\\pi^{\\star}\\right)^{\\top}$$\n",
    "を満たします"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ゲインについて\n",
    "\n",
    "平均報酬はゲインとも呼ばれ，次で定義され，以下の変形が成立します：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_g\\left(\\pi, s_0\\right) & :=\\lim _{t_{\\max } \\rightarrow \\infty} \\frac{1}{t_{\\max }} \\mathbb{E}_{S_t, A_t}\\left[\\sum_{t=0}^{t_{\\max }-1} r\\left(S_t, A_t\\right) \\mid S_0=s_0, \\pi\\right] \\\\\n",
    "& =\\sum_{s \\in \\mathcal{S}}\\left\\{\\lim _{t_{\\max } \\rightarrow \\infty} \\frac{1}{t_{\\max }} \\sum_{t=0}^{t_{\\max }-1} p_\\pi^t\\left(s \\mid s_0\\right)\\right\\} r_\\pi(s)=\\sum_{s \\in \\mathcal{S}} p_\\pi^{\\star}\\left(s \\mid s_0\\right) r_\\pi(s) \\\\\n",
    "& =\\lim _{t_{\\max } \\rightarrow \\infty} \\mathbb{E}_{S_{t_{\\max }}, A_{t_{\\max }}}\\left[r\\left(S_{t_{\\max }}, A_{t_{\\max }}\\right) \\mid S_0=s_0, \\pi\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "ここで，\n",
    "* $r_\\pi(s)=\\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) r(s, a)$\n",
    "* 最初のlimitは方策が定常かつMDPが有限であれば存在します（TODO: 証明）\n",
    "* ２つ目のLimitは簡単です\n",
    "* ３つ目は方策が定常かつMDPがaperiodic，もしくは報酬に何らかの構造が入っている場合に存在します（TODO: 証明）\n",
    "\n",
    "行列形式で書けば，\n",
    "\n",
    "$$\n",
    "\\boldsymbol{v}_g(\\pi)=\\lim _{t_{\\max } \\rightarrow \\infty} \\frac{1}{t_{\\max }} \\boldsymbol{v}_{t_{\\max }}(\\pi)=\\lim _{t_{\\max } \\rightarrow \\infty} \\frac{1}{t_{\\max }} \\sum_{t=0}^{t_{\\max }-1} \\boldsymbol{P}_\\pi^t \\boldsymbol{r}_\\pi=\\boldsymbol{P}_\\pi^{\\star} \\boldsymbol{r}_\\pi\n",
    "$$\n",
    "\n",
    "です．ここで，$\\boldsymbol{r}_\\pi \\in \\mathbb{R}^{|\\mathcal{S}|}$ は $r_\\pi(s)$をstackしたものです．\n",
    "\n",
    "定常分布は$s_0$に依存しないので，$\\boldsymbol{P}_\\pi^{\\star}$の行は全て同じであることに気をつけましょう．つまり，\n",
    "\n",
    "* $v_g(\\pi)=\\boldsymbol{p}_\\pi^{\\star} \\cdot \\boldsymbol{r}_\\pi=v_g\\left(\\pi, s_0\\right), \\forall s_0 \\in \\mathcal{S}$\n",
    "* $\\boldsymbol{v}_g(\\pi)=v_g(\\pi) \\cdot \\mathbf{1}$\n",
    "\n",
    "です．\n",
    "\n",
    "方策$\\pi^* \\in \\Pi_{\\mathrm{S}}$は次を満たす時ゲイン最適であると呼びます：\n",
    "\n",
    "$$\n",
    "v_g\\left(\\pi^*, s_0\\right) \\geq v_g(\\pi, s_0), \\quad \\forall \\pi \\in \\Pi_{\\mathrm{S}}, \\forall s_0 \\in \\mathcal{S}, \\quad \\text { hence } \\pi^* \\in \\underset{\\pi \\in \\Pi_{\\mathrm{S}}}{\\operatorname{argmax}}\\; v_g(\\pi)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ベルマン方程式\n",
    "\n",
    "平均報酬についても以下のベルマン方程式が成立します：\n",
    "\n",
    "$$\n",
    "v_b\\left(\\pi^*, s\\right)+v_g\\left(\\pi^*\\right)=\\max _{a \\in \\mathcal{A}}\\left\\{r(s, a)+\\sum_{c^{\\prime} \\in \\mathcal{S}} p\\left(s^{\\prime} \\mid s, a\\right) v_b\\left(\\pi^*, s^{\\prime}\\right)\\right\\}, \\quad \\forall s \\in \\mathcal{S},\n",
    "$$\n",
    "\n",
    "こおｋで，$v_b$はバイアスと呼ばれ，次で定義されます：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_b\\left(\\pi, s_0\\right) & :=\\lim _{t_{\\max } \\rightarrow \\infty} \\mathbb{E}_{S_t, A_t}\\left[\\sum_{t=0}^{t_{\\max }-1}\\left(r\\left(S_t, A_t\\right)-v_g(\\pi)\\right) \\mid S_0=s_0, \\pi\\right] \\\\\n",
    "& =\\lim _{t_{\\max } \\rightarrow \\infty} \\sum_{t=0}^{t_{\\max }-1} \\sum_{s \\in \\mathcal{S}}\\left(p_\\pi^t\\left(s \\mid s_0\\right)-p_\\pi^{\\star}(s)\\right) r_\\pi(s) \\\\\n",
    "& =\\underbrace{\\sum_{t=0}^{\\tau-1} \\sum_{s \\in \\mathcal{S}} p_\\pi^t\\left(s \\mid s_0\\right) r_\\pi(s)}_{\\text {the expected total reward } v_\\tau}-\\tau v_g(\\pi)+\\underbrace{\\lim _{t_{\\max } \\rightarrow \\infty} \\sum_{t=\\tau}^{t_{\\max }-1} \\sum_{s \\in \\mathcal{S}}\\left(p_\\pi^t\\left(s \\mid s_0\\right)-p_\\pi^{\\star}(s)\\right) r_\\pi(s)}_{\\text {approaches } 0 \\text { as } \\tau \\rightarrow \\infty} \\\\\n",
    "& =\\sum_{s \\in \\mathcal{S}}\\left\\{\\lim _{t_{\\max } \\rightarrow \\infty} \\sum_{t=0}^{t_{\\max }-1}\\left(p_\\pi^t\\left(s \\mid s_0\\right)-p_\\pi^{\\star}(s)\\right)\\right\\} r_\\pi(s)=\\sum_{s \\in \\mathcal{S}} d_\\pi\\left(s \\mid s_0\\right) r_\\pi(s),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "ここで，$d_\\pi(s|s_0)$は行列$\\boldsymbol{D}_\\pi:=\\left(\\boldsymbol{I}-\\boldsymbol{P}_\\pi+\\boldsymbol{P}_\\pi^{\\star}\\right)^{-1}\\left(\\boldsymbol{I}-\\boldsymbol{P}_\\pi^{\\star}\\right)$と同値です（limitと期待値を入れ替えて良い場合）．\n",
    "\n",
    "バイアスは以下の解釈ができます：\n",
    "\n",
    "1. （１行目）$s_0$から始まって$\\pi$に従うときの，即時報酬$r(s_t, a_t)$と定常報酬$v_g(\\pi)$の差\n",
    "2. （２行目）$s_0$から始まった過程と，$p_\\pi^\\star$からサンプルされた初期状態から始まった過程の，報酬の差．よって，$\\pi$によって得られる総報酬と，ずっと$v_g(\\pi)$の報酬の場合の総報酬の差．\n",
    "3. （３行目）$v_\\tau\\left(\\pi, s_0\\right) \\approx v_g(\\pi) \\tau+v_b\\left(\\pi, s_0\\right)$とみなせるので，期待総報酬$v_\\tau$がどれくらい振動するかを表している．\n",
    "4. （４行目）$\\left(p_\\pi^t\\left(s \\mid s_0\\right)-p_\\pi^{\\star}(s)\\right)$は定常状態になるまで非ゼロなので，バイアスは一時的な性能とみなせる．\n",
    "\n",
    "何らかの参照状態$s_{\\mathrm{ref}} \\in \\mathcal{S}$について，相対価値$v_{\\text {brel }}$が定義されます：\n",
    "\n",
    "$$\n",
    "v_{b r e l}(\\pi, s):=v_b(\\pi, s)-v_b\\left(\\pi, s_{\\text {ref }}\\right)=\\lim _{t_{\\max } \\rightarrow \\infty}\\left\\{v_{t_{\\max }}(\\pi, s)-v_{t_{\\max }}\\left(\\pi, s_{\\text {ref }}\\right)\\right\\}, \\quad \\forall \\pi \\in \\Pi_{\\mathrm{S}}, \\forall s \\in \\mathcal{S} .\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 価値反復法\n",
    "\n",
    "ベルマン方程式より，$\\boldsymbol{v}_b^* \\in \\mathbb{R}^{\\mid \\mathcal{S}\\mid}$がわかれば最適方策も求まります（このとき，最適ゲイン$v_g^*$も必要になります）．\n",
    "そのため，目標は$\\boldsymbol{v}_b^* \\in \\mathbb{R}^{\\mid \\mathcal{S}\\mid}$を求めればよいです．\n",
    "通常の動的計画法と同様にして，$\\boldsymbol{v}_b^*$の代わりに$\\boldsymbol{q}_b^*\\in \\mathbb{R}^{|\\mathcal{S}||\\mathcal{A}|}$を求めます．\n",
    "\n",
    "---\n",
    "\n",
    "**状態価値関数によるアプローチ**\n",
    "\n",
    "1. $\\hat{v}_b^{k=0}(s) \\leftarrow 0$として初期化します\n",
    "2. 適当な固定された参照状態$s_{\\text{ref }}$を使って，次のベルマン作用素で更新します：$\\hat{v}_b^{k+1}(s)=\\mathbb{B}^*\\left[\\hat{v}_b^k(s)\\right]-\\hat{v}_b^k\\left(s_{\\text {ref }}\\right), \\quad \\forall s \\in \\mathcal{S}$\n",
    "$$\n",
    "\\mathbb{B}^*\\left[\\hat{v}_b^k(s)\\right]:=\\max _{a \\in \\mathcal{A}}\\left[r(s, a)+\\sum_{s^{\\prime} \\in \\mathcal{S}} p\\left(s^{\\prime} \\mid s, a\\right) \\hat{v}_b^k\\left(s^{\\prime}\\right)\\right], \\quad \\forall s \\in \\mathcal{S} .\n",
    "$$\n",
    "3. スパン関数を$s p(\\boldsymbol{v}):=\\max _{s \\in \\mathcal{S}} v(s)-\\min _{s^{\\prime} \\in \\mathcal{S}} v\\left(s^{\\prime}\\right)$とします．$s p\\left(\\hat{\\boldsymbol{v}}_b^{k+1}-\\hat{\\boldsymbol{v}}_b^k\\right)\\leq\\varepsilon$ であればアルゴリズムを停止します．\n",
    "\n",
    "このアルゴリズムは$k\\to \\infty$で$\\hat{v}_b^k\\left(s_{\\text {ref }}\\right)$が$v_g^*$に収束することが知られています．\n",
    "\n",
    "---\n",
    "\n",
    "**行動価値関数によるアプローチ**\n",
    "\n",
    "行動価値関数を\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_b(\\pi, s, a) & :=\\lim _{t_{\\max } \\rightarrow \\infty} \\mathbb{E}_{S_t, A_t}\\left[\\sum_{t=0}^{t_{\\max }-1}\\left(r\\left(S_t, A_t\\right)-v_g^\\pi\\right) \\mid S_0=s, A_0=a, \\pi\\right] \\\\\n",
    "& =r(s, a)-v_g^\\pi+\\mathbb{E}\\left[v_b^\\pi\\left(S_{t+1}\\right)\\right], \\quad \\forall(s, a) \\in \\mathcal{S} \\times \\mathcal{A},\n",
    "\\end{aligned}\n",
    "$$\n",
    "で定義します．このとき，次のベルマン方程式が成立します．\n",
    "\n",
    "$$\n",
    "q_b^*(s, a)+v_g^*=r(s, a)+\\sum_{s^{\\prime} \\in \\mathcal{S}} p\\left(s^{\\prime} \\mid s, a\\right) \\underbrace{\\max _{a^{\\prime} \\in \\mathcal{A}} q_b^*\\left(s^{\\prime}, a^{\\prime}\\right)}_{v_b^*\\left(s^{\\prime}\\right)}, \\quad \\forall(s, a) \\in \\mathcal{S} \\times \\mathcal{A},\n",
    "$$\n",
    "\n",
    "これを使って，次によって$q$を更新することを考えます．\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{q}_b^{k+1}(s, a)&=\\mathbb{B}^*\\left[\\hat{q}_b^k(s, a)\\right]-v_b^k\\left(s_{\\text {ref }}\\right)\\\\\n",
    "&=\\underbrace{r(s, a)+\\sum_{s^{\\prime} \\in \\mathcal{S}} p\\left(s^{\\prime} \\mid s, a\\right) \\max _{a^{\\prime} \\in \\mathcal{A}} \\hat{q}_b^k\\left(s^{\\prime}, a^{\\prime}\\right)}_{\\mathbb{B}^*\\left[\\hat{q}_b^k(s, a)\\right]}-\\underbrace{\\max _{a^{\\prime \\prime} \\in \\mathcal{A}} \\hat{q}_b^k\\left(s_{\\text {ref }}, a^{\\prime \\prime}\\right)}_{\\text {can be interpreted as } \\hat{v}_g^*},\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "この反復は$\\hat{v}_b^k(s)=\\max _{a \\in \\mathcal{A}} \\hat{q}_b^k(s, a)$が$v_b^*(s)$に収束することが知られてます．\n",
    "\n",
    "---\n",
    "\n",
    "**方策評価**\n",
    "\n",
    "次の方程式を解くことで方策評価ができます：（TODO: どうやって解くの？）\n",
    "\n",
    "$$\n",
    "v_b^k(s)+v_g^k=r(s, a)+\\sum_{s^{\\prime} \\in \\mathcal{S}} p\\left(s^{\\prime} \\mid s, a\\right) v_b^k\\left(s^{\\prime}\\right), \\quad \\forall s \\in \\mathcal{S}, \\text { with } a=\\hat{\\pi}^k(s)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状態数： 10\n",
      "行動数： 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax.random import PRNGKey\n",
    "import jax\n",
    "from typing import NamedTuple, Optional\n",
    "\n",
    "key = PRNGKey(0)\n",
    "\n",
    "S = 10  # 状態集合のサイズ\n",
    "A = 3  # 行動集合のサイズ\n",
    "S_set = jnp.arange(S)  # 状態集合\n",
    "A_set = jnp.arange(A)  # 行動集合\n",
    "gamma = 0.8  # 割引率\n",
    "\n",
    "\n",
    "# 報酬行列を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "rew = jax.random.uniform(key=key, shape=(S, A))\n",
    "assert rew.shape == (S, A)\n",
    "\n",
    "\n",
    "# 遷移確率行列を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "P = jax.random.uniform(key=key, shape=(S*A, S))\n",
    "P = P / jnp.sum(P, axis=-1, keepdims=True)  # 正規化して確率にします\n",
    "P = P.reshape(S, A, S)\n",
    "np.testing.assert_allclose(P.sum(axis=-1), 1, atol=1e-6)  # ちゃんと確率行列になっているか確認します\n",
    "\n",
    "\n",
    "# 状態集合, 行動集合, 割引率, 報酬行列, 遷移確率行列が準備できたのでMDPのクラスを作ります\n",
    "\n",
    "class MDP(NamedTuple):\n",
    "    S_set: jnp.array  # 状態集合\n",
    "    A_set: jnp.array  # 行動集合\n",
    "    rew: jnp.array  # 報酬行列\n",
    "    P: jnp.array  # 遷移確率行列\n",
    "\n",
    "    @property\n",
    "    def S(self) -> int:  # 状態空間のサイズ\n",
    "        return len(self.S_set)\n",
    "\n",
    "    @property\n",
    "    def A(self) -> int:  # 行動空間のサイズ\n",
    "        return len(self.A_set)\n",
    "\n",
    "\n",
    "mdp = MDP(S_set, A_set, rew, P)\n",
    "\n",
    "print(\"状態数：\", mdp.S)\n",
    "print(\"行動数：\", mdp.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.58395743\n",
      "0.12138826\n",
      "0.026543736\n",
      "0.007195711\n",
      "0.0011954308\n",
      "0.0003478527\n",
      "5.9604645e-05\n",
      "1.2993813e-05\n",
      "2.861023e-06\n",
      "3.5762787e-07\n",
      "2.3841858e-07\n",
      "1.1920929e-07\n",
      "1.1920929e-07\n",
      "2.3841858e-07\n",
      "2.3841858e-07\n",
      "2.3841858e-07\n",
      "2.3841858e-07\n",
      "2.3841858e-07\n",
      "2.3841858e-07\n",
      "2.3841858e-07\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ref_state = 0\n",
    "Q = jnp.zeros((S, A))\n",
    "\n",
    "for i in range(20):\n",
    "    next_v = mdp.P @ Q.max(axis=1)\n",
    "    nQ = mdp.rew + next_v - Q[ref_state].max()\n",
    "    span = (nQ.max(axis=1) - Q.max(axis=1)).max() - (nQ.max(axis=1) - Q.max(axis=1)).min()\n",
    "    Q = nQ\n",
    "    print(span)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
