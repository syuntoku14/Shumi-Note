{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 平均報酬強化学習\n",
    "\n",
    "参考：\n",
    "* [Average-reward model-free reinforcement learning: a systematic review and literature mapping](https://arxiv.org/abs/2010.08920)\n",
    "\n",
    "今回は平均報酬強化学習について学びます．大体の証明はPuterman 1994にあるっぽい？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定常分布について\n",
    "\n",
    "次を定義します：\n",
    "* 方策から誘導されるマルコフ連鎖：$\\boldsymbol{P}_\\pi \\in[0,1]^{|\\mathcal{S}| \\times|\\mathcal{S}|}$\n",
    "    * その$t$回繰り返したもの：$\\boldsymbol{P}_\\pi^t\\left[s_0, s\\right]=\\operatorname{Pr}\\left\\{S_t=s \\mid s_0, \\pi\\right\\}=: p_\\pi^t\\left(s \\mid s_0\\right)$\n",
    "\n",
    "このとき，このマルコフ連鎖の収束先は\n",
    "$$p_\\pi^{\\star}\\left(s \\mid s_0\\right)=\\lim _{t_{\\max } \\rightarrow \\infty} \\frac{1}{t_{\\max }} \\sum_{t=0}^{t_{\\max }-1} p_\\pi^t\\left(s \\mid s_0\\right)=\\lim _{t_{\\max } \\rightarrow \\infty} p_\\pi^{t_{\\max }}\\left(s \\mid s_0\\right), \\quad \\forall s \\in \\mathcal{S}$$\n",
    "\n",
    "を満たします．この最初のlimitは有限なMDPならば必ず存在し，２つ目のLimitはMDPがaperiodicであれば存在します（TODO: 証明）\n",
    "\n",
    "また，$p_\\pi^\\star(s|s_0)$をベクトル化したものを$\\boldsymbol{p}_\\pi^{\\star} \\in[0,1]^{|\\mathcal{S}|}$とします．\n",
    "これは\n",
    "$$\\left(\\boldsymbol{p}_\\pi^{\\star}\\right)^{\\top} \\boldsymbol{P}_\\pi=\\left(\\boldsymbol{p}_\\pi^{\\star}\\right)^{\\top}$$\n",
    "を満たします"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ゲインについて\n",
    "\n",
    "平均報酬はゲインとも呼ばれ，次で定義され，以下の変形が成立します：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_g\\left(\\pi, s_0\\right) & :=\\lim _{t_{\\max } \\rightarrow \\infty} \\frac{1}{t_{\\max }} \\mathbb{E}_{S_t, A_t}\\left[\\sum_{t=0}^{t_{\\max }-1} r\\left(S_t, A_t\\right) \\mid S_0=s_0, \\pi\\right] \\\\\n",
    "& =\\sum_{s \\in \\mathcal{S}}\\left\\{\\lim _{t_{\\max } \\rightarrow \\infty} \\frac{1}{t_{\\max }} \\sum_{t=0}^{t_{\\max }-1} p_\\pi^t\\left(s \\mid s_0\\right)\\right\\} r_\\pi(s)=\\sum_{s \\in \\mathcal{S}} p_\\pi^{\\star}\\left(s \\mid s_0\\right) r_\\pi(s) \\\\\n",
    "& =\\lim _{t_{\\max } \\rightarrow \\infty} \\mathbb{E}_{S_{t_{\\max }}, A_{t_{\\max }}}\\left[r\\left(S_{t_{\\max }}, A_{t_{\\max }}\\right) \\mid S_0=s_0, \\pi\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "ここで，\n",
    "* $r_\\pi(s)=\\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) r(s, a)$\n",
    "* 最初のlimitは方策が定常かつMDPが有限であれば存在します（TODO: 証明）\n",
    "* ２つ目のLimitは簡単です\n",
    "* ３つ目は方策が定常かつMDPがaperiodic，もしくは報酬に何らかの構造が入っている場合に存在します（TODO: 証明）\n",
    "\n",
    "行列形式で書けば，\n",
    "\n",
    "$$\n",
    "\\boldsymbol{v}_g(\\pi)=\\lim _{t_{\\max } \\rightarrow \\infty} \\frac{1}{t_{\\max }} \\boldsymbol{v}_{t_{\\max }}(\\pi)=\\lim _{t_{\\max } \\rightarrow \\infty} \\frac{1}{t_{\\max }} \\sum_{t=0}^{t_{\\max }-1} \\boldsymbol{P}_\\pi^t \\boldsymbol{r}_\\pi=\\boldsymbol{P}_\\pi^{\\star} \\boldsymbol{r}_\\pi\n",
    "$$\n",
    "\n",
    "です．ここで，$\\boldsymbol{r}_\\pi \\in \\mathbb{R}^{|\\mathcal{S}|}$ は $r_\\pi(s)$をstackしたものです．\n",
    "\n",
    "定常分布は$s_0$に依存しないので，$\\boldsymbol{P}_\\pi^{\\star}$の行は全て同じであることに気をつけましょう．つまり，\n",
    "\n",
    "* $v_g(\\pi)=\\boldsymbol{p}_\\pi^{\\star} \\cdot \\boldsymbol{r}_\\pi=v_g\\left(\\pi, s_0\\right), \\forall s_0 \\in \\mathcal{S}$\n",
    "* $\\boldsymbol{v}_g(\\pi)=v_g(\\pi) \\cdot \\mathbf{1}$\n",
    "\n",
    "です．\n",
    "\n",
    "方策$\\pi^* \\in \\Pi_{\\mathrm{S}}$は次を満たす時ゲイン最適であると呼びます：\n",
    "\n",
    "$$\n",
    "v_g\\left(\\pi^*, s_0\\right) \\geq v_g(\\pi, s_0), \\quad \\forall \\pi \\in \\Pi_{\\mathrm{S}}, \\forall s_0 \\in \\mathcal{S}, \\quad \\text { hence } \\pi^* \\in \\underset{\\pi \\in \\Pi_{\\mathrm{S}}}{\\operatorname{argmax}}\\; v_g(\\pi)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ベルマン方程式\n",
    "\n",
    "平均報酬についても以下のベルマン方程式が成立します：\n",
    "\n",
    "$$\n",
    "v_b\\left(\\pi^*, s\\right)+v_g\\left(\\pi^*\\right)=\\max _{a \\in \\mathcal{A}}\\left\\{r(s, a)+\\sum_{c^{\\prime} \\in \\mathcal{S}} p\\left(s^{\\prime} \\mid s, a\\right) v_b\\left(\\pi^*, s^{\\prime}\\right)\\right\\}, \\quad \\forall s \\in \\mathcal{S},\n",
    "$$\n",
    "\n",
    "こおｋで，$v_b$はバイアスと呼ばれ，次で定義されます：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_b\\left(\\pi, s_0\\right) & :=\\lim _{t_{\\max } \\rightarrow \\infty} \\mathbb{E}_{S_t, A_t}\\left[\\sum_{t=0}^{t_{\\max }-1}\\left(r\\left(S_t, A_t\\right)-v_g(\\pi)\\right) \\mid S_0=s_0, \\pi\\right] \\\\\n",
    "& =\\lim _{t_{\\max } \\rightarrow \\infty} \\sum_{t=0}^{t_{\\max }-1} \\sum_{s \\in \\mathcal{S}}\\left(p_\\pi^t\\left(s \\mid s_0\\right)-p_\\pi^{\\star}(s)\\right) r_\\pi(s) \\\\\n",
    "& =\\underbrace{\\sum_{t=0}^{\\tau-1} \\sum_{s \\in \\mathcal{S}} p_\\pi^t\\left(s \\mid s_0\\right) r_\\pi(s)}_{\\text {the expected total reward } v_\\tau}-\\tau v_g(\\pi)+\\underbrace{\\lim _{t_{\\max } \\rightarrow \\infty} \\sum_{t=\\tau}^{t_{\\max }-1} \\sum_{s \\in \\mathcal{S}}\\left(p_\\pi^t\\left(s \\mid s_0\\right)-p_\\pi^{\\star}(s)\\right) r_\\pi(s)}_{\\text {approaches } 0 \\text { as } \\tau \\rightarrow \\infty} \\\\\n",
    "& =\\sum_{s \\in \\mathcal{S}}\\left\\{\\lim _{t_{\\max } \\rightarrow \\infty} \\sum_{t=0}^{t_{\\max }-1}\\left(p_\\pi^t\\left(s \\mid s_0\\right)-p_\\pi^{\\star}(s)\\right)\\right\\} r_\\pi(s)=\\sum_{s \\in \\mathcal{S}} d_\\pi\\left(s \\mid s_0\\right) r_\\pi(s),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "ここで，$d_\\pi(s|s_0)$は行列$\\boldsymbol{D}_\\pi:=\\left(\\boldsymbol{I}-\\boldsymbol{P}_\\pi+\\boldsymbol{P}_\\pi^{\\star}\\right)^{-1}\\left(\\boldsymbol{I}-\\boldsymbol{P}_\\pi^{\\star}\\right)$と同値です（limitと期待値を入れ替えて良い場合）．\n",
    "\n",
    "バイアスは以下の解釈ができます：\n",
    "\n",
    "1. （１行目）$s_0$から始まって$\\pi$に従うときの，即時報酬$r(s_t, a_t)$と定常報酬$v_g(\\pi)$の差\n",
    "2. （２行目）$s_0$から始まった過程と，$p_\\pi^\\star$からサンプルされた初期状態から始まった過程の，報酬の差．よって，$\\pi$によって得られる総報酬と，ずっと$v_g(\\pi)$の報酬の場合の総報酬の差．\n",
    "3. （３行目）$v_\\tau\\left(\\pi, s_0\\right) \\approx v_g(\\pi) \\tau+v_b\\left(\\pi, s_0\\right)$とみなせるので，期待総報酬$v_\\tau$がどれくらい振動するかを表している．\n",
    "4. （４行目）$\\left(p_\\pi^t\\left(s \\mid s_0\\right)-p_\\pi^{\\star}(s)\\right)$は定常状態になるまで非ゼロなので，バイアスは一時的な性能とみなせる．\n",
    "\n",
    "何らかの参照状態$s_{\\mathrm{ref}} \\in \\mathcal{S}$について，相対価値$v_{\\text {brel }}$が定義されます：\n",
    "\n",
    "$$\n",
    "v_{b r e l}(\\pi, s):=v_b(\\pi, s)-v_b\\left(\\pi, s_{\\text {ref }}\\right)=\\lim _{t_{\\max } \\rightarrow \\infty}\\left\\{v_{t_{\\max }}(\\pi, s)-v_{t_{\\max }}\\left(\\pi, s_{\\text {ref }}\\right)\\right\\}, \\quad \\forall \\pi \\in \\Pi_{\\mathrm{S}}, \\forall s \\in \\mathcal{S} .\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 価値反復法\n",
    "\n",
    "ベルマン方程式より，$\\boldsymbol{v}_b^* \\in \\mathbb{R}^{\\mid \\mathcal{S}\\mid}$がわかれば最適方策も求まります（このとき，最適ゲイン$v_g^*$も必要になります）．\n",
    "そのため，目標は$\\boldsymbol{v}_b^* \\in \\mathbb{R}^{\\mid \\mathcal{S}\\mid}$を求めればよいです．\n",
    "通常の動的計画法と同様にして，$\\boldsymbol{v}_b^*$の代わりに$\\boldsymbol{q}_b^*\\in \\mathbb{R}^{|\\mathcal{S}||\\mathcal{A}|}$を求めます．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
