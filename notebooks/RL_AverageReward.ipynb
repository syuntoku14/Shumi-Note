{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 平均報酬強化学習\n",
    "\n",
    "参考：\n",
    "* [Average-reward model-free reinforcement learning: a systematic review and literature mapping](https://arxiv.org/abs/2010.08920)\n",
    "\n",
    "今回は平均報酬強化学習について学びます．大体の証明はPuterman 1994にあるっぽい？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定常分布について\n",
    "\n",
    "次を定義します：\n",
    "* 方策から誘導されるマルコフ連鎖：$\\boldsymbol{P}_\\pi \\in[0,1]^{|\\mathcal{S}| \\times|\\mathcal{S}|}$\n",
    "    * その$t$回繰り返したもの：$\\boldsymbol{P}_\\pi^t\\left[s_0, s\\right]=\\operatorname{Pr}\\left\\{S_t=s \\mid s_0, \\pi\\right\\}=: p_\\pi^t\\left(s \\mid s_0\\right)$\n",
    "\n",
    "このとき，このマルコフ連鎖の収束先は\n",
    "$$p_\\pi^{\\star}\\left(s \\mid s_0\\right)=\\lim _{t_{\\max } \\rightarrow \\infty} \\frac{1}{t_{\\max }} \\sum_{t=0}^{t_{\\max }-1} p_\\pi^t\\left(s \\mid s_0\\right)=\\lim _{t_{\\max } \\rightarrow \\infty} p_\\pi^{t_{\\max }}\\left(s \\mid s_0\\right), \\quad \\forall s \\in \\mathcal{S}$$\n",
    "\n",
    "を満たします．この最初のlimitは有限なMDPならば必ず存在し，２つ目のLimitはMDPがaperiodicであれば存在します（TODO: 証明）\n",
    "\n",
    "また，$p_\\pi^\\star(s|s_0)$をベクトル化したものを$\\boldsymbol{p}_\\pi^{\\star} \\in[0,1]^{|\\mathcal{S}|}$とします．\n",
    "これは\n",
    "$$\\left(\\boldsymbol{p}_\\pi^{\\star}\\right)^{\\top} \\boldsymbol{P}_\\pi=\\left(\\boldsymbol{p}_\\pi^{\\star}\\right)^{\\top}$$\n",
    "を満たします"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ゲインについて\n",
    "\n",
    "平均報酬はゲインとも呼ばれ，次で定義され，以下の変形が成立します：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_g\\left(\\pi, s_0\\right) & :=\\lim _{t_{\\max } \\rightarrow \\infty} \\frac{1}{t_{\\max }} \\mathbb{E}_{S_t, A_t}\\left[\\sum_{t=0}^{t_{\\max }-1} r\\left(S_t, A_t\\right) \\mid S_0=s_0, \\pi\\right] \\\\\n",
    "& =\\sum_{s \\in \\mathcal{S}}\\left\\{\\lim _{t_{\\max } \\rightarrow \\infty} \\frac{1}{t_{\\max }} \\sum_{t=0}^{t_{\\max }-1} p_\\pi^t\\left(s \\mid s_0\\right)\\right\\} r_\\pi(s)=\\sum_{s \\in \\mathcal{S}} p_\\pi^{\\star}\\left(s \\mid s_0\\right) r_\\pi(s) \\\\\n",
    "& =\\lim _{t_{\\max } \\rightarrow \\infty} \\mathbb{E}_{S_{t_{\\max }}, A_{t_{\\max }}}\\left[r\\left(S_{t_{\\max }}, A_{t_{\\max }}\\right) \\mid S_0=s_0, \\pi\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "ここで，\n",
    "* $r_\\pi(s)=\\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) r(s, a)$\n",
    "* 最初のlimitは方策が定常かつMDPが有限であれば存在します（TODO: 証明）\n",
    "* ２つ目のLimitは簡単です\n",
    "* ３つ目は方策が定常かつMDPがaperiodic，もしくは報酬に何らかの構造が入っている場合に存在します（TODO: 証明）\n",
    "\n",
    "行列形式で書けば，\n",
    "\n",
    "$$\n",
    "\\boldsymbol{v}_g(\\pi)=\\lim _{t_{\\max } \\rightarrow \\infty} \\frac{1}{t_{\\max }} \\boldsymbol{v}_{t_{\\max }}(\\pi)=\\lim _{t_{\\max } \\rightarrow \\infty} \\frac{1}{t_{\\max }} \\sum_{t=0}^{t_{\\max }-1} \\boldsymbol{P}_\\pi^t \\boldsymbol{r}_\\pi=\\boldsymbol{P}_\\pi^{\\star} \\boldsymbol{r}_\\pi\n",
    "$$\n",
    "\n",
    "です．ここで，$\\boldsymbol{r}_\\pi \\in \\mathbb{R}^{|\\mathcal{S}|}$ は $r_\\pi(s)$をstackしたものです．\n",
    "\n",
    "定常分布は$s_0$に依存しないので，$\\boldsymbol{P}_\\pi^{\\star}$の行は全て同じであることに気をつけましょう．つまり，\n",
    "\n",
    "* $v_g(\\pi)=\\boldsymbol{p}_\\pi^{\\star} \\cdot \\boldsymbol{r}_\\pi=v_g\\left(\\pi, s_0\\right), \\forall s_0 \\in \\mathcal{S}$\n",
    "* $\\boldsymbol{v}_g(\\pi)=v_g(\\pi) \\cdot \\mathbf{1}$\n",
    "\n",
    "です．\n",
    "\n",
    "方策$\\pi^* \\in \\Pi_{\\mathrm{S}}$は次を満たす時ゲイン最適であると呼びます：\n",
    "\n",
    "$$\n",
    "v_g\\left(\\pi^*, s_0\\right) \\geq v_g(\\pi, s_0), \\quad \\forall \\pi \\in \\Pi_{\\mathrm{S}}, \\forall s_0 \\in \\mathcal{S}, \\quad \\text { hence } \\pi^* \\in \\underset{\\pi \\in \\Pi_{\\mathrm{S}}}{\\operatorname{argmax}}\\; v_g(\\pi)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ベルマン方程式\n",
    "\n",
    "平均報酬についても以下のベルマン方程式が成立します：\n",
    "\n",
    "$$\n",
    "v_b\\left(\\pi^*, s\\right)+v_g\\left(\\pi^*\\right)=\\max _{a \\in \\mathcal{A}}\\left\\{r(s, a)+\\sum_{c^{\\prime} \\in \\mathcal{S}} p\\left(s^{\\prime} \\mid s, a\\right) v_b\\left(\\pi^*, s^{\\prime}\\right)\\right\\}, \\quad \\forall s \\in \\mathcal{S},\n",
    "$$\n",
    "\n",
    "こおｋで，$v_b$はバイアスと呼ばれ，次で定義されます：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_b\\left(\\pi, s_0\\right) & :=\\lim _{t_{\\max } \\rightarrow \\infty} \\mathbb{E}_{S_t, A_t}\\left[\\sum_{t=0}^{t_{\\max }-1}\\left(r\\left(S_t, A_t\\right)-v_g(\\pi)\\right) \\mid S_0=s_0, \\pi\\right] \\\\\n",
    "& =\\lim _{t_{\\max } \\rightarrow \\infty} \\sum_{t=0}^{t_{\\max }-1} \\sum_{s \\in \\mathcal{S}}\\left(p_\\pi^t\\left(s \\mid s_0\\right)-p_\\pi^{\\star}(s)\\right) r_\\pi(s) \\\\\n",
    "& =\\underbrace{\\sum_{t=0}^{\\tau-1} \\sum_{s \\in \\mathcal{S}} p_\\pi^t\\left(s \\mid s_0\\right) r_\\pi(s)}_{\\text {the expected total reward } v_\\tau}-\\tau v_g(\\pi)+\\underbrace{\\lim _{t_{\\max } \\rightarrow \\infty} \\sum_{t=\\tau}^{t_{\\max }-1} \\sum_{s \\in \\mathcal{S}}\\left(p_\\pi^t\\left(s \\mid s_0\\right)-p_\\pi^{\\star}(s)\\right) r_\\pi(s)}_{\\text {approaches } 0 \\text { as } \\tau \\rightarrow \\infty} \\\\\n",
    "& =\\sum_{s \\in \\mathcal{S}}\\left\\{\\lim _{t_{\\max } \\rightarrow \\infty} \\sum_{t=0}^{t_{\\max }-1}\\left(p_\\pi^t\\left(s \\mid s_0\\right)-p_\\pi^{\\star}(s)\\right)\\right\\} r_\\pi(s)=\\sum_{s \\in \\mathcal{S}} d_\\pi\\left(s \\mid s_0\\right) r_\\pi(s),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "ここで，$d_\\pi(s|s_0)$は行列$\\boldsymbol{D}_\\pi:=\\left(\\boldsymbol{I}-\\boldsymbol{P}_\\pi+\\boldsymbol{P}_\\pi^{\\star}\\right)^{-1}\\left(\\boldsymbol{I}-\\boldsymbol{P}_\\pi^{\\star}\\right)$と同値です（limitと期待値を入れ替えて良い場合）．\n",
    "\n",
    "バイアスは以下の解釈ができます：\n",
    "\n",
    "1. （１行目）$s_0$から始まって$\\pi$に従うときの，即時報酬$r(s_t, a_t)$と定常報酬$v_g(\\pi)$の差\n",
    "2. （２行目）$s_0$から始まった過程と，$p_\\pi^\\star$からサンプルされた初期状態から始まった過程の，報酬の差．よって，$\\pi$によって得られる総報酬と，ずっと$v_g(\\pi)$の報酬の場合の総報酬の差．\n",
    "3. （３行目）$v_\\tau\\left(\\pi, s_0\\right) \\approx v_g(\\pi) \\tau+v_b\\left(\\pi, s_0\\right)$とみなせるので，期待総報酬$v_\\tau$がどれくらい振動するかを表している．\n",
    "4. （４行目）$\\left(p_\\pi^t\\left(s \\mid s_0\\right)-p_\\pi^{\\star}(s)\\right)$は定常状態になるまで非ゼロなので，バイアスは一時的な性能とみなせる．\n",
    "\n",
    "何らかの参照状態$s_{\\mathrm{ref}} \\in \\mathcal{S}$について，相対価値$v_{\\text {brel }}$が定義されます：\n",
    "\n",
    "$$\n",
    "v_{b r e l}(\\pi, s):=v_b(\\pi, s)-v_b\\left(\\pi, s_{\\text {ref }}\\right)=\\lim _{t_{\\max } \\rightarrow \\infty}\\left\\{v_{t_{\\max }}(\\pi, s)-v_{t_{\\max }}\\left(\\pi, s_{\\text {ref }}\\right)\\right\\}, \\quad \\forall \\pi \\in \\Pi_{\\mathrm{S}}, \\forall s \\in \\mathcal{S} .\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 価値反復法\n",
    "\n",
    "ベルマン方程式より，$\\boldsymbol{v}_b^* \\in \\mathbb{R}^{\\mid \\mathcal{S}\\mid}$がわかれば最適方策も求まります（このとき，最適ゲイン$v_g^*$も必要になります）．\n",
    "そのため，目標は$\\boldsymbol{v}_b^* \\in \\mathbb{R}^{\\mid \\mathcal{S}\\mid}$を求めればよいです．\n",
    "通常の動的計画法と同様にして，$\\boldsymbol{v}_b^*$の代わりに$\\boldsymbol{q}_b^*\\in \\mathbb{R}^{|\\mathcal{S}||\\mathcal{A}|}$を求めます．\n",
    "\n",
    "---\n",
    "\n",
    "**状態価値関数によるアプローチ**\n",
    "\n",
    "1. $\\hat{v}_b^{k=0}(s) \\leftarrow 0$として初期化します\n",
    "2. 適当な固定された参照状態$s_{\\text{ref }}$を使って，次のベルマン作用素で更新します：$\\hat{v}_b^{k+1}(s)=\\mathbb{B}^*\\left[\\hat{v}_b^k(s)\\right]-\\hat{v}_b^k\\left(s_{\\text {ref }}\\right), \\quad \\forall s \\in \\mathcal{S}$\n",
    "$$\n",
    "\\mathbb{B}^*\\left[\\hat{v}_b^k(s)\\right]:=\\max _{a \\in \\mathcal{A}}\\left[r(s, a)+\\sum_{s^{\\prime} \\in \\mathcal{S}} p\\left(s^{\\prime} \\mid s, a\\right) \\hat{v}_b^k\\left(s^{\\prime}\\right)\\right], \\quad \\forall s \\in \\mathcal{S} .\n",
    "$$\n",
    "3. スパン関数を$s p(\\boldsymbol{v}):=\\max _{s \\in \\mathcal{S}} v(s)-\\min _{s^{\\prime} \\in \\mathcal{S}} v\\left(s^{\\prime}\\right)$とします．$s p\\left(\\hat{\\boldsymbol{v}}_b^{k+1}-\\hat{\\boldsymbol{v}}_b^k\\right)\\leq\\varepsilon$ であればアルゴリズムを停止します．\n",
    "\n",
    "このアルゴリズムは$k\\to \\infty$で$\\hat{v}_b^k\\left(s_{\\text {ref }}\\right)$が$v_g^*$に収束することが知られています．\n",
    "\n",
    "---\n",
    "\n",
    "**行動価値関数によるアプローチ**\n",
    "\n",
    "行動価値関数を\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_b(\\pi, s, a) & :=\\lim _{t_{\\max } \\rightarrow \\infty} \\mathbb{E}_{S_t, A_t}\\left[\\sum_{t=0}^{t_{\\max }-1}\\left(r\\left(S_t, A_t\\right)-v_g^\\pi\\right) \\mid S_0=s, A_0=a, \\pi\\right] \\\\\n",
    "& =r(s, a)-v_g^\\pi+\\mathbb{E}\\left[v_b^\\pi\\left(S_{t+1}\\right)\\right], \\quad \\forall(s, a) \\in \\mathcal{S} \\times \\mathcal{A},\n",
    "\\end{aligned}\n",
    "$$\n",
    "で定義します．このとき，次のベルマン方程式が成立します．\n",
    "\n",
    "$$\n",
    "q_b^*(s, a)+v_g^*=r(s, a)+\\sum_{s^{\\prime} \\in \\mathcal{S}} p\\left(s^{\\prime} \\mid s, a\\right) \\underbrace{\\max _{a^{\\prime} \\in \\mathcal{A}} q_b^*\\left(s^{\\prime}, a^{\\prime}\\right)}_{v_b^*\\left(s^{\\prime}\\right)}, \\quad \\forall(s, a) \\in \\mathcal{S} \\times \\mathcal{A},\n",
    "$$\n",
    "\n",
    "これを使って，次によって$q$を更新することを考えます．\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{q}_b^{k+1}(s, a)&=\\mathbb{B}^*\\left[\\hat{q}_b^k(s, a)\\right]-v_b^k\\left(s_{\\text {ref }}\\right)\\\\\n",
    "&=\\underbrace{r(s, a)+\\sum_{s^{\\prime} \\in \\mathcal{S}} p\\left(s^{\\prime} \\mid s, a\\right) \\max _{a^{\\prime} \\in \\mathcal{A}} \\hat{q}_b^k\\left(s^{\\prime}, a^{\\prime}\\right)}_{\\mathbb{B}^*\\left[\\hat{q}_b^k(s, a)\\right]}-\\underbrace{\\max _{a^{\\prime \\prime} \\in \\mathcal{A}} \\hat{q}_b^k\\left(s_{\\text {ref }}, a^{\\prime \\prime}\\right)}_{\\text {can be interpreted as } \\hat{v}_g^*},\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "この反復は$\\hat{v}_b^k(s)=\\max _{a \\in \\mathcal{A}} \\hat{q}_b^k(s, a)$が$v_b^*(s)$に収束することが知られてます．\n",
    "\n",
    "---\n",
    "\n",
    "**方策評価**\n",
    "\n",
    "次の方程式を解くことで方策評価ができます：（TODO: どうやって解くの？）\n",
    "\n",
    "$$\n",
    "v_b^k(s)+v_g^k=r(s, a)+\\sum_{s^{\\prime} \\in \\mathcal{S}} p\\left(s^{\\prime} \\mid s, a\\right) v_b^k\\left(s^{\\prime}\\right), \\quad \\forall s \\in \\mathcal{S}, \\text { with } a=\\hat{\\pi}^k(s)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状態数： 10\n",
      "行動数： 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax.random import PRNGKey\n",
    "import jax\n",
    "from typing import NamedTuple, Optional\n",
    "\n",
    "key = PRNGKey(0)\n",
    "\n",
    "S = 10  # 状態集合のサイズ\n",
    "A = 3  # 行動集合のサイズ\n",
    "S_set = jnp.arange(S)  # 状態集合\n",
    "A_set = jnp.arange(A)  # 行動集合\n",
    "gamma = 0.8  # 割引率\n",
    "\n",
    "\n",
    "# 報酬行列を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "rew = jax.random.uniform(key=key, shape=(S, A))\n",
    "assert rew.shape == (S, A)\n",
    "\n",
    "\n",
    "# 遷移確率行列を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "P = jax.random.uniform(key=key, shape=(S*A, S))\n",
    "P = P / jnp.sum(P, axis=-1, keepdims=True)  # 正規化して確率にします\n",
    "P = P.reshape(S, A, S)\n",
    "np.testing.assert_allclose(P.sum(axis=-1), 1, atol=1e-6)  # ちゃんと確率行列になっているか確認します\n",
    "\n",
    "\n",
    "# 状態集合, 行動集合, 割引率, 報酬行列, 遷移確率行列が準備できたのでMDPのクラスを作ります\n",
    "\n",
    "class MDP(NamedTuple):\n",
    "    S_set: jnp.array  # 状態集合\n",
    "    A_set: jnp.array  # 行動集合\n",
    "    rew: jnp.array  # 報酬行列\n",
    "    P: jnp.array  # 遷移確率行列\n",
    "\n",
    "    @property\n",
    "    def S(self) -> int:  # 状態空間のサイズ\n",
    "        return len(self.S_set)\n",
    "\n",
    "    @property\n",
    "    def A(self) -> int:  # 行動空間のサイズ\n",
    "        return len(self.A_set)\n",
    "\n",
    "\n",
    "mdp = MDP(S_set, A_set, rew, P)\n",
    "\n",
    "print(\"状態数：\", mdp.S)\n",
    "print(\"行動数：\", mdp.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95673645\n",
      "1.014119\n",
      "1.0162408\n",
      "1.0300635\n",
      "1.0319318\n",
      "1.0320619\n",
      "1.0322145\n",
      "1.0322174\n",
      "1.0322226\n",
      "1.0322232\n",
      "1.0322232\n",
      "1.0322232\n",
      "1.0322232\n",
      "1.0322232\n",
      "1.0322231\n",
      "1.0322233\n",
      "1.0322232\n",
      "1.0322231\n",
      "1.0322233\n",
      "1.0322232\n",
      "1.0322232\n",
      "1.0322232\n",
      "1.0322232\n",
      "1.0322232\n",
      "1.0322232\n",
      "1.0322232\n",
      "1.0322231\n",
      "1.0322233\n",
      "1.0322232\n",
      "1.0322231\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ref_state = 0\n",
    "Q = jnp.zeros((S, A))\n",
    "\n",
    "for i in range(30):\n",
    "    next_v = mdp.P @ Q.max(axis=1)\n",
    "    nQ = mdp.rew + next_v - Q[ref_state].max()\n",
    "    span = (nQ.max(axis=1) - Q.max(axis=1)).max() - (nQ.max(axis=1) - Q.max(axis=1)).min()\n",
    "    Q = nQ\n",
    "    print(Q.max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
