{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Span情報がない平均報酬強化学習\n",
    "\n",
    "参考：\n",
    "* [Achieving Tractable Minimax Optimal Regret in Average Reward MDPs](https://arxiv.org/abs/2406.01234)\n",
    "\n",
    "Average RewardでのRLは一般にSpanの情報や半径の情報が必要になります（UCRL2はいらないけど，リグレットバウンドが半径に依存する）．\n",
    "今回はSpanが不要なMDPについて学んでみましょう．\n",
    "\n",
    "表記：\n",
    "* MDP：$M \\in \\mathcal{M}$\n",
    "* ゲイン：$g^\\pi(s):=\\lim \\frac{1}{T} \\mathbf{E}_s^\\pi\\left[R_0+\\ldots+R_{T-1}\\right]$\n",
    "* バイアス：$h^\\pi:=\\lim \\sum_{t=0}^{T-1}\\left(R_t-g\\left(S_t\\right)\\right)$\n",
    "* Poisson方程式：$h^\\pi+g^\\pi=r^\\pi+P^\\pi h^\\pi$\n",
    "* ベルマン方程式：$L u(s):=\\max _{a \\in \\mathcal{A}(s)}\\{r(s, a)+p(s, a) u\\}$\n",
    "  * 今回はWeakly communicatingの設定を考える．つまり，$Lh^* - h^* \\in \\boldsymbol{R}e$を満たすような$h^*$が存在する．\n",
    "  * これは任意の方策に対して$r^\\pi+P^\\pi h^* \\leq g^*+h^*$を満たす\n",
    "* ベルマン誤差：$\\Delta^*(s, a):=h^*(s)+g^*(s)-r(s, a)-p(s, a) h^* \\geq 0$\n",
    "* 直径：$D:=\\sup _{s \\neq s^{\\prime}} \\inf _\\pi \\mathbf{E}_s^\\pi\\left[\\inf \\left\\{t \\geq 1: S_t=s^{\\prime}\\right\\}\\right]$\n",
    "* リグレット：\n",
    "    * $\\operatorname{Reg}(T):=T g^*-\\sum_{t=0}^{T-1} R_t$\n",
    "    * $\\mathbf{E}[\\operatorname{Reg}(T)]=\\mathbf{E}\\left[\\sum_{t=0}^{T-1} \\Delta^*\\left(X_t\\right)\\right]+\\mathbf{E}\\left[h^*\\left(S_0\\right)-h^*\\left(S_T\\right)\\right]$\n",
    "* SpanがバウンドされたMDPの集合：$\\mathcal{M}_c:=\\left\\{M \\in \\mathcal{M}: \\exists h^* \\in \\operatorname{Fix}(L(M)), \\operatorname{sp}\\left(h^*\\right) \\leq c\\right\\}$\n",
    "  * 下界：$\\max _{M \\in \\mathcal{M}_c} \\mathbf{E}^{M, \\mathbf{A}}[\\operatorname{Reg}(T)]=\\Omega(\\sqrt{c S A T})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PMEVI-DT アルゴリズム\n",
    "\n",
    "基本的なアイデアはOFUと同じです．OFUは次の楽観的なゲインを計算します：\n",
    "$$\n",
    "g^*\\left(\\mathcal{M}_t\\right):=\\sup \\left\\{g^\\pi\\left(\\mathcal{M}_t\\right): \\pi \\in \\Pi, \\operatorname{sp}\\left(g^\\pi\\left(\\mathcal{M}_t\\right)\\right)=0\\right\\} \\text { with } g^\\pi\\left(\\mathcal{M}_t\\right):=\\sup \\left\\{g(\\pi, \\widetilde{M}): \\widetilde{M} \\in \\mathcal{M}_t\\right\\}\n",
    "$$\n",
    "\n",
    "OFUの更新タイミングはいろいろありますが，今回はDoubling trickを使います．つまり，\n",
    "$$\n",
    "N_t\\left(S_t, \\pi_k\\left(S_t\\right)\\right) \\geq 1 \\vee 2 N_{t_k}\\left(X_t\\right)\n",
    "$$\n",
    "のタイミングで更新を行います（$X_t = (S_t, A_t)$）．\n",
    "\n",
    "### Extended Value Iteration (EVI)について\n",
    "\n",
    "UCRL2など，OFUを実現するためには基本的にEVIを使います．\n",
    "$(s, a)$-rectangularな不確実集合$\\mathcal{M}_t \\equiv \\prod_{s, a}\\left(\\mathcal{R}_t(s, a) \\times \\mathcal{P}_t(s, a)\\right)$を作り，次の楽観的な作用素でバイアス関数を更新します：\n",
    "\n",
    "$$\n",
    "v_{i+1}(s) \\equiv \\mathcal{L}_t v_i(s):=\\max _{a \\in \\mathcal{A}(s)} \\max _{\\tilde{r}(s, a) \\in \\mathcal{R}_t(s, a)} \\max _{\\tilde{p}(s, a) \\in \\mathcal{P}_t(s, a)}\\left(\\tilde{r}(s, a)+\\tilde{p}(s, a) \\cdot v_i\\right)\n",
    "$$\n",
    "\n",
    "そして，スパンが$\\operatorname{sp}\\left(v_{i+1}-v_i\\right)<\\epsilon$を満たすまで繰り返すのがEVIです．このとき，$\\mathcal{L}_t v_i$を与える方策は\n",
    "$g^\\pi\\left(\\mathcal{M}_t\\right) \\geq g^*(\\mathcal{M})-\\epsilon$を満たすことが知られています．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projected Mitigated EVI\n",
    "\n",
    "基本的に，OFUは$\\mathcal{M}_t$の良さによって実現されます．\n",
    "よって，多くの先行研究は$\\mathcal{M}_t$を改善できるように様々な工夫をこらしてました．\n",
    "\n",
    "今回の論文は，あまり$\\mathcal{M}_t$を改善することに固執していません．\n",
    "* いい感じの挙動をする信頼区間を使い，\n",
    "* バイアスの推定をして，\n",
    "* EVIを改善する\n",
    "\n",
    "ことで，Minimax最適なアルゴリズムを達成するのが今回の論文です．\n",
    "\n",
    "これを説明するために，何らかの方法で，$h^*$を推定するためのバイアスの信頼区間$\\mathcal{H}_t$が与えられているとします．$M\\in \\mathcal{M}_t$かつ$h^* \\in \\mathcal{H}_t$が満たされているならば，\n",
    "* ゲインを最大化して，かつ$h(\\pi, \\tilde{M}) \\in \\mathcal{H}_t$を満たすような方策とMDPのペア$(\\pi, \\tilde{M})$を見つければ，OFUできそうな気がします．\n",
    "\n",
    "そこで，「Projection」と「Mitigation」の２つのテクニックを使います．\n",
    "\n",
    "1. Projection: もし$h^* \\in \\mathcal{H}_t$ならば，OFUで探す最適方策はバイアスが$\\mathcal{H}_t$の中に入るものに限定して構いません．そこで，$\\Gamma_t: \\mathbf{R}^{\\mathcal{S}} \\rightarrow \\mathcal{H}_t$を使ってバイアスを射影します．\n",
    "2. Mitigation: 一旦ボーナスベースのアルゴリズムについて考えてみましょう．ボーナスベースのアルゴリズムは，\n",
    "$$\n",
    "\\tilde{p}(s, a) u_i \\leq \\hat{p}_t(s, a) u_i+\\underbrace{\\left(p(s, a)-\\hat{p}_t(s, a)\\right) u_i}_{\\leq ボーナス関数}\n",
    "$$\n",
    "によって，$推定した遷移\\cdot 価値+ボーナス$を使ってOFUを実現します．\n",
    "今回のアルゴリズムはこれを利用します．\n",
    "もし$h^* \\in \\mathcal{H}_t$ならば，$\\beta_t(s, a):=\\max _{u \\in \\mathcal{H}_t} \\beta_t(s, a, u)$とすれば，$h^*$がわからなくても，$\\left(\\hat{p}_t(s, a)-p(s, a)\\right) h^* \\leq \\beta_t(s, a)$が成立します．\n",
    "これを使って，次のEVIを後で利用します．\n",
    "$$\n",
    "\\mathcal{L}_t^\\beta u(s):=\\max _{a \\in \\mathcal{A}(s)} \\sup _{\\tilde{r}(s, a) \\in \\mathcal{R}_t(s, a)} \\sup _{\\tilde{p}(s, a) \\in \\mathcal{P}_t(s, a)}\\left\\{\\tilde{r}(s, a)+\\min \\left\\{\\tilde{p}(s, a) u_i, \\hat{p}_t(s, a) u_i+\\beta_t(s, a)\\right\\}\\right\\}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "上のProjectionとMitigationを踏まえて，今回のアルゴリズムでは次の「MitigateしてProjection」を繰り返します：\n",
    "\n",
    "$$\\mathfrak{L}_t:=\\Gamma_t \\circ \\mathcal{L}_t^\\beta$$\n",
    "\n",
    "これはCompositionなので，うまく動くかは自明ではありません．しかし，次の定理によって挙動が保証されます：\n",
    "\n",
    "---\n",
    "\n",
    "固定した$\\beta$を考えます．ここで，次を満たす$\\Gamma_t: \\boldsymbol{R}^{\\mathcal{X}} \\to \\mathcal{H}_t$を考えましょう：\n",
    "1. $u \\leq v \\Rightarrow \\Gamma u \\leq \\Gamma v ;$\n",
    "2. $\\operatorname{sp}(\\Gamma u-\\Gamma v) \\leq \\operatorname{sp}(u-v)$\n",
    "3. $\\Gamma(u+\\lambda e)=\\Gamma u+\\lambda e$\n",
    "4. $\\Gamma u \\leq u$\n",
    "\n",
    "このとき，$\\mathfrak{L}_t:=\\Gamma_t \\circ \\mathcal{L}_t^\\beta$は次を満たします：\n",
    "\n",
    "書くのがめんどいので省略．\n",
    "結局Biasのconfidence regionが正しければうまく動く．\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## バイアスの推定機\n",
    "\n",
    "今回のバイアスの推定器として，次の制約を組み合わせたものを考えます：\n",
    "$$\\forall s \\neq s^{\\prime}, \\quad \\mathfrak{h}(s)-\\mathfrak{h}\\left(s^{\\prime}\\right)-c\\left(s, s^{\\prime}\\right) \\leq d\\left(s, s^{\\prime}\\right)$$\n",
    "\n",
    "---\n",
    "\n",
    "**Bias difference estimator**\n",
    "\n",
    "**コメント：** この表記めっちゃ分かりづらい．基本的に[RL_AverageReward_estimate-bias.ipynb](RL_AverageReward_estimate-bias.ipynb)と同じなので，そっちを参照．\n",
    "\n",
    "$s \\neq s^{\\prime}$が与えられたときに，次の$\\left(\\tau_i^{s \\leftrightarrow s^{\\prime}}\\right)_{i \\geq 0}$をcommute timeの系列と呼ぶ：\n",
    "* $\\tau_{2 i}^{s \\leftrightarrow s^{\\prime}}:=\\inf \\left\\{t>\\tau_{2 i-1}^{s \\leftrightarrow s^{\\prime}}: S_t=s\\right\\}$：$s$に訪れる，$\\tau_{2 i-1}^{s \\leftrightarrow s^{\\prime}}$より後の時刻\n",
    "* $\\tau_{2 i+1}^{s \\leftrightarrow s^{\\prime}}:=\\inf \\left\\{t>\\tau_{2 i}^{s \\leftrightarrow s^{\\prime}}: S_t=s^{\\prime}\\right\\}$：$s'$に訪れる，$\\tau_{2 i}^{s \\leftrightarrow s^{\\prime}}$より後の時刻．$i$は往復回数を表してるっぽい．\n",
    "* $\\tau_{2 -1}^{s \\leftrightarrow s^{\\prime}}:=-\\infty$とする\n",
    "* $N_t\\left(s \\leftrightarrow s^{\\prime}\\right):=\\sup \\left\\{i: \\tau_i^{s \\leftrightarrow s^{\\prime}} \\leq t\\right\\}$：時刻$t$以前に起きた往復回数？（多分supだと思われる）\n",
    "* $\\hat{g}(t):=\\frac{1}{t} \\sum_{i=0}^{t-1} R_i$：ゲインの推定器\n",
    "\n",
    "これを使って，Bias difference estimatorを次を満たすような$c_T$として設計します：\n",
    "\n",
    "$$\n",
    "N_t\\left(s \\leftrightarrow s^{\\prime}\\right) c_T\\left(s, s^{\\prime}\\right)=\n",
    "\\sum_{t=0}^{N_T\\left(s \\leftrightarrow s^{\\prime}\\right)-1}(-1)^i \\sum_{t=\\tau_i^{\\tau_s} s^{s^{\\prime}}}^{\\tau_{i+s^{\\prime}}^{s \\leftrightarrow s^{\\prime}}-1}\\left(\\hat{g}(T)-R_t\\right) .\n",
    "$$\n",
    "\n",
    "**実装について:**\n",
    "次の分解を考えると実装しやすいです：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "= \n",
    "&\\left(\\sum_{i=0}^{\\text{$s\\to s'$の往復回数}}\n",
    "(-1)^i \n",
    "\\cdot [\\text{$i$の往復時間}]\n",
    "\\right)\n",
    "\\cdot \\hat{g}(T)\\\\\n",
    "&-\\left(\\sum_{i=0}^{\\text{$s\\to s'$の往復回数}}\n",
    "(-1)^i \n",
    "\\cdot [\\text{$i$中の総報酬}]\n",
    "\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "このとき，次が高確率で成立します：　\n",
    "任意の$T^{\\prime} \\leq T$ と$\\tilde{g} \\geq g^*$，そして指標$c_T\\left(s, s^{\\prime}\\right) \\in \\mathbf{R}$について，\n",
    "$$\n",
    "N_{T^{\\prime}}\\left(s \\leftrightarrow s^{\\prime}\\right)\\left|\\underbrace{h^*(s)-h^*\\left(s^{\\prime}\\right)-c_{T^{\\prime}}\\left(s, s^{\\prime}\\right)}_{\"真のバイアスの差分\"と\"事前知識\"の差}\\right| \\leq \\underbrace{3 \\operatorname{sp}\\left(h^*\\right)}_{無視できそう}+\\left(1+\\operatorname{sp}\\left(h^*\\right)\\right) \\sqrt{8 T \\log \\left(\\frac{2}{\\delta}\\right)}+\\underbrace{2 \\sum_{t=0}^{T^{\\prime}-1}\\left(\\tilde{g}-R_t\\right)}_{リグレット}\n",
    "$$\n",
    "\n",
    "$\\operatorname{sp}$は基本的に未知なので，$c_0:=T^{1 / 5}$で近似します．\n",
    "\n",
    "まとめると，次の流れでバイアスの推定をします：\n",
    "* バイアス推定器：$c_T\\left(s, s^{\\prime}\\right)=\\frac{1}{N_t\\left(s \\leftrightarrow s^{\\prime}\\right)} \\sum_{t=0}^{N_T\\left(s \\leftrightarrow s^{\\prime}\\right)-1}(-1)^i \\sum_{t=\\tau_i^{\\tau_s} s^{s^{\\prime}}}^{\\tau_{i+s^{\\prime}}^{s \\leftrightarrow s^{\\prime}}-1}\\left(\\hat{g}(T)-R_t\\right)$\n",
    "* optimisticなゲインを計算（過去のゲインから一番小さいのを取ってくる？）：$\\tilde{g} \\leftarrow \\min _{k<K(t)} \\mathfrak{g}_k$\n",
    "* リグレットの推定：$B_0 \\leftarrow t \\tilde{g}-\\sum_{i=0}^{t-1} R_i$\n",
    "* $\\ell \\leftarrow \\sqrt{8 T \\log \\left(\\frac{2}{\\delta}\\right)}, c_0 \\leftarrow T^{\\frac{1}{5}}$\n",
    "* バイアス推定器の誤差：$d_t\\left(s, s^{\\prime}\\right) \\equiv \\operatorname{error}\\left(c_t, s, s^{\\prime}\\right):=\\frac{3 c_0+\\left(1+c_0\\right)(1+\\ell)+2 B_0}{N_t\\left(s \\leftrightarrow s^{\\prime}\\right)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リグレット解析\n",
    "\n",
    "まず，OptimismとConfidence boundを使ってリグレットを変形しましょう．最初のステップはUCRL2と同じです：　\n",
    "$$\n",
    "\\operatorname{Reg}(T) \n",
    "\\underbrace{\\leq}_{\\text{optimism}} \\sum_k \\sum_{t=t_k}^{t_{k+1}-1}\\left(\\mathfrak{g}_k-R_t\\right) \n",
    "\\underbrace{\\leq}_{\\text{confidence bound}} \\sum_k \\sum_{t=t_k}^{t_{k+1}-1}\\left(\\mathfrak{g}_k-\\tilde{r}_k\\left(X_t\\right)\\right)+\\mathrm{O}\\left(\\sqrt{S A T \\log \\left(\\frac{T}{\\delta}\\right)}\\right)\n",
    "$$\n",
    "\n",
    "以降，一項目を$\\tilde{B}(T):=\\sum_\\alpha \\sum_{t=t_k}^{t_{k+1}-1}\\left(\\mathfrak{g}_k-\\tilde{r}_k\\left(X_t\\right)\\right)$と置きましょう．こいつをバウンドします．まず，Posson方程式（$\\mathfrak{h}_k+\\mathfrak{g}_k=\\tilde{r}_k+\\tilde{P}_k \\mathfrak{b}_k$）を使って，\n",
    "\n",
    "$$\n",
    "\\tilde{B}(T)=\\sum_k \\sum_{t=t_k}^{t_{k+1}-1}\\left(\\tilde{p}_k\\left(S_t\\right)-e_{S_t}\\right) \\mathrm{b}_k\n",
    "$$\n",
    "\n",
    "を得ます．こいつを[RL_AverageReward_estimate.ipynb](RL_AverageReward_estimate-bias.ipynb)でやったように分解します：\n",
    "\n",
    "$$\n",
    "\\sum_{t=t_k}^{t_{k+1}-1}(\\underbrace{\\left(p_k\\left(S_t\\right)-e_{S_t}\\right) \\mathfrak{b}_k}_{\\text {navigation error (1k) }}+\\underbrace{\\left(\\hat{p}_k\\left(S_t\\right)-p_k\\left(S_t\\right)\\right) h^*}_{\\text {empirical bias error (2k) }}+\\underbrace{\\left(\\tilde{p}_k\\left(S_t\\right)-\\hat{p}_k\\left(S_t\\right)\\right) \\mathfrak{b}_k}_{\\text {optimistic overshoot }(3 k)}+\\underbrace{\\left(\\hat{p}_k\\left(S_t\\right)-p_k\\left(S_t\\right)\\right)\\left(\\mathrm{b}_k-h^*\\right)}_{\\text {second order error (4k) }})\n",
    "$$\n",
    "\n",
    "TODO: 後はそれぞれをバウンドする\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験\n",
    "\n",
    "![river-swim](figs/river-swim.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状態数： 3\n",
      "行動数： 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax.random import PRNGKey\n",
    "import jax\n",
    "from typing import NamedTuple, Optional\n",
    "\n",
    "key = PRNGKey(0)\n",
    "\n",
    "S = 3  # 状態集合のサイズ\n",
    "A = 2  # 行動集合のサイズ．LEFTが0, RIGHTが1とします\n",
    "S_set = jnp.arange(S)  # 状態集合\n",
    "A_set = jnp.arange(A)  # 行動集合\n",
    "\n",
    "\n",
    "# 報酬行列（論文中では確率的ですが，今回は面倒なので決定的にします）\n",
    "rew = np.zeros((S, A))\n",
    "rew[0, 0] = 0.05\n",
    "rew[-1, 1] = 0.95\n",
    "rew = jnp.array(rew)\n",
    "assert rew.shape == (S, A)\n",
    "\n",
    "\n",
    "# 遷移確率行列\n",
    "P = np.zeros((S, A, S))\n",
    "for s in range(1, S-1):\n",
    "    P[s, 0, s-1] = 1  # LEFT\n",
    "    P[s, 1, s-1] = 0.05  # RIGHT\n",
    "    P[s, 1, s] = 0.6  # RIGHT\n",
    "    P[s, 1, s+1] = 0.35  # RIGHT\n",
    "\n",
    "# at s1\n",
    "P[0, 0, 0] = 1  # LEFT\n",
    "P[0, 1, 0] = 0.6  # RIGHT\n",
    "P[0, 1, 1] = 0.4  # RIGHT\n",
    "P[-1, 0, -2] = 1  # LEFT\n",
    "P[-1, 1, -2] = 0.05  # RIGHT\n",
    "P[-1, 1, -1] = 0.95  # RIGHT\n",
    "\n",
    "P = P.reshape(S, A, S)\n",
    "P = jnp.array(P)\n",
    "np.testing.assert_allclose(P.sum(axis=-1), 1, atol=1e-6)  # ちゃんと確率行列になっているか確認します\n",
    "\n",
    "class MDP(NamedTuple):\n",
    "    S_set: jnp.array  # 状態集合\n",
    "    A_set: jnp.array  # 行動集合\n",
    "    rew: jnp.array  # 報酬行列\n",
    "    P: jnp.array  # 遷移確率行列\n",
    "\n",
    "    @property\n",
    "    def S(self) -> int:  # 状態空間のサイズ\n",
    "        return len(self.S_set)\n",
    "\n",
    "    @property\n",
    "    def A(self) -> int:  # 行動空間のサイズ\n",
    "        return len(self.A_set)\n",
    "\n",
    "\n",
    "mdp = MDP(S_set, A_set, rew, P)\n",
    "\n",
    "print(\"状態数：\", mdp.S)\n",
    "print(\"行動数：\", mdp.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linprog\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def solve_optimistic_PV(count_next_SAS: np.ndarray, mdp: MDP, V: np.ndarray, tk: int, delta: float):\n",
    "    def bonus(count_next_S: np.ndarray):\n",
    "        # 論文のパラメータはちょっといじります\n",
    "        return np.sqrt(np.log(2 * mdp.A * tk / delta) / np.maximum(1, count_next_S.sum()))\n",
    "        # return np.sqrt(14*mdp.S*np.log(2 * mdp.A * tk / delta) / np.maximum(1, count_next_S.sum()))\n",
    "\n",
    "    def solve_per_sa(count_next_S):\n",
    "        est_P_sa = count_next_S / np.maximum(1, count_next_S.sum())\n",
    "\n",
    "        c = np.hstack([-V, np.zeros(S)])  # maximize PV\n",
    "        A_ub = np.hstack([np.eye(mdp.S), -np.eye(mdp.S)])\n",
    "        nA_ub = np.hstack([-np.eye(mdp.S), -np.eye(mdp.S)])\n",
    "        tA_ub = np.hstack([np.zeros(mdp.S), np.ones(mdp.S)])\n",
    "        A_ub = np.vstack([A_ub, nA_ub, tA_ub])\n",
    "        b_ub = np.hstack([est_P_sa, -est_P_sa, np.array([bonus(count_next_S)])])\n",
    "\n",
    "        A_eq = np.hstack([np.ones(mdp.S), np.zeros(mdp.S)]).reshape(1, -1)  # 総和が1になる制約\n",
    "        b_eq = np.array([1.0])  # 総和は1\n",
    "        res = linprog(c, A_ub, b_ub, A_eq, b_eq, bounds=(0, None)) \n",
    "        return -res.fun\n",
    "\n",
    "    PV = np.zeros((mdp.S, mdp.A))\n",
    "    for s in range(mdp.S):\n",
    "        for a in range(mdp.A):\n",
    "            count_next_S = count_next_SAS[s, a]\n",
    "            PV[s, a] = solve_per_sa(count_next_S)\n",
    "    return jnp.array(PV)\n",
    "    \n",
    "\n",
    "ref_state = 0 \n",
    "\n",
    "def ExtendedValueIteration(count_SAS: jnp.ndarray, mdp: MDP, tk: int, delta: float = 0.9, tol: float = 1e-5) -> jnp.array:\n",
    "    def condition_fun(nQ_Q):\n",
    "        nQ, Q = nQ_Q\n",
    "        nbias = nQ.max(axis=1)  # S -> R\n",
    "        bias = Q.max(axis=1)  # S -> R\n",
    "        span_diff = (nbias - bias).max()\n",
    "        return span_diff > tol\n",
    "\n",
    "    def body_fun(nQ_Q):\n",
    "        Q, _ = nQ_Q\n",
    "        next_v = solve_optimistic_PV(count_SAS, mdp=mdp, V=Q.max(axis=1), tk=tk, delta=delta)\n",
    "        gain = Q[ref_state].max()\n",
    "        nQ = mdp.rew + next_v - gain\n",
    "        return (nQ, Q)\n",
    "\n",
    "    init_Q = jnp.zeros((mdp.S, mdp.A))\n",
    "    nQ_Q = (init_Q, init_Q)\n",
    "    nQ_Q = body_fun(nQ_Q)\n",
    "    while condition_fun(nQ_Q):\n",
    "        nQ_Q = body_fun(nQ_Q)\n",
    "    return nQ_Q[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample_next_state(mdp: MDP, s: int, a: int):\n",
    "    \"\"\" sample next state according to the transition matrix P\n",
    "    Args:\n",
    "        mdp: MDP\n",
    "        s: int\n",
    "        a: int\n",
    "    Returns:\n",
    "        next_s: int\n",
    "    \"\"\"\n",
    "    probs = np.array(mdp.P[s, a])\n",
    "    return np.random.choice(mdp.S_set, p=probs)\n",
    "\n",
    "\n",
    "def sample_eps_greedy_act(mdp: MDP, q_s: np.array, eps: float):\n",
    "    if random.random() < eps:\n",
    "        return random.randint(0, mdp.A-1)\n",
    "    else:\n",
    "        return q_s.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適ゲインを評価する用\n",
    "from itertools import product\n",
    "\n",
    "@jax.jit\n",
    "def compute_optimal_bias(mdp: MDP, tol: float = 1e-6) -> jnp.array:\n",
    "    ref_state = 0 \n",
    "    def condition_fun(nV_V):\n",
    "        nV, V = nV_V\n",
    "        span_diff = (nV - V).max()\n",
    "        return span_diff > tol\n",
    "\n",
    "    def body_fun(nV_V):\n",
    "        V, _ = nV_V\n",
    "        gain = V[ref_state]\n",
    "        next_v = mdp.P @ V\n",
    "        nV = (mdp.rew + next_v).max(axis=1) - gain\n",
    "        return (nV, V)\n",
    "\n",
    "    init_V = jnp.zeros((mdp.S))\n",
    "    nV_V = body_fun((init_V, init_V))\n",
    "    V, _ = jax.lax.while_loop(condition_fun, body_fun, nV_V)\n",
    "    return V\n",
    "\n",
    "optimal_bias = compute_optimal_bias(mdp)\n",
    "optimal_gain = optimal_bias[ref_state]\n",
    "optimal_bias_diff = np.zeros((S, S))\n",
    "for s, ns in product(range(S), range(S)):\n",
    "    optimal_bias_diff[s, ns] = optimal_bias[s] - optimal_bias[ns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:04<00:00,  4.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "K = 20\n",
    "init_s = 0\n",
    "\n",
    "count_SAS = np.zeros((S, A, S))  # 訪問回数\n",
    "\n",
    "# =====================================\n",
    "# s -> s' の往復のトリガー．\n",
    "# Trueならs -> s'の往復中\n",
    "# Falseなら s -> s' の往復がまだ始まっていない（sに出会うと往復スタート）\n",
    "com_trigger = np.zeros((S, S), dtype=bool)\n",
    "com_counts = np.zeros((S, S))  # s -> s' の往復回数\n",
    "# =====================================\n",
    "# バイアスの推定器\n",
    "bias_diff_est = np.zeros((S, S))\n",
    "com_signed_timesum = np.zeros((S, S))  # バイアスの推定に使う\n",
    "com_signed_rewsum = np.zeros((S, S))  # バイアスの推定に使う\n",
    "# =====================================\n",
    "\n",
    "total_rew = 0\n",
    "regrets = []\n",
    "t = 1\n",
    "s = init_s\n",
    "for epi in tqdm(range(K)):\n",
    "    epi_count_SAS = np.zeros((S, A, S))\n",
    "    Q = ExtendedValueIteration(count_SAS, mdp, t)\n",
    "\n",
    "    # 探索をします　\n",
    "    while True:\n",
    "        # =====================================\n",
    "        # 行動と報酬の選択\n",
    "        a = sample_eps_greedy_act(mdp, Q[s], 0.0)\n",
    "        r = mdp.rew[s, a]\n",
    "\n",
    "        # リグレットの計算\n",
    "        total_rew += r\n",
    "        regret = t * optimal_gain - total_rew\n",
    "        regrets.append(regret)\n",
    "        # =====================================\n",
    "        # 往復中のs -> s'では...sumを更新\n",
    "        com_sign = (-1)**com_counts * com_trigger  # 往復中のやつにsignをつける\n",
    "        com_signed_timesum += com_sign  # 往復中のやつは +sign x 1\n",
    "        com_signed_rewsum += com_sign * r  # 往復中のやつは +sign x r\n",
    "\n",
    "        # バイアスの推定器の更新\n",
    "        gain_est = total_rew / (t + 1)\n",
    "        bias_diff_est = (com_signed_timesum * gain_est - com_signed_rewsum) / (com_counts + 1e-6)\n",
    "        # =====================================\n",
    "        # 往復回数の更新\n",
    "        com_counts[:, s][com_trigger[:, s]] += 1  # trigger==1 & sに出会ったら往復終了\n",
    "        com_trigger[:, s] = False  # : -> s でtriggerが1のものは往復終了なので-1にする．それ以外は往復が始まってないので-1 （結局全部-1）\n",
    "        com_trigger[s, :] = True  # s -> : は往復が始まるので1にする\n",
    "        # =====================================\n",
    "        # 次状態に遷移\n",
    "        next_s = sample_next_state(mdp, s, a)\n",
    "        epi_count_SAS[s, a, next_s] += 1\n",
    "        s = next_s\n",
    "        t = t + 1\n",
    "        # =====================================\n",
    "        # doubling trick\n",
    "        if epi_count_SAS[s, a].sum() >= max(1, count_SAS[s, a].sum()):\n",
    "            break\n",
    "        # =====================================\n",
    "\n",
    "    count_SAS += epi_count_SAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 66.,  13.,   9.],\n",
       "       [ 13., 128.,  47.],\n",
       "       [  9.,  47., 779.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-0.77763534, -0.6554653 , -0.17446387],\n",
       "       [-1.2828251 ,  0.05487633,  0.71284974],\n",
       "       [ 3.9274797 , -0.27071762, -0.01300531]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_diff_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -2.04615235, -4.67692041],\n",
       "       [ 2.04615235,  0.        , -2.6307683 ],\n",
       "       [ 4.67692041,  2.6307683 ,  0.        ]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_bias_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方策ゲインを評価する用\n",
    "from itertools import product\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_bias(mdp: MDP, pol: np.ndarray, tol: float = 1e-6) -> jnp.array:\n",
    "    ref_state = 0 \n",
    "    def condition_fun(nV_V):\n",
    "        nV, V = nV_V\n",
    "        span_diff = jnp.abs(nV - V).max()\n",
    "        return span_diff > tol\n",
    "\n",
    "    def body_fun(nV_V):\n",
    "        V, _ = nV_V\n",
    "        gain = V[ref_state]\n",
    "        next_v = mdp.P @ V\n",
    "        nV = ((mdp.rew + next_v) * pol).sum(axis=1) - gain\n",
    "        return (nV, V)\n",
    "\n",
    "    init_V = jnp.zeros((mdp.S))\n",
    "    nV_V = body_fun((init_V, init_V))\n",
    "    V, _ = jax.lax.while_loop(condition_fun, body_fun, nV_V)\n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_policy = jnp.ones((S, A)) / A\n",
    "uniform_bias = compute_policy_bias(mdp, uniform_policy)\n",
    "uniform_bias_diff = np.zeros((S, S))\n",
    "for s, ns in product(range(S), range(S)):\n",
    "    uniform_bias_diff[s, ns] = uniform_bias[s] - uniform_bias[ns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -0.15789437, -0.95488614],\n",
       "       [ 0.15789437,  0.        , -0.79699177],\n",
       "       [ 0.95488614,  0.79699177,  0.        ]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniform_bias_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "S = 3\n",
    "T = 8\n",
    "state_history = np.array([0, 1, 2, 0, 0, 1, 2, 0])\n",
    "rew_history = np.random.uniform(0, 1.0, size=T)\n",
    "# s -> s' の往復のトリガー．\n",
    "# Trueならs -> s'の往復中\n",
    "# Falseなら s -> s' の往復がまだ始まっていない（sに出会うと往復スタート）\n",
    "com_trigger = np.zeros((S, S), dtype=bool)\n",
    "com_counts = np.zeros((S, S))  # s -> s' の往復回数\n",
    "\n",
    "bias_diff_est = np.zeros((S, S))  # バイアスの推定器\n",
    "com_signed_timesum = np.zeros((S, S))  # バイアスの推定に使う\n",
    "com_signed_rewsum = np.zeros((S, S))  # バイアスの推定に使う\n",
    "\n",
    "\n",
    "for s, r in zip(state_history, rew_history):\n",
    "    # 往復中のs -> s'では...sumを更新\n",
    "    com_sign = (-1)**(com_counts) * com_trigger  # 往復中のやつにsignをつける\n",
    "    com_signed_timesum += com_sign  # 往復中のやつは +sign x 1\n",
    "    com_signed_rewsum += com_sign * r  # 往復中のやつは +sign x r\n",
    "\n",
    "    com_counts[:, s][com_trigger[:, s]] += 1  # trigger==1 & sに出会ったら往復終了\n",
    "    # : -> s でtriggerが1のものは往復終了なので-1にする\n",
    "    # それ以外は往復が始まってないので-1 （結局全部-1）\n",
    "    com_trigger[:, s] = False\n",
    "\n",
    "    # s -> : は往復が始まるので1にする\n",
    "    com_trigger[s, :] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5., -1., -1.],\n",
       "       [ 0.,  2.,  0.],\n",
       "       [ 0.,  2.,  3.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com_signed_timesum\n",
    "# com_signed_rewsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 0, 0, 1, 2, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
