{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Span情報がない平均報酬強化学習\n",
    "\n",
    "参考：\n",
    "* [Achieving Tractable Minimax Optimal Regret in Average Reward MDPs](https://arxiv.org/abs/2406.01234)\n",
    "\n",
    "Average RewardでのRLは一般にSpanの情報や半径の情報が必要になります．今回はSpanが不要なMDPについて学んでみましょう．\n",
    "\n",
    "表記：\n",
    "* MDP：$M \\in \\mathcal{M}$\n",
    "* ゲイン：$g^\\pi(s):=\\lim \\frac{1}{T} \\mathbf{E}_s^\\pi\\left[R_0+\\ldots+R_{T-1}\\right]$\n",
    "* バイアス：$h^\\pi:=\\lim \\sum_{t=0}^{T-1}\\left(R_t-g\\left(S_t\\right)\\right)$\n",
    "* Poisson方程式：$h^\\pi+g^\\pi=r^\\pi+P^\\pi h^\\pi$\n",
    "* ベルマン方程式：$L u(s):=\\max _{a \\in \\mathcal{A}(s)}\\{r(s, a)+p(s, a) u\\}$\n",
    "  * 今回はWeakly communicatingの設定を考える．つまり，$Lh^* - h^* \\in \\boldsymbol{R}e$を満たすような$h^*$が存在する．\n",
    "  * これは任意の方策に対して$r^\\pi+P^\\pi h^* \\leq g^*+h^*$を満たす\n",
    "* ベルマン誤差：$\\Delta^*(s, a):=h^*(s)+g^*(s)-r(s, a)-p(s, a) h^* \\geq 0$\n",
    "* 直径：$D:=\\sup _{s \\neq s^{\\prime}} \\inf _\\pi \\mathbf{E}_s^\\pi\\left[\\inf \\left\\{t \\geq 1: S_t=s^{\\prime}\\right\\}\\right]$\n",
    "* リグレット：\n",
    "    * $\\operatorname{Reg}(T):=T g^*-\\sum_{t=0}^{T-1} R_t$\n",
    "    * $\\mathbf{E}[\\operatorname{Reg}(T)]=\\mathbf{E}\\left[\\sum_{t=0}^{T-1} \\Delta^*\\left(X_t\\right)\\right]+\\mathbf{E}\\left[h^*\\left(S_0\\right)-h^*\\left(S_T\\right)\\right]$\n",
    "* SpanがバウンドされたMDPの集合：$\\mathcal{M}_c:=\\left\\{M \\in \\mathcal{M}: \\exists h^* \\in \\operatorname{Fix}(L(M)), \\operatorname{sp}\\left(h^*\\right) \\leq c\\right\\}$\n",
    "  * 下界：$\\max _{M \\in \\mathcal{M}_c} \\mathbf{E}^{M, \\mathbf{A}}[\\operatorname{Reg}(T)]=\\Omega(\\sqrt{c S A T})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PMEVI-DT アルゴリズム\n",
    "\n",
    "基本的なアイデアはOFUと同じです．OFUは次の楽観的なゲインを計算します：\n",
    "$$\n",
    "g^*\\left(\\mathcal{M}_t\\right):=\\sup \\left\\{g^\\pi\\left(\\mathcal{M}_t\\right): \\pi \\in \\Pi, \\operatorname{sp}\\left(g^\\pi\\left(\\mathcal{M}_t\\right)\\right)=0\\right\\} \\text { with } g^\\pi\\left(\\mathcal{M}_t\\right):=\\sup \\left\\{g(\\pi, \\widetilde{M}): \\widetilde{M} \\in \\mathcal{M}_t\\right\\}\n",
    "$$\n",
    "\n",
    "OFUの更新タイミングはいろいろありますが，今回はDoubling trickを使います．つまり，\n",
    "$$\n",
    "N_t\\left(S_t, \\pi_k\\left(S_t\\right)\\right) \\geq 1 \\vee 2 N_{t_k}\\left(X_t\\right)\n",
    "$$\n",
    "のタイミングで更新を行います（$X_t = (S_t, A_t)$）．\n",
    "\n",
    "### Extended Value Iteration (EVI)について\n",
    "\n",
    "UCRL2など，OFUを実現するためには基本的にEVIを使います．\n",
    "$(s, a)$-rectangularな不確実集合$\\mathcal{M}_t \\equiv \\prod_{s, a}\\left(\\mathcal{R}_t(s, a) \\times \\mathcal{P}_t(s, a)\\right)$を作り，次の楽観的な作用素でバイアス関数を更新します：\n",
    "\n",
    "$$\n",
    "v_{i+1}(s) \\equiv \\mathcal{L}_t v_i(s):=\\max _{a \\in \\mathcal{A}(s)} \\max _{\\tilde{r}(s, a) \\in \\mathcal{R}_t(s, a)} \\max _{\\tilde{p}(s, a) \\in \\mathcal{P}_t(s, a)}\\left(\\tilde{r}(s, a)+\\tilde{p}(s, a) \\cdot v_i\\right)\n",
    "$$\n",
    "\n",
    "そして，スパンが$\\operatorname{sp}\\left(v_{i+1}-v_i\\right)<\\epsilon$を満たすまで繰り返すのがEVIです．このとき，$\\mathcal{L}_t v_i$を与える方策は\n",
    "$g^\\pi\\left(\\mathcal{M}_t\\right) \\geq g^*(\\mathcal{M})-\\epsilon$を満たすことが知られています．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projected Mitigated EVI\n",
    "\n",
    "基本的に，OFUは$\\mathcal{M}_t$の良さによって実現されます．\n",
    "よって，多くの先行研究は$\\mathcal{M}_t$を改善できるように様々な工夫をこらしてました．\n",
    "\n",
    "今回の論文は，あまり$\\mathcal{M}_t$を改善することに固執していません．\n",
    "* いい感じの挙動をする信頼区間を使い，\n",
    "* バイアスの推定をして，\n",
    "* EVIを改善する\n",
    "\n",
    "ことで，Minimax最適なアルゴリズムを達成するのが今回の論文です．\n",
    "\n",
    "これを説明するために，何らかの方法で，$h^*$を推定するためのバイアスの信頼区間$\\mathcal{H}_t$が与えられているとします．$M\\in \\mathcal{M}_t$かつ$h^* \\in \\mathcal{H}_t$が満たされているならば，\n",
    "* ゲインを最大化して，かつ$h(\\pi, \\tilde{M}) \\in \\mathcal{H}_t$を満たすような方策とMDPのペア$(\\pi, \\tilde{M})$を見つければ，OFUできそうな気がします．\n",
    "\n",
    "そこで，「Projection」と「Mitigation」の２つのテクニックを使います．\n",
    "\n",
    "1. Projection: もし$h^* \\in \\mathcal{H}_t$ならば，OFUで探す最適方策はバイアスが$\\mathcal{H}_t$の中に入るものに限定して構いません．そこで，$\\Gamma_t: \\mathbf{R}^{\\mathcal{S}} \\rightarrow \\mathcal{H}_t$を使ってバイアスを射影します．\n",
    "2. Mitigation: 一旦ボーナスベースのアルゴリズムについて考えてみましょう．ボーナスベースのアルゴリズムは，\n",
    "$$\n",
    "\\tilde{p}(s, a) u_i \\leq \\hat{p}_t(s, a) u_i+\\underbrace{\\left(p(s, a)-\\hat{p}_t(s, a)\\right) u_i}_{\\leq ボーナス関数}\n",
    "$$\n",
    "によって，$推定した遷移\\cdot 価値+ボーナス$を使ってOFUを実現します．\n",
    "今回のアルゴリズムはこれを利用します．\n",
    "もし$h^* \\in \\mathcal{H}_t$ならば，$\\beta_t(s, a):=\\max _{u \\in \\mathcal{H}_t} \\beta_t(s, a, u)$とすれば，$h^*$がわからなくても，$\\left(\\hat{p}_t(s, a)-p(s, a)\\right) h^* \\leq \\beta_t(s, a)$が成立します．\n",
    "これを使って，次のEVIを後で利用します．\n",
    "$$\n",
    "\\mathcal{L}_t^\\beta u(s):=\\max _{a \\in \\mathcal{A}(s)} \\sup _{\\tilde{r}(s, a) \\in \\mathcal{R}_t(s, a)} \\sup _{\\tilde{p}(s, a) \\in \\mathcal{P}_t(s, a)}\\left\\{\\tilde{r}(s, a)+\\min \\left\\{\\tilde{p}(s, a) u_i, \\hat{p}_t(s, a) u_i+\\beta_t(s, a)\\right\\}\\right\\}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "上のProjectionとMitigationを踏まえて，今回のアルゴリズムでは次の「MitigateしてProjection」を繰り返します：\n",
    "\n",
    "$$\\mathfrak{L}_t:=\\Gamma_t \\circ \\mathcal{L}_t^\\beta$$\n",
    "\n",
    "これはCompositionなので，うまく動くかは自明ではありません．しかし，次の定理によって挙動が保証されます：\n",
    "\n",
    "---\n",
    "\n",
    "固定した$\\beta$を考えます．ここで，次を満たす$\\Gamma_t: \\boldsymbol{R}^{\\mathcal{X}} \\to \\mathcal{H}_t$を考えましょう：\n",
    "1. $u \\leq v \\Rightarrow \\Gamma u \\leq \\Gamma v ;$\n",
    "2. $\\operatorname{sp}(\\Gamma u-\\Gamma v) \\leq \\operatorname{sp}(u-v)$\n",
    "3. $\\Gamma(u+\\lambda e)=\\Gamma u+\\lambda e$\n",
    "4. $\\Gamma u \\leq u$\n",
    "\n",
    "このとき，$\\mathfrak{L}_t:=\\Gamma_t \\circ \\mathcal{L}_t^\\beta$は次を満たします：\n",
    "\n",
    "書くのがめんどいので省略．\n",
    "結局Biasのconfidence regionが正しければうまく動く．\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## バイアスの推定機\n",
    "\n",
    "今回のバイアスの推定器として，次の制約を組み合わせたものを考えます：\n",
    "$$\\forall s \\neq s^{\\prime}, \\quad \\mathfrak{h}(s)-\\mathfrak{h}\\left(s^{\\prime}\\right)-c\\left(s, s^{\\prime}\\right) \\leq d\\left(s, s^{\\prime}\\right)$$\n",
    "\n",
    "---\n",
    "\n",
    "**Bias difference estimator**\n",
    "\n",
    "$s \\neq s^{\\prime}$が与えられたときに，次の$\\left(\\tau_i^{s \\leftrightarrow s^{\\prime}}\\right)_{i \\geq 0}$をcommute timeの系列と呼ぶ：\n",
    "* $\\tau_{2 i}^{s \\leftrightarrow s^{\\prime}}:=\\inf \\left\\{t>\\tau_{2 i-1}^{s \\leftrightarrow s^{\\prime}}: S_t=s\\right\\}$：$s$に訪れる，$\\tau_{2 i-1}^{s \\leftrightarrow s^{\\prime}}$より後の時刻\n",
    "* $\\tau_{2 i+1}^{s \\leftrightarrow s^{\\prime}}:=\\inf \\left\\{t>\\tau_{2 i}^{s \\leftrightarrow s^{\\prime}}: S_t=s^{\\prime}\\right\\}$：$s'$に訪れる，$\\tau_{2 i}^{s \\leftrightarrow s^{\\prime}}$より後の時刻．$i$は往復回数を表してるっぽい．\n",
    "* $\\tau_{2 -1}^{s \\leftrightarrow s^{\\prime}}:=-\\infty$とする\n",
    "* $N_t\\left(s \\leftrightarrow s^{\\prime}\\right):=\\sup \\left\\{i: \\tau_i^{s \\leftrightarrow s^{\\prime}} \\leq t\\right\\}$：時刻$t$以前に起きた往復回数？（多分supだと思われる）\n",
    "* $\\hat{g}(t):=\\frac{1}{t} \\sum_{i=0}^{t-1} R_i$：ゲインの推定器\n",
    "\n",
    "これを使って，Bias difference estimatorは\n",
    "\n",
    "$$\n",
    "N_t\\left(s \\leftrightarrow s^{\\prime}\\right) c_T\\left(s, s^{\\prime}\\right)=\\sum_{t=0}^{N_T\\left(s \\leftrightarrow s^{\\prime}\\right)-1}(-1)^i \\sum_{t=\\tau_i^{\\tau_s} s^{s^{\\prime}}}^{\\tau_{i+s^{\\prime}}^{s \\leftrightarrow s^{\\prime}}-1}\\left(\\hat{g}(T)-R_t\\right) .\n",
    "$$\n",
    "\n",
    "と表記する．\n",
    "\n",
    "---\n",
    "\n",
    "このとき，次が高確率で成立します：　\n",
    "任意の$T^{\\prime} \\leq T$ と$\\tilde{g} \\geq g^*$，そして指標$c_T\\left(s, s^{\\prime}\\right) \\in \\mathbf{R}$について，\n",
    "$$\n",
    "N_{T^{\\prime}}\\left(s \\leftrightarrow s^{\\prime}\\right)\\left|\\underbrace{h^*(s)-h^*\\left(s^{\\prime}\\right)-c_{T^{\\prime}}\\left(s, s^{\\prime}\\right)}_{\"真のバイアスの差分\"と\"事前知識\"の差}\\right| \\leq \\underbrace{3 \\operatorname{sp}\\left(h^*\\right)}_{無視できそう}+\\left(1+\\operatorname{sp}\\left(h^*\\right)\\right) \\sqrt{8 T \\log \\left(\\frac{2}{\\delta}\\right)}+\\underbrace{2 \\sum_{t=0}^{T^{\\prime}-1}\\left(\\tilde{g}-R_t\\right)}_{リグレット}\n",
    "$$\n",
    "\n",
    "$\\operatorname{sp}$は基本的に未知なので，$c_0:=T^{1 / 5}$で近似します．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験\n",
    "\n",
    "![river-swim](figs/river-swim.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状態数： 3\n",
      "行動数： 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax.random import PRNGKey\n",
    "import jax\n",
    "from typing import NamedTuple, Optional\n",
    "\n",
    "key = PRNGKey(0)\n",
    "\n",
    "S = 3  # 状態集合のサイズ\n",
    "A = 2  # 行動集合のサイズ．LEFTが0, RIGHTが1とします\n",
    "S_set = jnp.arange(S)  # 状態集合\n",
    "A_set = jnp.arange(A)  # 行動集合\n",
    "\n",
    "\n",
    "# 報酬行列（論文中では確率的ですが，今回は面倒なので決定的にします）\n",
    "rew = np.zeros((S, A))\n",
    "rew[0, 0] = 0.05\n",
    "rew[-1, 1] = 0.95\n",
    "rew = jnp.array(rew)\n",
    "assert rew.shape == (S, A)\n",
    "\n",
    "\n",
    "# 遷移確率行列\n",
    "P = np.zeros((S, A, S))\n",
    "for s in range(1, S-1):\n",
    "    P[s, 0, s-1] = 1  # LEFT\n",
    "    P[s, 1, s-1] = 0.05  # RIGHT\n",
    "    P[s, 1, s] = 0.6  # RIGHT\n",
    "    P[s, 1, s+1] = 0.35  # RIGHT\n",
    "\n",
    "# at s1\n",
    "P[0, 0, 0] = 1  # LEFT\n",
    "P[0, 1, 0] = 0.6  # RIGHT\n",
    "P[0, 1, 1] = 0.4  # RIGHT\n",
    "P[-1, 0, -2] = 1  # LEFT\n",
    "P[-1, 1, -2] = 0.05  # RIGHT\n",
    "P[-1, 1, -1] = 0.95  # RIGHT\n",
    "\n",
    "P = P.reshape(S, A, S)\n",
    "P = jnp.array(P)\n",
    "np.testing.assert_allclose(P.sum(axis=-1), 1, atol=1e-6)  # ちゃんと確率行列になっているか確認します\n",
    "\n",
    "class MDP(NamedTuple):\n",
    "    S_set: jnp.array  # 状態集合\n",
    "    A_set: jnp.array  # 行動集合\n",
    "    rew: jnp.array  # 報酬行列\n",
    "    P: jnp.array  # 遷移確率行列\n",
    "\n",
    "    @property\n",
    "    def S(self) -> int:  # 状態空間のサイズ\n",
    "        return len(self.S_set)\n",
    "\n",
    "    @property\n",
    "    def A(self) -> int:  # 行動空間のサイズ\n",
    "        return len(self.A_set)\n",
    "\n",
    "\n",
    "mdp = MDP(S_set, A_set, rew, P)\n",
    "\n",
    "print(\"状態数：\", mdp.S)\n",
    "print(\"行動数：\", mdp.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.05, 0.  ],\n",
       "       [0.  , 0.  ],\n",
       "       [0.  , 0.95]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp.rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve bellman equation\n",
    "\n",
    "ref_state = 0 \n",
    "\n",
    "@jax.jit\n",
    "def V_value_iteration(mdp: MDP, tol: float = 1e-6) -> jnp.array:\n",
    "    def condition_fun(nV_V):\n",
    "        nV, V = nV_V\n",
    "        span_diff = (nV - V).max()\n",
    "        return span_diff > tol\n",
    "\n",
    "    def body_fun(nV_V):\n",
    "        V, _ = nV_V\n",
    "        gain = V[ref_state]\n",
    "        next_v = mdp.P @ V\n",
    "        nV = (mdp.rew + next_v).max(axis=1) - gain\n",
    "        return (nV, V)\n",
    "\n",
    "    init_V = jnp.zeros((mdp.S))\n",
    "    nV_V = body_fun((init_V, init_V))\n",
    "    V, _ = jax.lax.while_loop(condition_fun, body_fun, nV_V)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.05, 0.  ],\n",
       "       [0.  , 0.  ],\n",
       "       [0.  , 0.95]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
