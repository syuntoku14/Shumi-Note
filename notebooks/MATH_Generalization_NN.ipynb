{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニューラルネットワークの汎化誤差解析\n",
    "\n",
    "今回はニューラルネットワークの汎化誤差解析をします．隠れ層１層のニューラルネットワークを考えましょう．また，ニューラルネットワークのラデマッハ複雑度の導出を考えるので，特に出力の形式や損失関数については定義せずに進めます．\n",
    "\n",
    "表記と設定：\n",
    "* $m$を隠れ層のノード数とします．\n",
    "* 観測データの入力：$\\left\\{x_i\\right\\}_{i=1}^n \\subset \\mathbb{R}^d$\n",
    "* NNのパラメータを$w \\in \\mathbb{R}^m$，$W \\in \\mathbb{R}^{m \\times d}$として，$\\theta = \\{w, W\\}$と表現します．\n",
    "* 活性化関数を$a(x)=\\max\\{0, x\\}$とします．つまりReluです．\n",
    "* 最後にNN$f_\\theta : \\mathbb{R}^d \\to \\mathbb{R}$を$f_\\theta(x)=w^{\\top} a(W x)$とします．\n",
    "\n",
    "また，次の制約を考えます．\n",
    "* $\\mathcal{F}=\\left\\{f_\\theta:\\|w\\|_2 \\leq B_2,\\left\\|W_j\\right\\|_2 \\leq B_1\\left(W_j \\in \\mathbb{R}^d, j=  1, \\ldots, m\\right)\\right\\}$\n",
    "* $\\mathbb{E}\\left[\\|x\\|_2\\right]=C$\n",
    "\n",
    "このとき，NNのラデマッハ複雑度は\n",
    "$$\n",
    "\\begin{aligned}\n",
    "R_n(\\mathcal{F}) & =\\mathbb{E}_{x, \\sigma}\\left[\\sup _{f_\\theta \\leq \\mathcal{F}} \\frac{1}{n} \\sum_{i=1}^n \\sigma_i f_\\theta\\left(x_i\\right)\\right] \\\\\n",
    "& \\leq 2 B_1 B_2 C \\sqrt{\\frac{m}{n}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "で得られます．\n",
    "\n",
    "**証明**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& R_n(\\mathcal{F})=\\underset{x, \\sigma}{\\mathbb{E}} \\sup _{\\substack{\\|w\\|_2 \\leq B_2 \\\\\n",
    "\\left\\|W_j\\right\\|_2 \\leq B_1}} \\frac{1}{n} \\sum_{i=1}^n \\sigma_i w^{\\top} a\\left(W x_i\\right) \\\\\n",
    "& =\\underset{x, \\sigma}{\\mathbb{E}} \\sup _{\\substack{\\|w\\|_2 \\leq B_2 \\\\\n",
    "\\left\\|W_j\\right\\|_2 \\leq B_1}} w^{\\top}\\left(\\frac{1}{n} \\sum_{i=1}^n \\sigma_i a\\left(W x_i\\right)\\right) \\\\\n",
    "& \\leq \\underset{x, \\sigma}{\\mathbb{E}} \\sup _{\\substack{\\|w\\|_2 \\leq B_2 \\\\\n",
    "\\left\\|W_j\\right\\|_2 \\leq B_1}}\\|w\\|_2\\left\\|\\frac{1}{n} \\sum_{i=1}^n \\sigma_i a\\left(W x_i\\right)\\right\\|_2 \\\\\n",
    "& \\leq B_2 \\underset{x, \\sigma}{\\mathbb{E}} \\sup _{\\left\\|W_j\\right\\|_2 \\leq B_1}\\left\\|\\frac{1}{n} \\sum_{i=1}^n \\sigma_i a\\left(W x_i\\right)\\right\\|_2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "逆に言えば，内積の形を抑えるためにL2正則化を仮定したわけですね．L2正則化はLayer normalizationなどで出てくるので一般的な仮定になります．\n",
    "ここからさらに\n",
    "\n",
    "$$\\begin{aligned} \n",
    "& \\leq B_2 \\underset{x, \\sigma}{\\mathbb{E}} \\max _j \\sup _{\\left\\|W_j\\right\\|_2 \\leq B_1} \\sqrt{m\\left(\\frac{1}{n} \\sum_{i=1}^n \\sigma_i a\\left(W_j^{\\top} x_i\\right)\\right)^2} \\\\ & =B_2 \\sqrt{m} \\underset{x, \\sigma}{\\mathbb{E}} \\max _j \\sup _{\\left\\|W_j\\right\\|_2 \\leq B_1}\\left|\\frac{1}{n} \\sum_{i=1}^n \\sigma_i a\\left(W_j^{\\top} x_i\\right)\\right|\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "が成り立ちます．ここで，重みが$m$個あるわけですが，一番影響が大きい$j$番目だけ持ってきて，それを$m$倍することで上から抑えています．\n",
    "ここでさらに絶対値の部分は$|x|=\\max\\{x, -x\\}$であることに気をつけましょう．また，特に$x, y \\geq 0$であれば，$\\max\\{x, y\\} \\leq x + y$です．これを使うと，\n",
    "$$\n",
    "\\sup _{\\left\\|W_j\\right\\|_2 \\leq B_1}\\left|\\frac{1}{n} \\sum_{i=1}^n \\sigma_i a\\left(W_j^{\\top} x_i\\right)\\right|\n",
    "\\leq \n",
    "\\sup _{\\left\\|W_j\\right\\|_2 \\leq B_1} \\frac{1}{n} \\sum_{i=1}^n \\sigma_i a\\left(W_j^{\\top} x_i\\right)\n",
    "+ \n",
    "\\sup _{\\left\\|W_j\\right\\|_2 \\leq B_1}(-1) \\times \\frac{1}{n} \\sum^n_{i=1} \\sigma_i a\\left(W_j^{\\top} x_i\\right)\n",
    "$$\n",
    "が成り立ちます．ここで$\\sup _{\\left\\|W_j\\right\\|_2 \\leq B_1}$を考えているので，絶対値の中身が０以上になることを使ってます（$W$に０が取れるため）．\n",
    "\n",
    "最後に，Reluのリプシッツ定数は$1$なので，結局\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\sup _{\\left\\|W_j\\right\\|_2 \\leq B_1}\\left|\\frac{1}{n} \\sum_{i=1}^n \\sigma_i a\\left(W_j^{\\top} x_i\\right)\\right| \\\\\n",
    "& \\leq \\sup _{\\left\\|W_j\\right\\|_2 \\leq B_1} \\frac{1}{n} \\sum_{i=1}^n \\sigma_i W_j^{\\top} x_i+\\sup _{\\left\\|W_j\\right\\|_2 \\leq B_1} \\frac{1}{n} \\sum_{i=1}^n(-1) \\sigma_i W_j^{\\top} x_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "が出てきます．よって，\n",
    "$$\n",
    "R_n(\\mathcal{F}) \\leq B_2 \\sqrt{m}\\left(2 \\frac{B_1 C}{\\sqrt{n}}\\right)\n",
    "$$\n",
    "が成り立ちます．\n",
    "\n",
    "**補足**\n",
    "\n",
    "この導出は小規模なNNのときは問題ありません．つまり，ノード数$m$がデータ数$n$より十分小さければ意味のあるバウンドです．\n",
    "しかし，最近のNNはOver parameterizationされる場合が多く，$m >> n$の場合がよくあります．このときに何か意味のあるバウンドは出せるでしょうか？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNetの解析\n",
    "\n",
    "TODO: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
