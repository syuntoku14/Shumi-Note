{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データソースが複数ある場合のオフラインRLについて\n",
    "\n",
    "参考\n",
    "* [Provably Efficient Offline Reinforcement Learning with Perturbed Data Sources](https://arxiv.org/abs/2306.08364)\n",
    "\n",
    "通常のオフライン強化学習の理論ではデータの出所が単一ですが，アプリケーションではそのような状況は稀です．\n",
    "今回はデータが複数のソースからくる場合のオフライン強化学習について見てみます．（論文のFigure 1参照）\n",
    "\n",
    "表記\n",
    "* 通常のエピソディックMDPを考えます．初期分布を$\\xi \\in \\Delta(\\mathcal{S})$として，$V_1^{\\pi, \\mathcal{M}}(\\xi):=\\mathbb{E}_{s \\sim \\xi}\\left[V_1^{\\pi, \\mathcal{M}}(s)\\right]$と書きます．\n",
    "* Occupancy measureは$d_h^{\\pi, \\mathcal{M}}(s ; \\xi):=\\mathbb{E}_{\\pi, \\mathcal{M}}\\left[\\mathbb{1}\\left\\{s_h=s\\right\\} \\mid s_1 \\sim \\xi\\right]$と書きます．\n",
    "* Suboptimality gapを$\\operatorname{Gap}(\\hat{\\pi} ; \\mathcal{M}, \\xi):=V_1^{\\pi^*, \\mathcal{M}}(\\xi)-V_1^{\\hat{\\pi}, \\mathcal{M}}(\\xi)$とします．\n",
    "* $L$個の異なるソースからデータがやってくる設定を考えます．$l$個目のデータソースを$\\mathcal{M}_l=\\left(H, \\mathcal{S}, \\mathcal{A}, \\mathbb{P}_l, r_l\\right)$とします．ターゲットタスクは$\\mathcal{M}$です．また，データソースのタスクのMDPとターゲットタスクのMDPには次の関係が成立することを仮定します：\n",
    "\n",
    "---\n",
    "\n",
    "データソースのMDP $\\left\\{\\mathcal{M}_l=\\left\\{H, \\mathcal{S}, \\mathcal{A}, \\mathbb{P}_l, r_l\\right\\}: l \\in[L]\\right\\}$は未知の分布$g=\\left\\{g_h: h \\in[H]\\right\\}$から，それぞれの$(l, h) \\in[L] \\times[H]$について，独立してサンプルされるとする．ここで，そのサンプルの期待値は$\\left\\{r_h, \\mathbb{P}_h\\right\\}$になると仮定する．\n",
    "\n",
    "---\n",
    "\n",
    "問題設定\n",
    "* それぞれのデータソースから，データセット$\\mathcal{D}_l:=\\left\\{\\left(s_{1, l}^k, a_{1, l}^k, r_{1, l}^k, \\cdots, s_{H, l}^k, a_{H, l}^k, r_{H, l}^k\\right): k \\in[K]\\right\\}$がサンプルされてくるとします．\n",
    "    * このようにして作られたデータセットは，各$l$内のデータは相関があることに注意しましょう．\n",
    "    * これはTwo-fold sub-samplingテクニックで回避できます：[Settling the Sample Complexity of Model-Based Offline Reinforcement Learning](https://arxiv.org/abs/2204.05275)など参照．\n",
    "    * Two-fold sub-samplingで再構成されたデータセットを$\\left\\{\\mathcal{D}_l^{\\prime}: l \\in[L]\\right\\}$と書くことにします．\n",
    "* $\\mathcal{L}_h(s, a):=\\left\\{l \\in[L]: d_h^{\\rho_l, \\mathcal{M}_l}\\left(s, a ; \\xi_l\\right)>0\\right\\} \\subseteq[L]$を，$(s, a, h)$を訪問可能なデータセットのインデックスの集合とします．\n",
    "    * 最適方策が訪問する$(s, a, h)$のデータソースの最小の数の集合を$L^{\\dagger}:=\\min \\left\\{\\left|\\mathcal{L}_h(s, a)\\right|:(s, a, h)\\right.$ s.t. $\\left.d_h^{\\pi^*, \\mathcal{M}}(s, a ; \\xi)>0\\right\\}$\n",
    "        * 最適方策の情報を与えるデータソースの数を表してます．\n",
    "    * Coverageのパラメータを$C^{\\dagger}:=\\max _{(s, a, h)}\\left\\{\\sum_{l \\in \\mathcal{L}_h(s, a)} \\frac{\\min \\left\\{d_h^{\\pi^*, \\mathcal{M}^{\\prime}}(s, a ; \\xi), \\frac{1}{S}\\right\\}}{\\left|\\mathcal{L}_h(s, a)\\right| \\cdot d_h^{\\rho_l, \\mathcal{M}_l}\\left(s, a ; \\xi_l\\right)}\\right\\}$とします．$0/0=0$とします．\n",
    "        * データソースがどれくらい良い情報を与えているかを表しています\n",
    "\n",
    "この$L^{\\dagger}$と$C^{\\dagger}$ですが，それぞれ一定以上大きくなければ，$\\varepsilon$-最適な方策の学習が絶対にできないMDPが存在します．（詳しくは論文のTheorem 3.1参照）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## アルゴリズム\n",
    "\n",
    "次のアルゴリズムを考えます：\n",
    "\n",
    "1. 入力：データセット$\\mathcal{D}=\\left\\{D_l: l \\in[L]\\right\\}$\n",
    "2. $\\mathcal{D}_l^{\\prime} \\leftarrow \\operatorname{subsampling}\\left(\\mathcal{D}_l\\right), \\forall l \\in[L]$\n",
    "3. データセットを使って，報酬と遷移，そしてそれぞれの状態行動のデータソースの数を推定します：\n",
    "   1. データソースの数：$\\hat{\\mathcal{L}}_h(s, a):=\\left\\{l \\in [L]: N_{h, l}(s, a)>0\\right\\}$． そのサイズ：$\\hat{L}_h(s, a)$\n",
    "4. ペナルティとQ値を計算し，動的計画法をします．\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\Gamma_h^\\alpha(s, a) \\leftarrow c \\sqrt{\\sum_{l \\in \\hat{\\mathcal{L}}_h(s, a)} \\frac{H^2 \\log (S A H / \\delta)}{\\left(\\hat{L}_h(s, a)\\right)^2 N_{h, l}(s, a)}} \\\\\n",
    "& \\Gamma_h^\\beta(s, a) \\leftarrow c \\sqrt{H^2 \\log (S A H / \\delta) / \\hat{L}_h(s, a)} \\\\\n",
    "& \\Gamma_h(s, a) \\leftarrow \\min \\left\\{\\Gamma_h^\\alpha(s, a)+\\Gamma_h^\\beta(s, a), H\\right\\} \\\\\n",
    "& \\hat{Q}_h(s, a) \\leftarrow \\max \\left\\{\\left(\\hat{\\mathbb{B}}_h \\hat{V}_{h+1}\\right)(s, a)-\\Gamma_h(s, a), 0\\right\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**ペナルティについて**\n",
    "\n",
    "今回はサンプルについての不確かさだけでなく，データソースについての不確かさも考えなければなりません．\n",
    "\n",
    "サンプルについての不確かさを：\n",
    "$$\\Gamma_h^\\alpha(s, a)=c \\sqrt{\\frac{1}{\\left(\\hat{L}_h(s, a)\\right)^2} \\sum_{l \\in \\hat{\\mathcal{L}}_h(s, a)} \\frac{H^2 \\log (S A H / \\delta)}{N_{h, l}(s, a)}}$$\n",
    "\n",
    "データソースについての不確かさを：\n",
    "$$\n",
    "\\Gamma_h^\\beta(s, a)=c \\sqrt{\\frac{H^2 \\log (S A H / \\delta)}{\\hat{L}_h(s, a)}} .\n",
    "$$\n",
    "とします．\n",
    "そしてペナルティを\n",
    "\n",
    "$$\n",
    "\\Gamma_h(s, a)=\\min \\left\\{\\Gamma_h^\\alpha(s, a)+\\Gamma_h^\\beta(s, a), H\\right\\},\n",
    "$$\n",
    "\n",
    "として表します．これにより，高確率で\n",
    "\n",
    "$$\\left|\\left(\\hat{\\mathbb{B}}_h \\hat{V}_{h+1}\\right)(s, a)-\\left(\\mathbb{B}_h \\hat{V}_{h+1}\\right)(s, a)\\right| \\leq \\Gamma_h(s, a)$$\n",
    "\n",
    "が保証できます．\n",
    "\n",
    "---\n",
    "\n",
    "**コメント** 途中でロバストRLについての応用も話しているが，Uncertainty setのパラメータ$\\sigma$は既知っぽい．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
