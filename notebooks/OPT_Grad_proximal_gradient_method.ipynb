{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 近接勾配法の基本\n",
    "\n",
    "参考：\n",
    "* [First-Order Methods in Optimization](https://epubs.siam.org/doi/10.1137/1.9781611974997)の10章\n",
    "* [NTTによる解説](https://www.msi.co.jp/solution/nuopt/docs/glossary/articles/ProximalGradientMethod.html)\n",
    "\n",
    "\n",
    "今回は次の問題について考えます：\n",
    "\n",
    "$$\n",
    "\\min_{x \\in \\mathbb{R}^d} \\{F(x) \\triangleq f(x) + g(x)\\}\n",
    "$$\n",
    "\n",
    "ここで，\n",
    "* $g: \\mathbb{R}^d \\to (-\\infty, \\infty]$はプロパーでclosedな凸関数\n",
    "* $f: \\mathbb{R}^d \\to (-\\infty, \\infty]$はプロパーでclosedな関数\n",
    "  * $\\operatorname{dom}(f)$は凸であり，$\\operatorname{dom}(f)\\subseteq \\operatorname{int}(\\operatorname{dom}(g))$\n",
    "  * $f$は$\\operatorname{int}(\\operatorname{dom}(f))$上で **$L_f$-平滑** とします．\n",
    "\n",
    "例えば次の問題がこの問題の具体的なインスタンスになります：\n",
    "\n",
    "---\n",
    "\n",
    "**滑らか＆制約付き最適化問題**\n",
    "\n",
    "closedで凸なnonempty集合$C$について，$g=\\delta_C$とします．このとき，\n",
    "$\n",
    "\\min_{x \\in C} f(x)\n",
    "$\n",
    "は条件を満たします．（$f$についての条件は省略）\n",
    "\n",
    "---\n",
    "\n",
    "**$l_1$-正則化**\n",
    "\n",
    "$\\lambda \\geq 0$に対して$g(x) = \\lambda \\|x\\|_1$とします．\n",
    "このとき，\n",
    "$\n",
    "\\min_{x \\in \\mathbb{R}^d} f(x)\n",
    "$\n",
    "は条件を満たします．\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 近接勾配法\n",
    "\n",
    "上で見た制約付き最適化問題について考えましょう．つまり，\n",
    "$\n",
    "\\min\\{f(x) : x \\in C\\}\n",
    "$\n",
    "を考えます．\n",
    "これについて，次の射影勾配法を考えます：\n",
    "\n",
    "$$\n",
    "x^{k+1} = \\operatorname{Proj}_C (x^k - t_k \\nabla f(x^k))\n",
    "$$\n",
    "\n",
    "これはつまり，**制約なし**の空間で移動して，その後**制約付き**の空間に射影するという方法です．\n",
    "\n",
    "---\n",
    "\n",
    "**補足: projectionの変形**\n",
    "\n",
    "よく知られてますが，$x=x^{k+1}$は次の式を$x$について最小化します．\n",
    "$$\n",
    "\\frac{1}{2t_k}\\|x - (x^k - t_k \\nabla f(x^k))\\|^2_2 + \\mathrm{const}\n",
    "= \n",
    "f(x^k) + \\langle \\nabla f(x^k), x - x^k\\rangle + \\frac{1}{2t_k} \\|x - x^k\\|^2_2\n",
    "$$\n",
    "よって，\n",
    "$$\n",
    "x^{k+1} = \\operatorname{Proj}_C (x^k - t_k \\nabla f(x^k))\n",
    "\\in \\arg\\min_{x \\in C}\n",
    "f(x^k) + \\langle \\nabla f(x^k), x - x^k\\rangle + \\frac{1}{2t_k} \\|x - x^k\\|^2_2\n",
    "$$\n",
    "の関係が成り立ちます．\n",
    "すなわち，$x^{k+1}$は「$f(x^k)$を$x^k$付近で線形化して，さらに$x^k$から離れないような正則化をつけたときの最小化問題」\n",
    "で得られます．\n",
    "\n",
    "---\n",
    "\n",
    "さて，これは$C$についての制約付き最適化$\\min_{x\\in C}$を考えてます．\n",
    "これは結局ディラックの$\\delta$関数を使って表せるので，より一般的な形に対するアプローチを考えましょう．次の式が自然に考えられます：\n",
    "$$\n",
    "x^{k+1} \\in \\arg\\min_{x \\in \\mathbb{R}^d} f(x^k) + \\langle \\nabla f(x^k), x - x^k\\rangle + g(x) + \\frac{1}{2t_k} \\|x - x^k\\|^2_2\n",
    "$$\n",
    "これは変形すれば次と等価です：\n",
    "$$\n",
    "x^{k+1} \\in \\arg\\min_{x \\in  \\mathbb{R}^d} t_k g(x) + \\frac{1}{2} \\|x - (x^k - t_k \\nabla f(x^k))\\|^2_2\n",
    "$$\n",
    "\n",
    "このようなminimizationによる更新を一般に**近接射影**と呼びます．具体的には次で定義されます：\n",
    "\n",
    "---\n",
    "\n",
    "**定義：近接射影**\n",
    "\n",
    "関数$f: \\mathbb{R}^d \\to (-\\infty, \\infty]$について，$f$の近接射影は\n",
    "$$\n",
    "\\operatorname{prox}_f(x) = \\arg\\min_{u \\in \\mathbb{R}^d} \\{f(u) + \\frac{1}{2} \\|u - x\\|^2\\}\n",
    "$$\n",
    "を実現する作用素のことです．\n",
    "これは[モーロー包](CVX_weakly_convex_and_Moreau_envelope.ipynb)の定義とほぼ同じです．\n",
    "$f$が支配的な項であれば$f$を最小にするような$u$を返しますが，$f$の影響が小さければ，$u\\approx x$を返します．\n",
    "\n",
    "---\n",
    "\n",
    "これを使うと，$x^{k+1}$は\n",
    "$$\n",
    "x^{k+1} = \\operatorname{prox}_{t_k g} (x^k - t_k \\nabla f(x^k))\n",
    "$$\n",
    "\n",
    "と等価です．$g$がδ関数などであれば，これは射影勾配と同じになります．\n",
    "実際，次の更新方法を一般化してます：\n",
    "\n",
    "\n",
    "|        問題      |       更新方法 |                     名前                    |\n",
    "|:---------------------:|:---------------------:|:--------------------------------------------:|\n",
    "| $\\min_{\\mathbf{x \\in \\mathbb{E}}} f(\\mathbf{x})$ | $\\mathbf{x}^{k+1} = \\mathbf{x}^{k} - t_k \\nabla f(\\mathbf{x}^k)$ | 勾配法|\n",
    "| $\\min_{\\mathbf{x \\in C}} f(\\mathbf{x})$ | $\\mathbf{x}^{k+1} = P_C (\\mathbf{x}^{k} - t_k \\nabla f(\\mathbf{x}^k))$ | 射影勾配法|\n",
    "| $\\min_{\\mathbf{x \\in \\mathbb{E}}} f(\\mathbf{x}) + \\lambda \\|\\mathbf{x}\\|_1$ | TODO | Iterative shrinkage-thresholding algorithm (ISTA) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 近接勾配法の収束証明\n",
    "\n",
    "以降，特に定数$L_k$を使って$t_k = \\frac{1}{L_k}$として更新することにします．\n",
    "また，簡単のために次の表記も導入します：\n",
    "* $T^{f, g}_L(x) \\triangleq \\operatorname{prox}_{\\frac{1}{L} g} (x - \\frac{1}{L} \\nabla f(x))$：つまり，$t_k$を$L$で置き換えただけです．\n",
    "以降，簡単のために$T_L$とおきます．\n",
    "* $F = f + g$：これはデルタ関数みたいなものまで含めた目的関数です．\n",
    "* $G^{f, g}_L(\\mathbf{x})=L(\\mathbf{x} - T_L(\\mathbf{x}))$：これは「Gradient mapping」と呼ばれる量です（Definition 10.5）．明らかな場合は$f, g$を省略します．これは次が成立するので，Projectionを抜かした勾配を表してます：\n",
    "  * $\\mathbf{x}^{k+1} = \\mathbf{x} - \\frac{1}{L_k}G_{L_k}(\\mathbf{x}^k)$\n",
    "\n",
    "\n",
    "まず，次が成立します．\n",
    "\n",
    "---\n",
    "\n",
    "**sufficient decrease lemma**\n",
    "\n",
    "$L \\in (\\frac{L_f}{2}, \\infty)$のとき，\n",
    "\n",
    "$$\n",
    "F(x) - F(T_L(x)) \\geq \\frac{(L-\\frac{L_f}{2})}{L^2}\\|G_L^{f, g}(\\mathbf{x})\\|^2_2\n",
    "$$\n",
    "特に$L=L_f$なら，\n",
    "$$\n",
    "F(x) - F(T_L(x)) \\geq \\frac{1}{2L_f}\\|G_L^{f, g}(\\mathbf{x})\\|^2_2\n",
    "$$\n",
    "つまり，一回更新すると，一定以上の値が減ります．\n",
    "\n",
    "**証明**\n",
    "\n",
    "簡単のために$x^+ = T_L(x)$とおきます．\n",
    "$f$は$L_f$-Smoothなので，滑らかな関数のdescent lemma（Beck本Lemma 5.7）を使うと，\n",
    "\n",
    "$$\n",
    "f(x^+) \\leq f(x) + \\langle \\nabla f(x) , x^+ - x\\rangle  + \\frac{L_f}{2} \\|x - T_L^{f, g}(x)\\|^2_2\n",
    "$$\n",
    "が成立します（凸関数は$f(y) \\geq f(x) + \\langle g, y - x\\rangle + \\frac{\\sigma}{2}\\|y - x\\|^2$みたいに下から抑えるのが定義です．Smoothだと上から抑えることができます）．\n",
    "\n",
    "ここで，次のSecond prox theorem （Theorem 6.39）を使います．\n",
    "\n",
    "---\n",
    "\n",
    "**Second prox theorem**\n",
    "\n",
    "$f: \\mathbb{E}\\to(-\\infty, \\infty]$をproperでclosedな凸関数とします．このとき，任意の$x, u \\in \\mathbb{E}$について，次の３つが等価です：\n",
    "\n",
    "1. $u=\\operatorname{prox}_f(x)$\n",
    "2. $x-u \\in \\partial f(u)$\n",
    "3. $\\langle x - u, y-u\\rangle \\leq f(y) - f(u)$ が任意の$y\\in \\mathbb{E}$で成立\n",
    "\n",
    "**証明**\n",
    "\n",
    "$u=\\operatorname{prox}_f(x)$は定義より，iffで$u$が$\\min_v\\left\\{f(v) + \\frac{1}{2} \\|v - x\\|^2\\right\\}$を満たすときだけ．\n",
    "よって，それが起こるのはiffで$u$はその劣勾配を$0$にするときだけなので（Fermat's optimality condition Theorem 3.63），\n",
    "\n",
    "$$\n",
    "0 \\in \\partial f(u) + u - x\n",
    "$$\n",
    "が成り立ちます．\n",
    "後は並び替えて終わりです．\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "よって，$\\mathbf{x}^+= \\operatorname{prox}_{g / L} (\\mathbf{x} - \\nabla f(\\mathbf{x}) / L)$なので，\n",
    "\n",
    "$$\n",
    "\\left\\langle \\mathbf{x} - \\frac{1}{L} \\nabla f(\\mathbf{x}) - \\mathbf{x}^+, \\mathbf{x} - \\mathbf{x}^+ \\right\\rangle\n",
    "\\leq \\frac{1}{L} g(\\mathbf{x}) - \\frac{1}{L} g(\\mathbf{x}^+)\n",
    "$$\n",
    "が成立します．よって，\n",
    "$$\n",
    "\\langle \\nabla f(\\mathbf{x}), \\mathbf{x}^+ - \\mathbf{x}\\rangle\n",
    "\\leq -L \\| \\mathbf{x}^+ - \\mathbf{x}\\|^2 + g(\\mathbf{x}) - g(\\mathbf{x}^+)\n",
    "$$\n",
    "が成り立ちます．最後にSmoothによるDescent lemmaと合体して，\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}^+) + g(\\mathbf{x}^+)\n",
    "\\leq f(\\mathbf{x}) + g(\\mathbf{x}) + (-L + \\frac{L_f}{2})\\|\\mathbf{x}^+ - \\mathbf{x}\\|^2_2\n",
    "$$\n",
    "が成立します．\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Gradient Mappingの性質\n",
    "\n",
    "---\n",
    "\n",
    "**Theorem 10.7**\n",
    "\n",
    "$G_L$は勾配の一般化みたいな量になっています．実際，次が成立します．\n",
    "* $g_0(x)=0$なら，$G^{f, g_0}_L(x)=\\nabla f(x)$ が任意の$x \\in \\operatorname{int}(\\operatorname{dom}(f))$で成立する．つまり$G_L$が勾配と同じ\n",
    "* $G^{f, g}_L(x^*)=0$ iff $x^*$がstationary point\n",
    "\n",
    "**証明**\n",
    "\n",
    "$\\operatorname{prox}_{1/L g_0}(y)=y$が任意の$y\\in \\mathbb{E}$について成り立ってるので，一個目は自明\n",
    "\n",
    "二個目の証明：$G_L^{f, g_0}(x^*)=0$が成り立つのは iff で$x^*=\\operatorname{prox}_{1/L g}(x^* - \\frac{1}{L}\\nabla f(x^*))$のときだけ．後はSecond prox theoremでproxを変形すれば出てきます．\n",
    "\n",
    "---\n",
    "\n",
    "特に$f$が凸ならば，Stationary pointは最適解であることの必要十分条件なので，$G_L=0$はもとの問題が最適解であることの必要十分条件になります．\n",
    "\n",
    "よって，$\\|G_L(x)\\|$を**最適性を測る量**として捉えることができます．\n",
    "\n",
    "* **コメント**：$G$は**モーロー包の停留点を評価する指標**として捉えるとわかりやすいです．[Efficiency of minimizing compositions of convex functions and smooth maps](https://arxiv.org/abs/1605.00125)を読んでおこう．\n",
    "\n",
    "$G_L$は次のように$L$，つまり学習率について単調な性質を持ちます：\n",
    "\n",
    "---\n",
    "\n",
    "**Theorem 10.9（学習率についての単調性）**\n",
    "\n",
    "もし$L_1 \\geq L_2 > 0$ならば，次の２つが成立\n",
    "\n",
    "* $\\|G_{L_1}(x) \\| \\geq \\|G_{L_2}(x)\\|$\n",
    "* $\\frac{\\|G_{L_1}(x)\\|}{L_1}  \\leq \\frac{\\|G_{L_2}(x)\\|}{L_2}$\n",
    "\n",
    "証明はめんどいので省略\n",
    "\n",
    "---\n",
    "\n",
    "TODO: 続き書く"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 近接線形法（近接勾配法の一般化）\n",
    "\n",
    "参考：\n",
    "* [Efficiency of minimizing compositions of convex functions and smooth maps](https://arxiv.org/abs/1605.00125)：だいぶわかりやすい．おすすめ．\n",
    "\n",
    "上より一般的な関数として，次のcompositeな関数について考えましょう：\n",
    "\n",
    "$$\n",
    "\\min _x F(x):=g(x)+h(c(x))\n",
    "$$\n",
    "\n",
    "ここで，\n",
    "* $g: \\mathbf{R}^d \\rightarrow \\mathbf{R} \\cup\\{\\infty\\}$はプロパーな閉凸関数\n",
    "* $h: \\mathbf{R}^m \\rightarrow \\mathbf{R}$ は凸かつ$L$-Lipschitz連続．\n",
    "    * つまり，$|h(x)-h(y)| \\leq L\\|x-y\\| \\quad$ for all $x, y \\in \\mathbf{R}^m$\n",
    "* $c: \\mathbf{R}^d \\rightarrow \\mathbf{R}^m$は$\\beta$平滑な関数\n",
    "    * つまり，$\\|\\nabla c(x)-\\nabla c(y)\\|_{\\mathrm{op}} \\leq \\beta\\|x-y\\| \\quad$ for all $x, y \\in \\mathbf{R}^d$．\n",
    "\n",
    "とします．\n",
    "\n",
    "## Motivating Examples\n",
    "\n",
    "### Additive Composition\n",
    "\n",
    "上でやったのは$h$がIdentityかつ$c$が実数を返すときです．\n",
    "$$\n",
    "\\min _x c(x)+g(x)\n",
    "$$\n",
    "を考えます．これは統計的学習や画像処理などでめっちゃでてきます．特に$c$が凸関数の場合についていろんな結果が存在してます．\n",
    "\n",
    "### Non-linear least squares\n",
    "\n",
    "次のように，制約付きの非線形最小二乗法はComposite問題に含まれます．\n",
    "$$\n",
    "\\min _x\\|c(x)\\| \\quad \\text { subject to } \\quad l_i \\leq x_i \\leq u_i \\quad \\text { for } i=1, \\ldots, m .\n",
    "$$\n",
    "ガウス-ニュートン法はこの問題を解くための有名な方法です．\n",
    "\n",
    "### Exact penalty法\n",
    "\n",
    "$\\min _x\\{f(x): G(x) \\in \\mathcal{K}\\}$なる問題を考えましょう．\n",
    "ここで，$f: \\mathbf{R}^d \\rightarrow \\mathbf{R}$ と $G: \\mathbf{R}^d \\rightarrow \\mathbf{R}^m$ は平滑な関数で，$\\mathcal{K} \\subseteq \\mathbf{R}^m$は閉凸錘です．\n",
    "\n",
    "Exact penalty法は，次の変形を考えます．\n",
    "\n",
    "$$\n",
    "\\min _x f(x)+\\lambda \\cdot \\theta_{\\mathcal{K}}(G(x))\n",
    "$$\n",
    "\n",
    "ここで，$\\theta_{\\mathcal{K}}: \\mathbf{R}^m \\rightarrow \\mathbf{R}$は非負の凸関数で，$\\theta_{\\mathcal{K}}(x)=0$ならば$x \\in \\mathcal{K}$です．\n",
    "$\\lambda > 0$はペナルティパラメータです．\n",
    "\n",
    "Composite問題において，\n",
    "* $c(x)=(f(x), G(x))$\n",
    "* $h(f, G)=f+\\lambda \\theta_{\\mathcal{K}}(G)$\n",
    "\n",
    "のケースに相当します．\n",
    "\n",
    "### 統計的な推定問題\n",
    "\n",
    "何らかの非線形な過程のモデル$G(x)$と，その観測データ$b$について，誤差指標$h$を最小化したい場合がよくあります．この問題は\n",
    "$$\n",
    "\\min _x h(b-G(x))+g(x)\n",
    "$$\n",
    "として表現できます．\n",
    "\n",
    "* $g$は$x$に関する事前知識などを表す正則化項で，例えば$l_1$や$l_2$正則化などが考えられます．\n",
    "* $h=\\|\\cdot\\|_2$ならば二乗誤差最小化問題になります．\n",
    "* $h=\\|\\cdot\\|_1$ならばLeast Absolute Deviation と呼ばれる問題になります．\n",
    "* $h_\\kappa(\\tau)= \\begin{cases}\\frac{1}{2 \\kappa} \\tau^2 & , \\tau \\in[-\\kappa, \\kappa] \\\\ |\\tau|-\\frac{\\kappa}{2} & , \\text { otherwise }\\end{cases}$ならhuber-lossの最小化ですね．\n",
    "\n",
    "\n",
    "### Grey-box最適化\n",
    "\n",
    "産業の応用では，$c(x)$はシミュレーションモデルで，$h$はその出力に関するコスト関数である場合を考えることがあります．\n",
    "ここで，$h$は既知ですが，$c(x)$と$\\nabla c(x)$はシミュレータの出力であり，解析的な表現が未知です．\n",
    "\n",
    "[Introduction to Derivative-Free Optimization](https://epubs.siam.org/doi/book/10.1137/1.9780898718768)などを読むといいらしい．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 近接線形法\n",
    "\n",
    "**表記**\n",
    "\n",
    "* モーロー包：$f_\\nu(x) :=\\inf _z\\left\\{f(z)+\\frac{1}{2 \\nu}\\|z-x\\|^2\\right\\}$\n",
    "* 近接射影：$\\operatorname{prox}_{\\nu f}(x) :=\\underset{z}{\\operatorname{argmin}}\\left\\{f(z)+\\frac{1}{2 \\nu}\\|z-x\\|^2\\right\\},$\n",
    "* $F: \\mathbf{R}^d \\rightarrow \\mathbf{R}^m$のリプシッツ連続性：$\\operatorname{lip}(F):=\\sup _{x \\neq y} \\frac{\\|F(y)-F(x)\\|}{\\|y-x\\|}$\n",
    "* $\\mu = L\\beta$\n",
    "* $A^*$は次を満たす随伴行列：$\\langle A x, y\\rangle=\\left\\langle x, A^* y\\right\\rangle \\quad$ for all $x \\in \\mathbf{R}^d, y \\in \\mathbf{R}^l$\n",
    "\n",
    "モーロー包に成立する性質は論文のLemma 2.1などを参照すると良いです．\n",
    "\n",
    "---\n",
    "\n",
    "**目標**\n",
    "\n",
    "さて，composition問題は非凸なので，自然な目標は１次の停留点を見つけることです． \n",
    "\n",
    "Frechet劣微分を使って停留点を定義しましょう．\n",
    "任意の関数$f: \\mathbf{R}^d \\rightarrow \\bar{\\mathbf{R}}$について，Frechet劣微分$\\hat{\\partial} f(\\bar{x})$は次を満たすベクトル$v$の集合です：\n",
    "$$\n",
    "f(x) \\geq f(\\bar{x})+\\langle v, x-\\bar{x}\\rangle+o(\\|x-\\bar{x}\\|) \\quad \\text { as } x \\rightarrow \\bar{x} .\n",
    "$$\n",
    "一般にFrechet劣微分はlimiting劣微分$\\partial f(x)$とは異なります（limiting 劣微分は調べてね）．\n",
    "以下では，$0 \\in \\partial f(x)$のときに，$x$はstationary pointであるとします．\n",
    "\n",
    "一般には異なりますが，この章で考える$F$についてはlimitingとFrechet劣微分は一致し，次のChain ruleが言えます：\n",
    "\n",
    "$$\n",
    "\\partial F(x)=\\partial g(x)+\\nabla c(x)^* \\partial h(c(x)) .\n",
    "$$\n",
    "\n",
    "まとめると，今回の目標は停留点$0 \\in \\partial F(x)$を見つけることです．これは任意の方向微分が非負である状況に相当します：\n",
    "$$\n",
    "\\operatorname{dist}(0 ; \\partial F(x))=-\\inf _{v:\\|v\\| \\leq 1} F^{\\prime}(x ; v)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "近接線形法は次の更新を繰り返します：\n",
    "\n",
    "$$\n",
    "x_{k+1}=\\underset{x}{\\operatorname{argmin}}\\left\\{g(x)+h\\left(c\\left(x_k\\right)+\\nabla c\\left(x_k\\right)\\left(x-x_k\\right)\\right)+\\frac{1}{2 t}\\left\\|x-x_k\\right\\|^2\\right\\}\n",
    "$$\n",
    "\n",
    "つまり，平滑な$c$の部分で勾配法を適用して，$h$の部分は無視してます．\n",
    "\n",
    "上でやったように，$\\mathcal{G}_t\\left(x_k\\right):=t^{-1}\\left(x_k-x_{k+1}\\right)$が停留点までの近さを表現しています．この$\\mathcal{G}_t$を**prox-gradient**と呼びましょう（いろんな呼び方がある）．\n",
    "\n",
    "後で見ますが，$t=(L\\beta)^{-1}$のとき，近接線形法は$\\left\\|\\mathcal{G}_{\\frac{1}{L \\beta}}(x)\\right\\| \\leq \\varepsilon$なる点に収束します．\n",
    "これを見ていきましょう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
