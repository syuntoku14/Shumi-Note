{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 双対法による制約付きMDPの解法\n",
    "\n",
    "参考\n",
    "\n",
    "* [Exploration-Exploitation in Constrained MDPs](https://arxiv.org/abs/2003.02189)\n",
    "* [CONSTRAINED MARKOV DECISION PROCESSES](https://www-sop.inria.fr/members/Eitan.Altman/TEMP/h.pdf)\n",
    "\n",
    "[前回](RL_CMDP_explore_exploit.ipynb)は線型計画法によってCMDPを解く方法を見ました．\n",
    "ラグランジュの未定乗数法を使うと，動的計画法で解くこともできます．\n",
    "\n",
    "\n",
    "表記\n",
    "\n",
    "* 有限MDP: $\\mathcal{M}=\\left(\\mathcal{S}, \\mathcal{A}, c, p, s_1, H\\right)$\n",
    "    1. 有限状態集合: $S=\\{1, \\dots, |S|\\}$\n",
    "    2. 有限行動集合: $A=\\{1, \\dots, |A|\\}$\n",
    "    3. 非定常なコスト関数（元論文では確率変数ですが、ややこしいので決定的にします）: $c_h(s, a)$\n",
    "    4. 非定常遷移確率: $p_h(s'|s, a)$\n",
    "    5. 初期状態: $s_1$\n",
    "    6. ホライゾン: $H$\n",
    "    7. すべての状態行動の中で最大の次状態への遷移の数: $\\mathcal{N}:=\\max _{s, a, h}\\left|\\left\\{s^{\\prime}: p_h\\left(s^{\\prime} \\mid s, a\\right)>0\\right\\}\\right|$\n",
    "* 占有率：$q_h^\\pi(s, a ; p):=\\mathbb{E}\\left[\\mathbb{1}\\left\\{s_h=s, a_h=a\\right\\} \\mid s_1=s_1, p, \\pi\\right]=\\operatorname{Pr}\\left\\{s_h=s, a_h=a \\mid s_1=s_1, p, \\pi\\right\\}$\n",
    "* 価値関数：$V_1^\\pi\\left(s_1 ; p, c\\right)=\\sum_{h, s, a} q_h^\\pi(s, a ; p) c_h(s, a):=c^T q^\\pi(p)$\n",
    "* 非定常方策の集合：$\\pi=\\left(\\pi_1, \\pi_2, \\ldots, \\pi_H\\right) \\in \\Pi^{\\mathrm{MR}}$\n",
    "    * $(1 - \\alpha)\\pi_1 + \\alpha \\pi_2 \\in \\Pi^{\\mathrm{MR}}$なので，これは凸集合\n",
    "    * 明記しない限り，$\\min_\\pi$などは$\\Pi^{\\mathrm{MR}}$について取るとします"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 双対問題の導出\n",
    "\n",
    "\n",
    "* $\\{d_i, \\alpha_i\\}_{i=1}^I$：$I$個の制約\n",
    "    * $d_i \\in \\mathbb{R}^{SAH}$\n",
    "    * $\\alpha_i \\in [0, H]$\n",
    "    * $i$番目の制約（元論文では確率変数ですが、ややこしいので決定的にします）$d_{i, h}(s, a)$\n",
    "    * $V_h^\\pi\\left(s ; p, d_i\\right):=\\mathbb{E}\\left[\\sum_{h^{\\prime}=h}^H d_{i, h^{\\prime}}\\left(s_{h^{\\prime}}, a_{h^{\\prime}}\\right) \\mid s_h=s, p, \\pi\\right]$.\n",
    "\n",
    "CMDPについて振り返ってみましょう．CMDPの目的は次の最適方策（主問題）の導出です．\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\pi^{\\star} \\in \\underset{\\pi \\in \\Pi^{\\mathrm{MR}}}{\\arg \\min } c^T q^\\pi(p) \\\\\n",
    "\\text { s.t. } D q^\\pi(p) \\leq \\alpha\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "ここで、\n",
    "$$\n",
    "D=\\left[\\begin{array}{c}\n",
    "d_1^T \\\\\n",
    "\\vdots \\\\\n",
    "d_I^T\n",
    "\\end{array}\\right], \\quad \\alpha=\\left[\\begin{array}{c}\n",
    "\\alpha_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\alpha_I\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "としました．\n",
    "\n",
    "この双対問題を導出してみましょう．\n",
    "まず，$\\delta_C(x)$は$x\\in C$なら0であり、それ以外では$\\infty$を取る関数とします．\n",
    "制約条件より，$\\alpha-Dq^\\pi(p) \\geq 0$であるべきです．\n",
    "$f(\\pi)=c^T q^\\pi(p)$, \n",
    "$g(\\pi)=\\delta_{\\mathbb{R}_+}(\\alpha-Dq^\\pi(p))$とすれば，主問題は次と等価です．\n",
    "\n",
    "$$\n",
    "\\min_\\pi f(\\pi) + g(\\pi)\\\\\n",
    "$$\n",
    "\n",
    "一方で，上のような強いバリア関数ではなく，次のようにラグランジュ未定乗数$\\lambda \\in \\mathbb{R}_{+}^I$を使ったラグランジアンを考えてみます．\n",
    "\n",
    "$$\n",
    "L(\\pi, \\lambda)=c^T q^\\pi(p)+\\lambda^T\\left(D q^\\pi(p)-\\alpha\\right)\n",
    "$$\n",
    "\n",
    "このとき，主問題の双対問題は次の形式で与えられます.\n",
    "\n",
    "$$\n",
    "L^* = \\min_\\lambda \\max_\\pi L(\\pi, \\lambda)\n",
    "$$\n",
    "\n",
    "このCMDPには強双対性が成り立つので，このミニマックスゲームは$L^*=V_1^*(s_1)$を満たします．\n",
    "これを解くことを考えます．\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primal-Dual法による解法\n",
    "\n",
    "まず固定された$\\lambda$について，次の問題を解きます．\n",
    "\n",
    "$$\n",
    "\\pi_k \\in \\underset{\\pi \\in \\Pi^{\\mathrm{MR}}}{\\arg \\min }\\left({c}+{D}^T \\lambda_k\\right)^{\\top} q^\\pi\\left(p\\right)-\\lambda_k^T \\alpha\n",
    "$$\n",
    "\n",
    "これはMDP $\\mathcal{M}_k = \\left\\{M=(S, A, r^+, p): r_h^{+}(s, a)={c}_h(s, a)+\\sum_id_{i, h}(s, a) \\lambda_i^k\\right\\}$の最適方策を求めれば良いので，次の価値反復法で解けます：\n",
    "\n",
    "$$\n",
    "Q_h^k(s, a)=r_h^{+}(s, a)+ \\sum_{s^{\\prime}} p\\left(s^{\\prime} \\mid s, a\\right) \\min _{a^{\\prime}} Q_{h+1}^k\\left(s^{\\prime}, a^{\\prime}\\right)\n",
    "$$\n",
    "\n",
    "続いてラグランジュ未定乗数を更新します．\n",
    "\n",
    "$$\n",
    "\\lambda_{k+1}=\\left[\\lambda_k+\\eta\\left({D} q^{\\pi_k}\\left({p}\\right)-\\alpha\\right)\\right]\n",
    "$$\n",
    "\n",
    "ここで出てくる占有率$q^{\\pi_k}$も簡単な反復法で計算できます．\n",
    "\n",
    "この２つを繰り返すと，CMDPの最適方策をよく近似できます．"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 線形計画法による解法\n",
    "\n",
    "参考\n",
    "* [A Primal-Dual Approach to Constrained Markov Decision Processes with Applications to Queue Scheduling and Inventory Management](http://www.columbia.edu/~jd2736/publication/CMDP.pdf)\n",
    "\n",
    "$\\pi_k$は報酬関数をいじったMDPの最適方策でした．つまり，固定された$\\lambda$に対して，その最適価値関数は次のベルマン方程式を満たします．\n",
    "\n",
    "$$\n",
    "V_h(s)=\\min _{a \\in \\mathcal{A}}\\cdot\\left(c(s, a)+\\sum_{i=1}^I \\lambda_i \\cdot d_i(s, a)\\right)+\\sum_{s^{\\prime} \\in \\mathcal{S}} V_{h+1}\\left(s^{\\prime}\\right) p_{h}\\left(s^{\\prime} \\mid s, a\\right), \\forall s \\in \\mathcal{S}\n",
    "$$\n",
    "\n",
    "よって，次の線形計画法を解いてもCMDPは解けます．\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\max_{V, \\lambda} \\sum_s V_0(s) - \\sum^I_{i=1}\\lambda_i \\alpha_i &&\\\\\n",
    "\\text { s.t. } \n",
    "&V_h(s) \\leq \\left(c(s, a) + \\sum^I_{i=1} \\lambda_i d_i(s, a) \\right) + \\sum_{s'\\in \\mathcal{S}} p_{h}\\left(s' \\mid s, a\\right) V_{h+1}(s^{\\prime}) & & \\forall (s, a) \\in \\mathcal{S}\\times \\mathcal{A} \\\\\n",
    "&v_{H+1}(s)=0 \\quad & & \\forall s\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "TODO: これは前回と違って単純に$\\sum_{h, s}$とするとラグランジュによる項との重み付けが狂う可能性がある？実験したら狂った．理論も要検証．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状態数： 7\n",
      "行動数： 3\n",
      "ホライゾン： 5\n",
      "制約： 1.5\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from typing import NamedTuple, Optional\n",
    "from jax.random import PRNGKey\n",
    "\n",
    "key = PRNGKey(0)\n",
    "\n",
    "S = 7  # 状態集合のサイズ\n",
    "A = 3  # 行動集合のサイズ\n",
    "S_set = jnp.arange(S)  # 状態集合\n",
    "A_set = jnp.arange(A)  # 行動集合\n",
    "H = 5  # ホライゾン\n",
    "\n",
    "# 報酬行列を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "rew = jax.random.uniform(key=key, shape=(H, S, A))\n",
    "assert rew.shape == (H, S, A)\n",
    "\n",
    "\n",
    "# コスト行列を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "cost = jax.random.uniform(key=key, shape=(H, S, A))\n",
    "assert cost.shape == (H, S, A)\n",
    "\n",
    "\n",
    "# 遷移確率行列を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "P = jax.random.uniform(key=key, shape=(H, S*A, S))\n",
    "P = P / jnp.sum(P, axis=-1, keepdims=True)  # 正規化して確率にします\n",
    "P = P.reshape(H, S, A, S)\n",
    "np.testing.assert_allclose(P.sum(axis=-1), 1, atol=1e-6)  # ちゃんと確率行列になっているか確認します\n",
    "\n",
    "\n",
    "# 初期状態分布を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "init_dist = jax.random.uniform(key, shape=(S,))\n",
    "init_dist = init_dist / jnp.sum(init_dist)\n",
    "np.testing.assert_allclose(init_dist.sum(axis=-1), 1, atol=1e-6)  # ちゃんと確率行列になっているか確認します\n",
    "\n",
    "\n",
    "# 状態集合, 行動集合, 割引率, 報酬行列, 遷移確率行列が準備できたのでMDPのクラスを作ります\n",
    "\n",
    "class CMDP(NamedTuple):\n",
    "    S_set: jnp.array  # 状態集合\n",
    "    A_set: jnp.array  # 行動集合\n",
    "    H: int  # ホライゾン\n",
    "    rew: jnp.array  # 報酬行列\n",
    "    cost: jnp.array  # 報酬行列\n",
    "    const: float  # 制約の閾値\n",
    "    P: jnp.array  # 遷移確率行列\n",
    "    init_dist: jnp.array  # 初期分布\n",
    "    optimal_V_rew: Optional[jnp.ndarray] = None  # 報酬についての最適V値\n",
    "    optimal_V_cost: Optional[jnp.ndarray] = None  # コストについての最適V値\n",
    "\n",
    "    @property\n",
    "    def S(self) -> int:  # 状態空間のサイズ\n",
    "        return len(self.S_set)\n",
    "\n",
    "    @property\n",
    "    def A(self) -> int:  # 行動空間のサイズ\n",
    "        return len(self.A_set)\n",
    "\n",
    "\n",
    "const = 0.3 * H  # 制約は適当です。このときに実行可能である保証はとくにありません。\n",
    "mdp = CMDP(S_set, A_set, H, rew, cost, const, P, init_dist)\n",
    "\n",
    "print(\"状態数：\", mdp.S)\n",
    "print(\"行動数：\", mdp.A)\n",
    "print(\"ホライゾン：\", mdp.H)\n",
    "print(\"制約：\", mdp.const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import chex\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_Q(mdp: CMDP, policy: jnp.ndarray):\n",
    "    \"\"\"ベルマン期待作用素をホライゾン回走らせて価値関数を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (CMDP)\n",
    "        policy (np.ndarray): (HxSxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        policy_Q_rew (jnp.ndarray): (HxSxA)の行列. 報酬関数についてのQ\n",
    "        policy_Q_cost (jnp.ndarray): (HxSxA)の行列. コスト関数についてのQ\n",
    "    \"\"\"\n",
    "    H, S, A = policy.shape\n",
    "\n",
    "    def backup(i, args):\n",
    "        policy_Q, g = args\n",
    "        h = H - i - 1\n",
    "        max_Q = (policy[h+1] * policy_Q[h+1]).sum(axis=1)\n",
    "        next_v = mdp.P[h] @ max_Q\n",
    "        chex.assert_shape(next_v, (S, A))\n",
    "        policy_Q = policy_Q.at[h].set(g[h] + next_v)\n",
    "        return policy_Q, g\n",
    "    \n",
    "    policy_Q_rew = jnp.zeros((H+1, S, A))\n",
    "    args = policy_Q_rew, mdp.rew\n",
    "    policy_Q_rew, _ = jax.lax.fori_loop(0, mdp.H, backup, args)\n",
    "\n",
    "    policy_Q_cost = jnp.zeros((H+1, S, A))\n",
    "    args = policy_Q_cost, mdp.cost\n",
    "    policy_Q_cost, _ = jax.lax.fori_loop(0, mdp.H, backup, args)\n",
    "    return policy_Q_rew[:-1], policy_Q_cost[:-1]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず線型計画法で解いてみます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最適方策の累積コスト和 1.5\n",
      "最適方策の累積報酬和 3.2056034\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import linprog\n",
    "from itertools import product\n",
    "B = np.zeros((H, S, A, H, S, A))\n",
    "\n",
    "# 初期状態についての制約\n",
    "for s, a in product(range(S), range(A)):\n",
    "    B[0, s, a, 0, s] = 1\n",
    "\n",
    "\n",
    "# 遷移についての制約\n",
    "for h, s, a in product(range(1, H), range(S), range(A)):\n",
    "    B[h, s, a, h, s] = 1  # sum_a d(h, s, a) を実現します\n",
    "    B[h, s, a, h-1] = -mdp.P[h-1, :, :, s]  # sum_a d(h, s, a) を実現します\n",
    "\n",
    "\n",
    "B = B.reshape((H*S*A, H*S*A))\n",
    "mu = np.repeat(mdp.init_dist[:, None], A, axis=1).reshape(-1)\n",
    "b = np.hstack((mu, np.zeros((H-1)*S*A)))\n",
    "\n",
    "# コストについての制約\n",
    "C = mdp.cost.reshape(1, -1)\n",
    "c = np.array((mdp.const,))\n",
    "\n",
    "r = - mdp.rew.reshape(-1)\n",
    "lin_res = linprog(r, A_eq=B, b_eq=b, bounds=(0, None), A_ub=C, b_ub=c)\n",
    "\n",
    "d_arr = lin_res.x.reshape(H, S, A)\n",
    "np.testing.assert_allclose(d_arr.sum(axis=(1, 2)), 1.0, atol=1e-4)\n",
    "\n",
    "# 行動の確率が全て０の箇所はUniformにします（この状態には訪れないことを意味しますが、NanがでちゃうのでUniformで回避します）\n",
    "optimal_policy = d_arr / d_arr.sum(axis=-1, keepdims=True)\n",
    "optimal_policy = jnp.where(jnp.isnan(optimal_policy), 1 / mdp.A, optimal_policy)\n",
    "Q_rew, Q_cost = compute_policy_Q(mdp, optimal_policy)\n",
    "V_rew, V_cost = (Q_rew * optimal_policy).sum(axis=-1), (Q_cost * optimal_policy).sum(axis=-1)\n",
    "\n",
    "total_cost = V_cost[0] @ mdp.init_dist\n",
    "assert total_cost <= mdp.const\n",
    "print(\"最適方策の累積コスト和\", total_cost)\n",
    "\n",
    "total_rew = V_rew[0] @ mdp.init_dist\n",
    "print(\"最適方策の累積報酬和\", total_rew)\n",
    "\n",
    "mdp = mdp._replace(optimal_V_rew=V_rew, optimal_V_cost=V_cost)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続いてPrimal-Dual法によって双対問題を解きます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import jax\n",
    "import chex\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_greedy_policy(Q: jnp.ndarray):\n",
    "    \"\"\"Q関数の貪欲方策を返します\n",
    "\n",
    "    Args:\n",
    "        Q (jnp.ndarray): (HxSxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        greedy_policy (jnp.ndarray): (HxSxA)の行列\n",
    "    \"\"\"\n",
    "    greedy_policy = jnp.zeros_like(Q)\n",
    "    H, S, A = Q.shape\n",
    "    \n",
    "    def body_fn(i, greedy_policy):\n",
    "        greedy_policy = greedy_policy.at[i, jnp.arange(S), Q[i].argmax(axis=-1)].set(1)\n",
    "        return greedy_policy\n",
    "\n",
    "    greedy_policy = jax.lax.fori_loop(0, H, body_fn, greedy_policy)\n",
    "    chex.assert_shape(greedy_policy, (H, S, A))\n",
    "    return greedy_policy\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"H\", \"S\", \"A\"))\n",
    "def _compute_optimal_Q(mdp: CMDP, H: int, S: int, A: int):\n",
    "    \"\"\"ベルマン最適作用素をホライゾン回走らせて最適価値関数を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (CMDP)\n",
    "\n",
    "    Returns:\n",
    "        optimal_Q (jnp.ndarray): (HxSxA)の行列\n",
    "    \"\"\"\n",
    "\n",
    "    def backup(i, optimal_Q):\n",
    "        h = H - i - 1\n",
    "        max_Q = optimal_Q[h+1].max(axis=1)\n",
    "        next_v = mdp.P[h] @ max_Q\n",
    "        chex.assert_shape(next_v, (S, A))\n",
    "        optimal_Q = optimal_Q.at[h].set(mdp.rew[h] + next_v)\n",
    "        return optimal_Q\n",
    "    \n",
    "    optimal_Q = jnp.zeros((H+1, S, A))\n",
    "    optimal_Q = jax.lax.fori_loop(0, mdp.H, backup, optimal_Q)\n",
    "    return optimal_Q[:-1]\n",
    "\n",
    "compute_optimal_Q = lambda mdp: _compute_optimal_Q(mdp, mdp.H, mdp.S, mdp.A)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_Q(mdp: CMDP, policy: jnp.ndarray):\n",
    "    \"\"\"ベルマン期待作用素をホライゾン回走らせて価値関数を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (CMDP)\n",
    "        policy (np.ndarray): (HxSxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        policy_Q_rew (jnp.ndarray): (HxSxA)の行列. 報酬関数についてのQ\n",
    "        policy_Q_cost (jnp.ndarray): (HxSxA)の行列. コスト関数についてのQ\n",
    "    \"\"\"\n",
    "    H, S, A = policy.shape\n",
    "\n",
    "    def backup(i, args):\n",
    "        policy_Q, g = args\n",
    "        h = H - i - 1\n",
    "        max_Q = (policy[h+1] * policy_Q[h+1]).sum(axis=1)\n",
    "        next_v = mdp.P[h] @ max_Q\n",
    "        chex.assert_shape(next_v, (S, A))\n",
    "        policy_Q = policy_Q.at[h].set(g[h] + next_v)\n",
    "        return policy_Q, g\n",
    "    \n",
    "    policy_Q_rew = jnp.zeros((H+1, S, A))\n",
    "    args = policy_Q_rew, mdp.rew\n",
    "    policy_Q_rew, _ = jax.lax.fori_loop(0, mdp.H, backup, args)\n",
    "\n",
    "    policy_Q_cost = jnp.zeros((H+1, S, A))\n",
    "    args = policy_Q_cost, mdp.cost\n",
    "    policy_Q_cost, _ = jax.lax.fori_loop(0, mdp.H, backup, args)\n",
    "    return policy_Q_rew[:-1], policy_Q_cost[:-1]\n",
    "\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_matrix(policy: jnp.ndarray):\n",
    "    \"\"\"\n",
    "    上で定義した方策行列を計算します。方策についての内積が取りたいときに便利です。\n",
    "    Args:\n",
    "        policy (jnp.ndarray): (HxSxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        policy_matrix (jnp.ndarray): (HxSxSA)の行列\n",
    "    \"\"\"\n",
    "    H, S, A = policy.shape\n",
    "    PI = policy.reshape(H, 1, S, A)\n",
    "    PI = jnp.tile(PI, (1, S, 1, 1))\n",
    "    eyes = jnp.tile(jnp.eye(S).reshape(1, S, S, 1), (H, 1, 1, 1))\n",
    "    PI = (eyes * PI).reshape(H, S, S*A)\n",
    "    return PI\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_visit(mdp: CMDP, policy: jnp.ndarray, init_dist: jnp.ndarray):\n",
    "    \"\"\"MDPと方策について、訪問頻度を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (CMDP)\n",
    "        policy (jnp.ndarray): (HxSxA)の行列\n",
    "        init_dist (jnp.ndarray): (S) 初期状態の分布\n",
    "\n",
    "    Returns:\n",
    "        visit (jnp.ndarray): (HxSxA)のベクトル\n",
    "    \"\"\"\n",
    "    H, S, A = policy.shape\n",
    "    Pi = compute_policy_matrix(policy)\n",
    "    P = mdp.P.reshape(H, S*A, S)\n",
    "\n",
    "    def body_fn(h, visit):\n",
    "        next_visit = visit[h] @ P[h] @ Pi[h+1]\n",
    "        visit = visit.at[h+1].set(next_visit)\n",
    "        return visit\n",
    "    \n",
    "    visit = jnp.zeros((H+1, S*A))\n",
    "    visit = visit.at[0].set((init_dist @ Pi[0]))\n",
    "    visit = jax.lax.fori_loop(0, mdp.H, body_fn, visit)\n",
    "    visit = visit[:-1].reshape(H, S, A)\n",
    "    return visit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最適方策の累積コスト和 1.5063109\n",
      "最適方策の累積報酬和 3.213574\n"
     ]
    }
   ],
   "source": [
    "@partial(jax.jit, static_argnames=(\"H\", \"S\", \"A\"))\n",
    "def _solve_dual_CMDP(mdp: CMDP, H: int, S: int, A: int, num_iter: int=1000, lam_coef: float=0.01):\n",
    "    \"\"\"双対問題を通じてCMDPを解きます．\n",
    "    Args:\n",
    "        mdp (CMDP)\n",
    "\n",
    "    Returns:\n",
    "        optimal_policy (jnp.ndarray): (HxSxA)の行列\n",
    "    \"\"\"\n",
    "\n",
    "    def loop_fn(k, lam):\n",
    "        reg_rew = mdp.rew - mdp.cost * lam\n",
    "        reg_mdp = mdp._replace(rew=reg_rew)\n",
    "        Q = _compute_optimal_Q(reg_mdp, H, S, A)\n",
    "        new_policy = compute_greedy_policy(Q)\n",
    "        new_policy_visit = compute_policy_visit(reg_mdp, new_policy, reg_mdp.init_dist)\n",
    "\n",
    "        new_lam = lam + lam_coef * (reg_mdp.cost.reshape(-1) @ new_policy_visit.reshape(-1) - mdp.const)\n",
    "        return new_lam\n",
    "    \n",
    "    lam = 0.0\n",
    "    lam = jax.lax.fori_loop(0, num_iter, loop_fn, lam)\n",
    "\n",
    "    reg_rew = mdp.rew - mdp.cost * lam\n",
    "    reg_mdp = mdp._replace(rew=reg_rew)\n",
    "    Q = _compute_optimal_Q(reg_mdp, H, S, A)\n",
    "    policy = compute_greedy_policy(Q)\n",
    "\n",
    "    return policy, lam\n",
    "\n",
    "\n",
    "solve_dual_CMDP = lambda mdp, num_iter, lam_coef: _solve_dual_CMDP(mdp, mdp.H, mdp.S, mdp.A, num_iter, lam_coef)\n",
    "dual_policy, lam = solve_dual_CMDP(mdp, 10000, 0.001)\n",
    "\n",
    "Q_rew, Q_cost = compute_policy_Q(mdp, dual_policy)\n",
    "V_rew, V_cost = (Q_rew * dual_policy).sum(axis=-1), (Q_cost * dual_policy).sum(axis=-1)\n",
    "\n",
    "dual_total_cost = V_cost[0] @ mdp.init_dist\n",
    "print(\"最適方策の累積コスト和\", dual_total_cost)\n",
    "\n",
    "dual_total_rew = V_rew[0] @ mdp.init_dist\n",
    "print(\"最適方策の累積報酬和\", dual_total_rew)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に線形計画法によって双対問題を解きます．\n",
    "変形して，次を解きます\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\min_{V, \\lambda} \\sum_{s} \\mu_0(s) V_0(s) + \\sum^I_{i=1}\\lambda_i \\alpha_i &&\\\\\n",
    "\\text { s.t. } \n",
    "&\n",
    "-r(s, a)\\geq - V_h(s) \n",
    "- \\sum^I_{i=1} \\lambda_i d_i(s, a)\n",
    "+ \\sum_{s'\\in \\mathcal{S}} p_{h}\\left(s' \\mid s, a\\right) V_{h+1}(s^{\\prime}) \n",
    "& & \\forall (s, a) \\in \\mathcal{S}\\times \\mathcal{A} \\\\\n",
    "&v_{H+1}(s)=0 \\quad & & \\forall s\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最適方策の累積報酬和 1.4260185\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "Bup = np.zeros((H+1, S, A, H+1, S))\n",
    "Beq = np.zeros((H+1, S, H+1, S))\n",
    "\n",
    "# h=H+1についての制約\n",
    "for s in range(S):\n",
    "    Beq[H, s, H, s] = 1\n",
    "\n",
    "# 遷移についての制約\n",
    "for h, s, a in product(range(H), range(S), range(A)):\n",
    "    Bup[h, s, a, h, s] = -1  # v(h, s) を実現します\n",
    "    Bup[h, s, a, h+1] = mdp.P[h, s, a]  # sum_s' p(s', s, a)v(s') を実現します\n",
    "\n",
    "Bup = Bup.reshape(((H+1)*S*A, (H+1)*S))\n",
    "Beq = Beq.reshape(((H+1)*S, (H+1)*S))\n",
    "bup = np.hstack((-mdp.rew.reshape(-1), np.zeros(S*A)))\n",
    "beq = np.zeros((H+1)*S)\n",
    "\n",
    "# 制約の一番最後の部分にlambdaの要素を追加します\n",
    "Bup = np.concatenate((Bup, np.zeros(((H+1)*S*A, 1))), axis=-1)\n",
    "Bup = Bup.reshape((H+1), S, A, -1)\n",
    "\n",
    "# 遷移についての制約にλの要素を追加\n",
    "for h, s, a in product(range(H), range(S), range(A)):\n",
    "    Bup[h, s, a, -1] = -mdp.cost[h, s, a]  # -lambda d(s, a) を実現します\n",
    "\n",
    "Bup = Bup.reshape(((H+1)*S*A, -1))\n",
    "Beq = np.concatenate((Beq, np.zeros(((H+1)*S, 1))), axis=-1)\n",
    "\n",
    "w = np.zeros(((H+1), S))\n",
    "w[0, :] = 1 / S\n",
    "w = w.reshape(-1)\n",
    "w = np.hstack((w, mdp.const))  # λの分です\n",
    "lin_res = linprog(w, A_eq=Beq, b_eq=beq, A_ub=Bup, b_ub=bup)\n",
    "\n",
    "V_rew, LP_lam = lin_res.x[:-1], lin_res.x[-1]\n",
    "V_rew = V_rew.reshape(H+1, S)[:-1]\n",
    "\n",
    "reg_rew = mdp.rew - mdp.cost * LP_lam\n",
    "reg_mdp = mdp._replace(rew=reg_rew)\n",
    "Q = _compute_optimal_Q(reg_mdp, H, S, A)\n",
    "LP_dual_policy = compute_greedy_policy(Q)\n",
    "\n",
    "Q_rew, Q_cost = compute_policy_Q(mdp, LP_dual_policy)\n",
    "V_rew, V_cost = (Q_rew * LP_dual_policy).sum(axis=-1), (Q_cost * LP_dual_policy).sum(axis=-1)\n",
    "\n",
    "LP_dual_total_cost = V_cost[0] @ mdp.init_dist\n",
    "print(\"最適方策の累積コスト和\", LP_dual_total_cost)\n",
    "\n",
    "LP_dual_total_rew = V_rew[0] @ mdp.init_dist\n",
    "print(\"最適方策の累積報酬和\", LP_dual_total_rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "線形計画による累積コスト和 1.5\n",
      "双対法による累積コスト和 1.5063109\n",
      "線形計画法＆双対法による累積コスト和 1.5063109\n",
      "線型計画法による累積報酬和 3.2056034\n",
      "双対法による累積報酬和 3.213574\n",
      "線形計画法＆双対法による累積報酬和 3.213574\n"
     ]
    }
   ],
   "source": [
    "print(\"線形計画による累積コスト和\", total_cost)\n",
    "print(\"双対法による累積コスト和\", dual_total_cost)\n",
    "print(\"線形計画法＆双対法による累積コスト和\", LP_dual_total_cost)\n",
    "\n",
    "print(\"線型計画法による累積報酬和\", total_rew)\n",
    "print(\"双対法による累積報酬和\", dual_total_rew)\n",
    "print(\"線形計画法＆双対法による累積報酬和\", LP_dual_total_rew)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "双対法で近い解が得られていますが，ちょっと制約をオーバーしていますね．\n",
    "これが数値的な問題なのか，それとも仕組み的なものなのかは微妙です．\n",
    "実際，線型計画法で求めた方策は確率的方策ですが，双対法で求めたのは決定的方策になっています．\n",
    "（TODO: ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shumi-VTLwuKSy-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
