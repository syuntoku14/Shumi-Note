{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 双対法による制約付きMDPの解法\n",
    "\n",
    "参考\n",
    "\n",
    "* [Exploration-Exploitation in Constrained MDPs](https://arxiv.org/abs/2003.02189)\n",
    "* [CONSTRAINED MARKOV DECISION PROCESSES](https://www-sop.inria.fr/members/Eitan.Altman/TEMP/h.pdf)（未読．読んでおこう．）\n",
    "\n",
    "[前回](RL_CMDP_explore_exploit.ipynb)は線型計画法によってCMDPを解く方法を見ました．\n",
    "ラグランジュの未定乗数法を使うと，動的計画法で解くこともできます．\n",
    "\n",
    "\n",
    "表記\n",
    "\n",
    "* 有限MDP: $\\mathcal{M}=\\left(\\mathcal{S}, \\mathcal{A}, c, p, s_1, H\\right)$\n",
    "    1. 有限状態集合: $S=\\{1, \\dots, |S|\\}$\n",
    "    2. 有限行動集合: $A=\\{1, \\dots, |A|\\}$\n",
    "    3. 非定常なコスト関数（元論文では確率変数ですが、ややこしいので決定的にします）: $c_h(s, a)$\n",
    "    4. 非定常遷移確率: $p_h(s'|s, a)$\n",
    "    5. 初期状態: $s_1$\n",
    "    6. ホライゾン: $H$\n",
    "    7. すべての状態行動の中で最大の次状態への遷移の数: $\\mathcal{N}:=\\max _{s, a, h}\\left|\\left\\{s^{\\prime}: p_h\\left(s^{\\prime} \\mid s, a\\right)>0\\right\\}\\right|$\n",
    "* 占有率：$q_h^\\pi(s, a ; p):=\\mathbb{E}\\left[\\mathbb{1}\\left\\{s_h=s, a_h=a\\right\\} \\mid s_1=s_1, p, \\pi\\right]=\\operatorname{Pr}\\left\\{s_h=s, a_h=a \\mid s_1=s_1, p, \\pi\\right\\}$\n",
    "* 価値関数：$V_1^\\pi\\left(s_1 ; p, c\\right)=\\sum_{h, s, a} q_h^\\pi(s, a ; p) c_h(s, a):=c^T q^\\pi(p)$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\{d_i, \\alpha_i\\}_{i=1}^I$：$I$個の制約\n",
    "    * $d_i \\in \\mathbb{R}^{SAH}$\n",
    "    * $\\alpha_i \\in [0, H]$\n",
    "    * $i$番目の制約（元論文では確率変数ですが、ややこしいので決定的にします）$d_{i, h}(s, a)$\n",
    "    * $V_h^\\pi\\left(s ; p, d_i\\right):=\\mathbb{E}\\left[\\sum_{h^{\\prime}=h}^H d_{i, h^{\\prime}}\\left(s_{h^{\\prime}}, a_{h^{\\prime}}\\right) \\mid s_h=s, p, \\pi\\right]$.\n",
    "\n",
    "CMDPについて振り返ってみましょう．CMDPの目的は次の最適方策の導出です．\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\pi^{\\star} \\in \\underset{\\pi \\in \\Pi^{\\mathrm{MR}}}{\\arg \\min } c^T q^\\pi(p) \\\\\n",
    "\\text { s.t. } D q^\\pi(p) \\leq \\alpha\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "ここで、\n",
    "$$\n",
    "D=\\left[\\begin{array}{c}\n",
    "d_1^T \\\\\n",
    "\\vdots \\\\\n",
    "d_I^T\n",
    "\\end{array}\\right], \\quad \\alpha=\\left[\\begin{array}{c}\n",
    "\\alpha_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\alpha_I\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "としました．\n",
    "これはラグランジュ未定乗数$\\lambda \\in \\mathbb{R}_{+}^I$を使うことで，次の式に直すことができます：\n",
    "\n",
    "$$\n",
    "L^*=\\max _{\\lambda \\in \\mathbb{R}_{+}^I} \\min _{\\pi \\in \\Delta_A^S}\\left\\{c^T q^\\pi(p)+\\lambda^T\\left(D q^\\pi(p)-\\alpha\\right)\\right\\}\n",
    "$$\n",
    "\n",
    "このCMDPには強双対性が成り立つので，このミニマックスゲームは$L^*-V_1^*(s_1)$を満たします．\n",
    "これを解くことを考えます．\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 動的計画法による解法\n",
    "\n",
    "まず次の問題を解きます．\n",
    "\n",
    "$$\n",
    "\\pi_k \\in \\underset{\\pi \\in \\Pi^{\\mathrm{MR}}}{\\arg \\min }\\left({c}+{D}^T \\lambda_k\\right)^{\\top} q^\\pi\\left(p\\right)-\\lambda_k^T \\alpha\n",
    "$$\n",
    "\n",
    "これはMDP $\\mathcal{M}_k = \\left\\{M=(S, A, r^+, p): r_h^{+}(s, a)={c}_h(s, a)+\\sum_i\\left(d_{i, h}(s, a)-\\alpha_i\\right) \\lambda_i^k\\right\\}$の最適方策を求めれば良いので，次の価値反復法で解けます：\n",
    "\n",
    "$$\n",
    "Q_h^k(s, a)=r_h^{+}(s, a)+ \\sum_{s^{\\prime}} p\\left(s^{\\prime} \\mid s, a\\right) \\min _{a^{\\prime}} Q_{h+1}^k\\left(s^{\\prime}, a^{\\prime}\\right)\n",
    "$$\n",
    "\n",
    "続いてラグランジュ未定乗数を更新します．\n",
    "\n",
    "$$\n",
    "\\lambda_{k+1}=\\left[\\lambda_k+\\frac{1}{t_\\lambda}\\left({D} q^{\\pi_k}\\left({p}\\right)-\\alpha\\right)\\right]\n",
    "$$\n",
    "\n",
    "ここで出てくる占有率$q^{\\pi_k}$も簡単な反復法で計算できます．（$t_\\lambda$は少し厄介ですが，なんらかの小さい定数で置いておきます）\n",
    "\n",
    "この２つを繰り返すと，CMDPの最適方策に収束します．確認してみましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状態数： 7\n",
      "行動数： 3\n",
      "ホライゾン： 5\n",
      "制約： 1.5\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from typing import NamedTuple, Optional\n",
    "from jax.random import PRNGKey\n",
    "\n",
    "key = PRNGKey(0)\n",
    "\n",
    "S = 7  # 状態集合のサイズ\n",
    "A = 3  # 行動集合のサイズ\n",
    "S_set = jnp.arange(S)  # 状態集合\n",
    "A_set = jnp.arange(A)  # 行動集合\n",
    "H = 5  # ホライゾン\n",
    "\n",
    "# 報酬行列を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "rew = jax.random.uniform(key=key, shape=(H, S, A))\n",
    "assert rew.shape == (H, S, A)\n",
    "\n",
    "\n",
    "# コスト行列を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "cost = jax.random.uniform(key=key, shape=(H, S, A))\n",
    "assert cost.shape == (H, S, A)\n",
    "\n",
    "\n",
    "# 遷移確率行列を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "P = jax.random.uniform(key=key, shape=(H, S*A, S))\n",
    "P = P / jnp.sum(P, axis=-1, keepdims=True)  # 正規化して確率にします\n",
    "P = P.reshape(H, S, A, S)\n",
    "np.testing.assert_allclose(P.sum(axis=-1), 1, atol=1e-6)  # ちゃんと確率行列になっているか確認します\n",
    "\n",
    "\n",
    "# 初期状態分布を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "init_dist = jax.random.uniform(key, shape=(S,))\n",
    "init_dist = init_dist / jnp.sum(init_dist)\n",
    "np.testing.assert_allclose(init_dist.sum(axis=-1), 1, atol=1e-6)  # ちゃんと確率行列になっているか確認します\n",
    "\n",
    "\n",
    "# 状態集合, 行動集合, 割引率, 報酬行列, 遷移確率行列が準備できたのでMDPのクラスを作ります\n",
    "\n",
    "class CMDP(NamedTuple):\n",
    "    S_set: jnp.array  # 状態集合\n",
    "    A_set: jnp.array  # 行動集合\n",
    "    H: int  # ホライゾン\n",
    "    rew: jnp.array  # 報酬行列\n",
    "    cost: jnp.array  # 報酬行列\n",
    "    const: float  # 制約の閾値\n",
    "    P: jnp.array  # 遷移確率行列\n",
    "    init_dist: jnp.array  # 初期分布\n",
    "    optimal_V_rew: Optional[jnp.ndarray] = None  # 報酬についての最適V値\n",
    "    optimal_V_cost: Optional[jnp.ndarray] = None  # コストについての最適V値\n",
    "\n",
    "    @property\n",
    "    def S(self) -> int:  # 状態空間のサイズ\n",
    "        return len(self.S_set)\n",
    "\n",
    "    @property\n",
    "    def A(self) -> int:  # 行動空間のサイズ\n",
    "        return len(self.A_set)\n",
    "\n",
    "\n",
    "const = 0.3 * H  # 制約は適当です。このときに実行可能である保証はとくにありません。\n",
    "mdp = CMDP(S_set, A_set, H, rew, cost, const, P, init_dist)\n",
    "\n",
    "print(\"状態数：\", mdp.S)\n",
    "print(\"行動数：\", mdp.A)\n",
    "print(\"ホライゾン：\", mdp.H)\n",
    "print(\"制約：\", mdp.const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import chex\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_Q(mdp: CMDP, policy: jnp.ndarray):\n",
    "    \"\"\"ベルマン期待作用素をホライゾン回走らせて価値関数を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (CMDP)\n",
    "        policy (np.ndarray): (HxSxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        policy_Q_rew (jnp.ndarray): (HxSxA)の行列. 報酬関数についてのQ\n",
    "        policy_Q_cost (jnp.ndarray): (HxSxA)の行列. コスト関数についてのQ\n",
    "    \"\"\"\n",
    "    H, S, A = policy.shape\n",
    "\n",
    "    def backup(i, args):\n",
    "        policy_Q, g = args\n",
    "        h = H - i - 1\n",
    "        max_Q = (policy[h+1] * policy_Q[h+1]).sum(axis=1)\n",
    "        next_v = mdp.P[h] @ max_Q\n",
    "        chex.assert_shape(next_v, (S, A))\n",
    "        policy_Q = policy_Q.at[h].set(g[h] + next_v)\n",
    "        return policy_Q, g\n",
    "    \n",
    "    policy_Q_rew = jnp.zeros((H+1, S, A))\n",
    "    args = policy_Q_rew, mdp.rew\n",
    "    policy_Q_rew, _ = jax.lax.fori_loop(0, mdp.H, backup, args)\n",
    "\n",
    "    policy_Q_cost = jnp.zeros((H+1, S, A))\n",
    "    args = policy_Q_cost, mdp.cost\n",
    "    policy_Q_cost, _ = jax.lax.fori_loop(0, mdp.H, backup, args)\n",
    "    return policy_Q_rew[:-1], policy_Q_cost[:-1]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず線型計画法で解いてみます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最適方策の累積コスト和 1.5\n",
      "最適方策の累積報酬和 3.2056034\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import linprog\n",
    "from itertools import product\n",
    "B = np.zeros((H, S, A, H, S, A))\n",
    "\n",
    "# 初期状態についての制約\n",
    "for s, a in product(range(S), range(A)):\n",
    "    B[0, s, a, 0, s] = 1\n",
    "\n",
    "\n",
    "# 遷移についての制約\n",
    "for h, s, a in product(range(1, H), range(S), range(A)):\n",
    "    B[h, s, a, h, s] = 1  # sum_a d(h, s, a) を実現します\n",
    "    B[h, s, a, h-1] = -mdp.P[h-1, :, :, s]  # sum_a d(h, s, a) を実現します\n",
    "\n",
    "\n",
    "B = B.reshape((H*S*A, H*S*A))\n",
    "mu = np.repeat(mdp.init_dist[:, None], A, axis=1).reshape(-1)\n",
    "b = np.hstack((mu, np.zeros((H-1)*S*A)))\n",
    "\n",
    "# コストについての制約\n",
    "C = mdp.cost.reshape(1, -1)\n",
    "c = np.array((mdp.const,))\n",
    "\n",
    "r = - mdp.rew.reshape(-1)\n",
    "lin_res = linprog(r, A_eq=B, b_eq=b, bounds=(0, None), A_ub=C, b_ub=c)\n",
    "\n",
    "d_arr = lin_res.x.reshape(H, S, A)\n",
    "np.testing.assert_allclose(d_arr.sum(axis=(1, 2)), 1.0, atol=1e-4)\n",
    "\n",
    "# 行動の確率が全て０の箇所はUniformにします（この状態には訪れないことを意味しますが、NanがでちゃうのでUniformで回避します）\n",
    "optimal_policy = d_arr / d_arr.sum(axis=-1, keepdims=True)\n",
    "optimal_policy = jnp.where(jnp.isnan(optimal_policy), 1 / mdp.A, optimal_policy)\n",
    "Q_rew, Q_cost = compute_policy_Q(mdp, optimal_policy)\n",
    "V_rew, V_cost = (Q_rew * optimal_policy).sum(axis=-1), (Q_cost * optimal_policy).sum(axis=-1)\n",
    "\n",
    "total_cost = V_cost[0] @ mdp.init_dist\n",
    "assert total_cost <= mdp.const\n",
    "print(\"最適方策の累積コスト和\", total_cost)\n",
    "\n",
    "total_rew = V_rew[0] @ mdp.init_dist\n",
    "print(\"最適方策の累積報酬和\", total_rew)\n",
    "\n",
    "mdp = mdp._replace(optimal_V_rew=V_rew, optimal_V_cost=V_cost)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続いて双対問題を解きます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import jax\n",
    "import chex\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_greedy_policy(Q: jnp.ndarray):\n",
    "    \"\"\"Q関数の貪欲方策を返します\n",
    "\n",
    "    Args:\n",
    "        Q (jnp.ndarray): (HxSxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        greedy_policy (jnp.ndarray): (HxSxA)の行列\n",
    "    \"\"\"\n",
    "    greedy_policy = jnp.zeros_like(Q)\n",
    "    H, S, A = Q.shape\n",
    "    \n",
    "    def body_fn(i, greedy_policy):\n",
    "        greedy_policy = greedy_policy.at[i, jnp.arange(S), Q[i].argmax(axis=-1)].set(1)\n",
    "        return greedy_policy\n",
    "\n",
    "    greedy_policy = jax.lax.fori_loop(0, H, body_fn, greedy_policy)\n",
    "    chex.assert_shape(greedy_policy, (H, S, A))\n",
    "    return greedy_policy\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"H\", \"S\", \"A\"))\n",
    "def _compute_optimal_Q(mdp: CMDP, H: int, S: int, A: int):\n",
    "    \"\"\"ベルマン最適作用素をホライゾン回走らせて最適価値関数を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (CMDP)\n",
    "\n",
    "    Returns:\n",
    "        optimal_Q (jnp.ndarray): (HxSxA)の行列\n",
    "    \"\"\"\n",
    "\n",
    "    def backup(i, optimal_Q):\n",
    "        h = H - i - 1\n",
    "        max_Q = optimal_Q[h+1].max(axis=1)\n",
    "        next_v = mdp.P[h] @ max_Q\n",
    "        chex.assert_shape(next_v, (S, A))\n",
    "        optimal_Q = optimal_Q.at[h].set(mdp.rew[h] + next_v)\n",
    "        return optimal_Q\n",
    "    \n",
    "    optimal_Q = jnp.zeros((H+1, S, A))\n",
    "    optimal_Q = jax.lax.fori_loop(0, mdp.H, backup, optimal_Q)\n",
    "    return optimal_Q[:-1]\n",
    "\n",
    "compute_optimal_Q = lambda mdp: _compute_optimal_Q(mdp, mdp.H, mdp.S, mdp.A)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_Q(mdp: CMDP, policy: jnp.ndarray):\n",
    "    \"\"\"ベルマン期待作用素をホライゾン回走らせて価値関数を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (CMDP)\n",
    "        policy (np.ndarray): (HxSxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        policy_Q_rew (jnp.ndarray): (HxSxA)の行列. 報酬関数についてのQ\n",
    "        policy_Q_cost (jnp.ndarray): (HxSxA)の行列. コスト関数についてのQ\n",
    "    \"\"\"\n",
    "    H, S, A = policy.shape\n",
    "\n",
    "    def backup(i, args):\n",
    "        policy_Q, g = args\n",
    "        h = H - i - 1\n",
    "        max_Q = (policy[h+1] * policy_Q[h+1]).sum(axis=1)\n",
    "        next_v = mdp.P[h] @ max_Q\n",
    "        chex.assert_shape(next_v, (S, A))\n",
    "        policy_Q = policy_Q.at[h].set(g[h] + next_v)\n",
    "        return policy_Q, g\n",
    "    \n",
    "    policy_Q_rew = jnp.zeros((H+1, S, A))\n",
    "    args = policy_Q_rew, mdp.rew\n",
    "    policy_Q_rew, _ = jax.lax.fori_loop(0, mdp.H, backup, args)\n",
    "\n",
    "    policy_Q_cost = jnp.zeros((H+1, S, A))\n",
    "    args = policy_Q_cost, mdp.cost\n",
    "    policy_Q_cost, _ = jax.lax.fori_loop(0, mdp.H, backup, args)\n",
    "    return policy_Q_rew[:-1], policy_Q_cost[:-1]\n",
    "\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_matrix(policy: jnp.ndarray):\n",
    "    \"\"\"\n",
    "    上で定義した方策行列を計算します。方策についての内積が取りたいときに便利です。\n",
    "    Args:\n",
    "        policy (jnp.ndarray): (HxSxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        policy_matrix (jnp.ndarray): (HxSxSA)の行列\n",
    "    \"\"\"\n",
    "    H, S, A = policy.shape\n",
    "    PI = policy.reshape(H, 1, S, A)\n",
    "    PI = jnp.tile(PI, (1, S, 1, 1))\n",
    "    eyes = jnp.tile(jnp.eye(S).reshape(1, S, S, 1), (H, 1, 1, 1))\n",
    "    PI = (eyes * PI).reshape(H, S, S*A)\n",
    "    return PI\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_visit(mdp: CMDP, policy: jnp.ndarray, init_dist: jnp.ndarray):\n",
    "    \"\"\"MDPと方策について、訪問頻度を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (CMDP)\n",
    "        policy (jnp.ndarray): (HxSxA)の行列\n",
    "        init_dist (jnp.ndarray): (S) 初期状態の分布\n",
    "\n",
    "    Returns:\n",
    "        visit (jnp.ndarray): (HxSxA)のベクトル\n",
    "    \"\"\"\n",
    "    H, S, A = policy.shape\n",
    "    Pi = compute_policy_matrix(policy)\n",
    "    P = mdp.P.reshape(H, S*A, S)\n",
    "\n",
    "    def body_fn(h, visit):\n",
    "        next_visit = visit[h] @ P[h] @ Pi[h+1]\n",
    "        visit = visit.at[h+1].set(next_visit)\n",
    "        return visit\n",
    "    \n",
    "    visit = jnp.zeros((H+1, S*A))\n",
    "    visit = visit.at[0].set((init_dist @ Pi[0]))\n",
    "    visit = jax.lax.fori_loop(0, mdp.H, body_fn, visit)\n",
    "    visit = visit[:-1].reshape(H, S, A)\n",
    "    return visit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最適方策の累積コスト和 1.5063109\n",
      "最適方策の累積報酬和 3.213574\n"
     ]
    }
   ],
   "source": [
    "@partial(jax.jit, static_argnames=(\"H\", \"S\", \"A\"))\n",
    "def _solve_dual_CMDP(mdp: CMDP, H: int, S: int, A: int, num_iter: int=1000, lam_coef: float=0.01):\n",
    "    \"\"\"双対問題を通じてCMDPを解きます．\n",
    "    Args:\n",
    "        mdp (CMDP)\n",
    "\n",
    "    Returns:\n",
    "        optimal_policy (jnp.ndarray): (HxSxA)の行列\n",
    "    \"\"\"\n",
    "\n",
    "    def loop_fn(k, lam):\n",
    "        reg_rew = mdp.rew - (mdp.cost - mdp.const) * lam\n",
    "        reg_mdp = mdp._replace(rew=reg_rew)\n",
    "        Q = _compute_optimal_Q(reg_mdp, H, S, A)\n",
    "        new_policy = compute_greedy_policy(Q)\n",
    "        new_policy_visit = compute_policy_visit(reg_mdp, new_policy, reg_mdp.init_dist)\n",
    "\n",
    "        new_lam = lam + lam_coef * (reg_mdp.cost.reshape(-1) @ new_policy_visit.reshape(-1) - mdp.const)\n",
    "        return new_lam\n",
    "    \n",
    "    lam = 1.0\n",
    "    lam = jax.lax.fori_loop(0, num_iter, loop_fn, lam)\n",
    "\n",
    "    reg_rew = mdp.rew - (mdp.cost - mdp.const) * lam\n",
    "    reg_mdp = mdp._replace(rew=reg_rew)\n",
    "    Q = _compute_optimal_Q(reg_mdp, H, S, A)\n",
    "    policy = compute_greedy_policy(Q)\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "solve_dual_CMDP = lambda mdp, num_iter, lam_coef: _solve_dual_CMDP(mdp, mdp.H, mdp.S, mdp.A, num_iter, lam_coef)\n",
    "dual_policy = solve_dual_CMDP(mdp, 10000, 0.001)\n",
    "\n",
    "Q_rew, Q_cost = compute_policy_Q(mdp, dual_policy)\n",
    "V_rew, V_cost = (Q_rew * dual_policy).sum(axis=-1), (Q_cost * dual_policy).sum(axis=-1)\n",
    "\n",
    "dual_total_cost = V_cost[0] @ mdp.init_dist\n",
    "print(\"最適方策の累積コスト和\", dual_total_cost)\n",
    "\n",
    "dual_total_rew = V_rew[0] @ mdp.init_dist\n",
    "print(\"最適方策の累積報酬和\", dual_total_rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "線形計画による累積コスト和 1.5\n",
      "双対法による累積コスト和 1.5063109\n",
      "線型計画法による累積報酬和 3.2056034\n",
      "双対法による累積報酬和 3.213574\n"
     ]
    }
   ],
   "source": [
    "print(\"線形計画による累積コスト和\", total_cost)\n",
    "print(\"双対法による累積コスト和\", dual_total_cost)\n",
    "\n",
    "print(\"線型計画法による累積報酬和\", total_rew)\n",
    "print(\"双対法による累積報酬和\", dual_total_rew)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "双対法で近い解が得られていますが，ちょっと制約をオーバーしていますね．\n",
    "これが数値的な問題なのか，それとも仕組み的なものなのかは微妙です．\n",
    "実際，線型計画法で求めた方策は確率的方策ですが，双対法で求めたのは決定的方策になっています．\n",
    "（TODO: ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shumi-VTLwuKSy-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
