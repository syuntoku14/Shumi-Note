{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 平均報酬MDPの便利な定理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ErgodicなMDP\n",
    "\n",
    "参考：\n",
    "* [Model-free Reinforcement Learning in Infinite-horizon Average-reward Markov Decision Processes](https://arxiv.org/abs/1910.07072)\n",
    "\n",
    "任意の方策について，それに誘導されるマルコフ連鎖がirreducible（transientが存在しない）かつaperiodic（periodが1）ならば，そのMDPはErgodicといいます．\n",
    "\n",
    "表記：\n",
    "* 方策の定常分布：$\\left(\\mu^\\pi\\right)^{\\top} P^\\pi=\\left(\\mu^\\pi\\right)^{\\top}$．つまり，定常分布から一回遷移しても，また定常分布に戻ります．\n",
    "  * これは[RL_AverageReward.ipynb](RL_AverageReward.ipynb)でちょっと言及してます．特にpositive recurrent irreducibleならユニークな解を持ちます．簡単に言えば，**Ergodicなマルコフ連鎖はユニークな解を持ちます．**\n",
    "* ErgodicなMDPでは期待収益について$J^\\pi=\\left(\\mu^\\pi\\right)^{\\top} r^\\pi$が成り立ち，これは状態に依存しません．\n",
    "* ErgodicなMDPでは，次のベルマン方程式の解$q^\\pi$が存在し，それは定数項についてユニークです（$v^\\pi(s)=\\sum_a \\pi(a \\mid s) q^\\pi(s, a)$）：\n",
    "$$\n",
    "J^\\pi+q^\\pi(s, a)=r(s, a)+\\mathbb{E}_{s^{\\prime} \\sim p(\\cdot \\mid s, a)}\\left[v^\\pi\\left(s^{\\prime}\\right)\\right]\n",
    "$$\n",
    "特に$\\sum_s \\mu^\\pi(s) v^\\pi(s)=0$の制約を課せば，$q^\\pi$は唯一の解を持ちます．このとき，\n",
    "$$\n",
    "v^\\pi(s)=\\sum_{t=0}^{\\infty}\\left(\\mathbf{e}_s^{\\top}\\left(P^\\pi\\right)^t-\\left(\\mu^\\pi\\right)^{\\top}\\right) r^\\pi\n",
    "$$\n",
    "を満たします．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**mixing timeのバウンド**\n",
    "\n",
    "$$\n",
    "t_{\\operatorname{mix}}(\\epsilon):=\\max _\\pi \\min \\left\\{t \\geq 1 \\mid\\left\\|\\left(P^\\pi\\right)^t(s, \\cdot)-\\mu^\\pi\\right\\|_1 \\leq \\epsilon, \\forall s\\right\\}\n",
    "$$\n",
    "のことをMixing timeと呼びます．\n",
    "\n",
    "$$\n",
    "t_{\\operatorname{mix}}(1/4) = t_{\\operatorname{mix}}\n",
    "$$\n",
    "としましょう．\n",
    "このとき，\n",
    "$$\n",
    "t_{m i x}(\\epsilon) \\leq\\left\\lceil\\log _2 \\frac{1}{\\epsilon}\\right\\rceil t_{m i x}\n",
    "$$\n",
    "が任意の$\\epsilon \\in\\left(0, \\frac{1}{2}\\right]$で成立します．\n",
    "\n",
    "**証明**\n",
    "\n",
    "* [MDP and Mixing Time](https://pages.uoregon.edu/dlevin/MARKOV/markovmixing.pdf)の4.5章参照\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "任意の$t \\geq 2 t_{mix}$について，次の補題が成立します：\n",
    "\n",
    "$$\n",
    "\\left\\|\\left(P^\\pi\\right)^t(s, \\cdot)-\\mu^\\pi\\right\\|_1 \\leq 2 \\cdot 2^{-\\frac{t}{t_{\\text {mix }}}}, \\quad \\forall \\pi, s\n",
    "$$\n",
    "\n",
    "**証明**\n",
    "\n",
    "上の補題から，任意の$\\epsilon \\in (0, 0.5]$から，$t \\geq\\left\\lceil\\log _2(1 / \\epsilon)\\right\\rceil t_{\\operatorname{mix}}$ならば，\n",
    "$t_{m i x}(\\epsilon) \\leq t$なので，\n",
    "$$\n",
    "\\left\\|\\left(P^\\pi\\right)^t(s, \\cdot)-\\mu^\\pi\\right\\|_1 \\leq \\epsilon\n",
    "$$\n",
    "\n",
    "が成り立ちます．\n",
    "後は$\\log _2(1 / \\epsilon)=\\frac{t}{t_{\\text {mix }}}-1$を選択すれば，$\\epsilon=2 \\cdot 2^{-\\frac{t}{t_{\\text {mix }}}}$であり，補題が成り立ちます．\n",
    "\n",
    "---\n",
    "\n",
    "$N = 4 t_{mix} \\log_2 T$とします．このとき，mixing timeが$t_{mix} < T / 4$であるergodicなMDPについて，任意の方策$\\pi$に対して，\n",
    "\n",
    "$$\n",
    "\\sum_{t=N}^{\\infty}\\left\\|\\left(P^\\pi\\right)^t(s, \\cdot)-\\mu^\\pi\\right\\|_1 \\leq \\frac{1}{T^3}\n",
    "$$\n",
    "\n",
    "が成立します．\n",
    "つまり，mixing timeを使うと，定常分布を有限ステップで近似できます（多分）．\n",
    "\n",
    "**証明**\n",
    "\n",
    "上の補題から，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\sum_{t=N}^{\\infty}\\left\\|\\left(P^\\pi\\right)^t(s, \\cdot)-\\mu^\\pi\\right\\|_1 \\leq \\sum_{t=N}^{\\infty} 2 \\cdot 2^{-\\frac{t}{t_{\\text {mix }}}}\\\\\n",
    "=&\\frac{2 \\cdot 2^{-\\frac{N}{t_{\\text {mix }}}}}{1-2^{-\\frac{1}{t_{\\text {mix }}}}} \\leq \\frac{2 t_{\\text {mix }}}{\\ln 2} \\cdot 2 \\cdot 2^{-\\frac{N}{t_{\\text {mix }}}}=\\frac{2 t_{\\text {mix }}}{\\ln 2} \\cdot 2 \\cdot \\frac{1}{T^4} \\leq \\frac{1}{T^3}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**バイアス関数のバウンド**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left|v^\\pi(s)\\right| & \\leq 5 t_{\\text {mix }} \\\\\n",
    "\\left|q^\\pi(s, a)\\right| & \\leq 6 t_{\\text {mix }}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**証明**\n",
    "\n",
    "バイアス関数の定義から，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v^\\pi(s)\n",
    "&=\\lim_{T\\to \\infty} \\mathbb{E}^\\pi\\left[\\sum^{T-1}_{t=0}r(s_t, a_t) - J^\\pi\\mid s_0=s, \\pi\\right]\\\\\n",
    "&=\n",
    "\\sum_{t=0}^{\\infty}\\left(\\mathbf{e}_s^{\\top}\\left(P^\\pi\\right)^t-\\left(\\mu^\\pi\\right)^{\\top}\\right) r^\\pi\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "が成り立ちます．よって，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left|v^\\pi(s)\\right| & =\\left|\\sum_{t=0}^{\\infty}\\left(\\left(P^\\pi\\right)^t(s, \\cdot)-\\mu^\\pi\\right)^{\\top} r^\\pi\\right| \\\\\n",
    "& \\leq \\sum_{t=0}^{\\infty}\\left\\|\\left(P^\\pi\\right)^t(s, \\cdot)-\\mu^\\pi\\right\\|_1\\left\\|r^\\pi\\right\\|_{\\infty} \\\\\n",
    "& \\leq \\sum_{t=0}^{2 t_{\\text {mix }}-1}\\left\\|\\left(P^\\pi\\right)^t(s, \\cdot)-\\mu^\\pi\\right\\|_1+\\sum_{i=2}^{\\infty} \\sum_{t=i t_{\\text {mix }}}^{(i+1) t_{\\text {mix }}-1}\\left\\|\\left(P^\\pi\\right)^t(s, \\cdot)-\\mu^\\pi\\right\\|_1 \\\\\n",
    "& \\leq 4 t_{\\text {mix }}+\\sum_{i=2}^{\\infty} 2 \\cdot 2^{-i} t_{\\text {mix }} \\quad \\quad\\left(\\text { by }\\left\\|\\left(P^\\pi\\right)^t(s, \\cdot)-\\mu^\\pi\\right\\|_1 \\leq 2\\right. \\text { and Corollary 13.1) } \\\\\n",
    "& \\leq 5 t_{\\text {mix }}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "ここで，4行目では$t \\geq 2t_{mix}$で成り立つ式変形を使ってます．\n",
    "\n",
    "よって，\n",
    "$$\n",
    "\\left|q^\\pi(s, a)\\right|=\\left|r(s, a)+\\mathbb{E}_{s^{\\prime} \\sim p(\\cdot \\mid s, a)}\\left[v^\\pi\\left(s^{\\prime}\\right)\\right]\\right| \\leq 1+5 t_{\\text {mix }} \\leq 6 t_{\\text {mix }}\n",
    "$$\n",
    "も成り立ちます．\n",
    "\n",
    "---\n",
    "\n",
    "**Performance difference lemma**\n",
    "\n",
    "$$\n",
    "J^{\\tilde{\\pi}}-J^\\pi=\\sum_s \\sum_a \\mu^{\\tilde{\\pi}}(s)(\\tilde{\\pi}(a \\mid s)-\\pi(a \\mid s)) q^\\pi(s, a)\n",
    "$$\n",
    "\n",
    "**証明**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\sum_s \\sum_a \\mu^{\\tilde{\\pi}}(s) \\tilde{\\pi}(a \\mid s) q^\\pi(s, a) \\\\\n",
    "& =\\sum_s \\sum_a \\mu^{\\tilde{\\pi}}(s) \\tilde{\\pi}(a \\mid s)\\left(r(s, a)-J^\\pi+\\sum_{s^{\\prime}} p\\left(s^{\\prime} \\mid s, a\\right) v^\\pi\\left(s^{\\prime}\\right)\\right) \\\\\n",
    "& =J^{\\tilde{\\pi}}-J^\\pi+\\sum_{s^{\\prime}} \\mu^{\\tilde{\\pi}}\\left(s^{\\prime}\\right) v^\\pi\\left(s^{\\prime}\\right) \\\\\n",
    "& =J^{\\tilde{\\pi}}-J^\\pi+\\sum_s \\mu^{\\tilde{\\pi}}(s) v^\\pi(s) \\\\\n",
    "& =J^{\\tilde{\\pi}}-J^\\pi+\\sum_s \\sum_a \\mu^{\\tilde{\\pi}}(s) \\pi(a \\mid s) q^\\pi(s, a)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weakly Communicating MDP\n",
    "\n",
    "MDPの状態が次の２つに分割できる場合，そのMDPのことをWeakly communicatingと呼びます．\n",
    "1. すべての状態が，任意の定常方策について，transientである．\n",
    "2. 任意の２つの状態を行き来できる定常方策が存在する．\n",
    "\n",
    "この仮定がない場合は平均報酬でLow regretを達成するのは無理らしいです．（[REGAL: A Regularization based Algorithm for Reinforcement Learning in Weakly Communicating MDPs](https://arxiv.org/abs/1205.2661)を読もう）\n",
    "\n",
    "表記：\n",
    "* 平均総報酬：$J^\\pi(s):=\\liminf _{T \\rightarrow \\infty} \\frac{1}{T} \\mathbb{E}\\left[\\sum_{t=1}^T r\\left(s_t, \\pi\\left(s_t\\right)\\right) \\mid s_1=s\\right]$\n",
    "    * 最適ベルマン方程式：$J^*+q^*(s, a)=r(s, a)+\\mathbb{E}_{s^{\\prime} \\sim p(\\cdot \\mid s, a)}\\left[v^*\\left(s^{\\prime}\\right)\\right]$\n",
    "    * Weakly communicatingでは全ての$s$で$J^*(s)=J^*$な$J^*$が存在する．\n",
    "    * ここで，$q^*$は（定数項を除けば）唯一に定まる．\n",
    "* リグレット：$R_T:=\\sum_{t=1}^T\\left(J^*-r\\left(s_t, a_t\\right)\\right)$．最適な平均報酬から，道中で出会った報酬を引いた値．\n",
    "* Span：$\\operatorname{sp}\\left(v^*\\right)=\\max _s v^*(s)-\\min _s v^*(s)$\n",
    "* 割引報酬の表記：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\forall(s, a), & Q_\\gamma^*(s, a) & =r(s, a)+\\gamma \\mathbb{E}_{s^{\\prime} \\sim p(\\cdot \\mid s, a)}\\left[V_\\gamma^*\\left(s^{\\prime}\\right)\\right] \\\\\n",
    "\\forall s, & V_\\gamma^*(s) & =\\max _{a \\in \\mathcal{A}} Q_\\gamma^*(s, a) .\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**最適な平均報酬と割引報酬の差**\n",
    "\n",
    "1. $\\left|J^*-(1-\\gamma) V_\\gamma^*(s)\\right| \\leq(1-\\gamma) \\operatorname{sp}\\left(v^*\\right), \\forall s \\in \\mathcal{S}$,\n",
    "2. $\\operatorname{sp}\\left(V_\\gamma^*\\right) \\leq 2 \\operatorname{sp}\\left(v^*\\right)$.\n",
    "\n",
    "つまり，$\\gamma$が１に近いときは割引なしの価値と（$(1-\\gamma)$でスケールした）ありの価値の差がほぼありません．\n",
    "\n",
    "**証明**\n",
    "\n",
    "平均報酬のベルマン方程式より，\n",
    "\n",
    "$$\n",
    "v^*(s)=r\\left(s, \\pi^*(s)\\right)-J^*+\\mathbb{E}_{s^{\\prime} \\sim p\\left(\\cdot \\mid s, \\pi^*(s)\\right)} v^*\\left(s^{\\prime}\\right)\n",
    "$$\n",
    "\n",
    "が成立します．ここで，$\\pi^*$の割引設定におけるsub-optimalityを考えると，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V_\\gamma^*\\left(s_1\\right) & = \\mathbb{E}\\left[\\sum_{t=1}^{\\infty} \\gamma^{t-1} r\\left(s_t, \\pi^*\\left(s_t\\right)\\right) \\mid s_1\\right] \\\\\n",
    "& =\\mathbb{E}\\left[\\sum_{t=1}^{\\infty} \\gamma^{t-1}\\left(J^*+v^*\\left(s_t\\right)-v^*\\left(s_{t+1}\\right)\\right) \\mid s_1\\right] \\\\\n",
    "& =\\frac{J^*}{1-\\gamma}+v^*\\left(s_1\\right)-\\mathbb{E}\\left[\\sum_{t=2}^{\\infty}\\left(\\gamma^{t-2}-\\gamma^{t-1}\\right) v^*\\left(s_t\\right) \\mid s_1\\right] \\\\\n",
    "& \\geq \\frac{J^*}{1-\\gamma}+\\min _s v^*(s)-\\max _s v^*(s) \\underbrace{\\sum_{t=2}^{\\infty}\\left(\\gamma^{t-2}-\\gamma^{t-1}\\right)}_{1 - \\gamma + \\gamma - ...} \\\\\n",
    "& =\\frac{J^*}{1-\\gamma}-\\operatorname{sp}\\left(v^*\\right),\n",
    "\\end{aligned}\n",
    "$$\n",
    "２行目は報酬をベルマン方程式で変形しました．\n",
    "\n",
    "同様にして，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V_\\gamma^*\\left(s_1\\right) & =\\mathbb{E}\\left[\\sum_{t=1}^{\\infty} \\gamma^{t-1} r\\left(s_t, \\pi_\\gamma\\left(s_t\\right)\\right) \\mid s_1\\right] \\\\\n",
    "& = \\mathbb{E}\\left[\\sum_{t=1}^{\\infty} \\gamma^{t-1}\\left(J^*+v^*\\left(s_t\\right)-v^*\\left(s_{t+1}\\right)\\right) \\mid s_1\\right] \\\\\n",
    "& =\\frac{J^*}{1-\\gamma}+v^*\\left(s_1\\right)-\\mathbb{E}\\left[\\sum_{t=2}^{\\infty}\\left(\\gamma^{t-2}-\\gamma^{t-1}\\right) v^*\\left(s_t\\right) \\mid s_1\\right] \\\\\n",
    "& \\leq \\frac{J^*}{1-\\gamma}+\\max _s v^*(s)-\\min _s v^*(s) \\underbrace{\\sum_{t=2}^{\\infty}\\left(\\gamma^{t-2}-\\gamma^{t-1}\\right)}_{1 - \\gamma + \\gamma - \\dots} \\\\\n",
    "& =\\frac{J^*}{1-\\gamma}+\\operatorname{sp}\\left(v^*\\right),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "これで１つ目は証明完了です．続いて，\n",
    "\n",
    "$$\n",
    "\\left|V_\\gamma^*\\left(s_1\\right)-V_\\gamma^*\\left(s_2\\right)\\right| \\leq\\left|V_\\gamma^*\\left(s_1\\right)-\\frac{J^*}{1-\\gamma}\\right|+\\left|V_\\gamma^*\\left(s_2\\right)-\\frac{J^*}{1-\\gamma}\\right| \\leq 2 \\operatorname{sp}\\left(v^*\\right)\n",
    "$$\n",
    "\n",
    "より，２つ目が成立します．\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**バイアスのスパンのバウンド**\n",
    "\n",
    "* [REGAL: A Regularization based Algorithm for Reinforcement Learning in Weakly Communicating MDPs](https://arxiv.org/abs/1205.2661)の定理４\n",
    "\n",
    "任意の状態$s_1, s_2$と任意の方策$\\pi$について，\n",
    "\n",
    "$$\n",
    "v^{\\star}\\left(s_2\\right)-v^{\\star}\\left(s_1\\right) \\leq \\lambda^{\\star} T_{s_1 \\rightarrow s_2}^\\pi\n",
    "$$\n",
    "\n",
    "が成り立つ．\n",
    "ここで$T_{s_1 \\rightarrow s_2}^\\pi$は$\\pi$が$s_1$から$s_2$へ到達する期待ステップ数を表します．\n",
    "\n",
    "**証明**\n",
    "\n",
    "まずaperiodicなweakly communicatingなMDPを考えます．\n",
    "このとき，価値反復法が収束することがしられてます．つまり，\n",
    "$v^{n+1}=\\mathcal{T} v^n$を考えると，\n",
    "\n",
    "$$\n",
    "\\lim _{n \\rightarrow \\infty} v^n\\left(s_1\\right)-v^n\\left(s_2\\right)=h^{\\star}\\left(s_1\\right)-h^{\\star}\\left(s_2\\right)\n",
    "$$\n",
    "\n",
    "です．ここで，$v^0=\\boldsymbol{0}$とすると，$v^n=V^n$です．ここで，$V^n(s)$は状態$s$から$n$ステップ経過した際に得られる最大の期待報酬です．\n",
    "よって，\n",
    "$$\n",
    "\\lim _{n \\rightarrow \\infty} V^n\\left(s_2\\right)-V^n\\left(s_1\\right)=h^{\\star}\\left(s_2\\right)-h^{\\star}\\left(s_1\\right)\n",
    "$$\n",
    "です．\n",
    "\n",
    "さて，次の$n$-ステップの非定常方策を考えましょう．\n",
    "* $s_1$から$s_2$に至るまでは$\\pi$を使います．これが$\\tau$ステップ目に起きるとします．\n",
    "* 以降は$n-\\tau$-ステップ最適な方策を使います\n",
    "\n",
    "このとき，$\\tau$は確率変数であり，$\\mathbb{E}[\\tau]=T_{s_1 \\rightarrow s_2}^\\pi$です．\n",
    "そして，得られる期待リターンは$\\mathbb{E}\\left[V^{n-\\tau}\\left(s_2\\right)\\right]$です．これは$n$-ステップ最適リターンよりも必ず小さくなります．つまり，\n",
    "$$\n",
    "V^n\\left(s_1\\right) \\geq \\mathbb{E}\\left[V^{n-\\tau}\\left(s_2\\right)\\right]\n",
    "$$\n",
    "よって，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h^{\\star}\\left(s_2\\right)-h^{\\star}\\left(s_1\\right) & =\\lim _{n \\rightarrow \\infty} V^n\\left(s_2\\right)-V^n\\left(s_1\\right) \\\\\n",
    "& \\leq \\lim _{n \\rightarrow \\infty} V^n\\left(s_2\\right)-\\mathbb{E}\\left[V^{n-\\tau}\\left(s_2\\right)\\right] \\\\\n",
    "& =\\lim _{n \\rightarrow \\infty} \\mathbb{E}\\left[V^n\\left(s_2\\right)-V^{n-\\tau}\\left(s_2\\right)\\right] \\\\\n",
    "& =\\mathbb{E}\\left[\\lim _{n \\rightarrow \\infty} V^n\\left(s_2\\right)-V^{n-\\tau}\\left(s_2\\right)\\right] \\\\\n",
    "& =\\mathbb{E}\\left[\\lambda^{\\star} \\tau\\right] \\\\\n",
    "& =\\lambda^{\\star} T_{s_1 \\rightarrow s_2}^\\pi .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "です．よってaperiodicなときは成立します．続いて，periodicなときは次のaperiodicity transformが使えます：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\tilde{r}(s, a) & =\\theta r(s, a) \\\\\n",
    "\\tilde{P}_{s, a} & =(1-\\theta) \\mathbf{e}_s+\\theta P_{s, a}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "ここで，$\\theta \\in (0, 1)$です．これを$\\tilde{M}$としましょう．このとき，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\tilde{h}^{\\star} & =h^{\\star} \\\\\n",
    "\\tilde{\\lambda}^{\\star} & =\\theta \\lambda^{\\star} \\\\\n",
    "\\tilde{T}_{s_1 \\rightarrow s_2}^\\pi & =\\frac{T_{s_1 \\rightarrow s_2}^\\pi}{\\theta}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "が成り立つことがすぐにわかります（証明）．\n",
    "\n",
    "よって，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h^{\\star}\\left(s_2\\right)-h^{\\star}\\left(s_1\\right) & =\\tilde{h}^{\\star}\\left(s_2\\right)-\\tilde{h}^{\\star}\\left(s_1\\right) \\\\\n",
    "& \\leq \\tilde{\\lambda}^{\\star} \\tilde{T}_{s_1 \\rightarrow s_2}^\\pi \\\\\n",
    "& =\\theta \\lambda^{\\star} \\frac{T_{s_1 \\rightarrow s_2}^\\pi}{\\theta} \\\\\n",
    "& =\\lambda^{\\star} T_{s_1 \\rightarrow s_2}^\\pi\n",
    "\\end{aligned}\n",
    "$$\n",
    "です．\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**エピソード回数のバウンド**\n",
    "\n",
    "* [Near-optimal Regret Bounds for Reinforcement Learning](https://www.jmlr.org/papers/volume11/jaksch10a/jaksch10a.pdf)のAppendix C.2\n",
    "\n",
    "下のスタイルのアルゴリズム（UCRL2など）の解析などで便利です．\n",
    "\n",
    "各エピソード$k=1, 2, \\dots$において，\n",
    "1. $t_k$は現在の時刻です\n",
    "5. どれかの$s, a$が$N_k(s, a)$回訪問されるまで$\\pi^k$を実行します．\n",
    "\n",
    "このとき，$T \\geq $SA$ステップ目までのエピソードの数$m$は,\n",
    "$$\n",
    "m \\leq S A \\log _2\\left(\\frac{8 T}{S A}\\right)\n",
    "$$\n",
    "でバウンドされる．\n",
    "\n",
    "**証明**\n",
    "\n",
    "（TODO: 論文をちゃんと読もう）\n",
    "\n",
    "$$\n",
    "N(s, a):=\\#\\left\\{\\mathrm{~T}<T+1: s_{\\mathrm{T}}=s, a_{\\mathrm{T}}=a\\right\\}\n",
    "$$\n",
    "を$(s, a)$を$T$ステップ目までに訪問した回数とします．各エピソード$k < m$では，$\\nu_k(s, a)=N_k(s, a)$である状態行動が存在します（もしくは$v_k(s, a)=1, N_k(s, a)=0$）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
