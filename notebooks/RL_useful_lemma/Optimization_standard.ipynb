{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 通常の最適化に関する定理など"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "参考:\n",
    "* [https://arxiv.org/abs/1908.00261](https://arxiv.org/abs/1908.00261)のAppendix E\n",
    "\n",
    "次の最適化問題を考えましょう．ここで$C$は空ではない凸集合です．\n",
    "\n",
    "$$\n",
    "\\min _{x \\in C}\\{f(x)\\}\n",
    "$$\n",
    "\n",
    "また，次を仮定します：\n",
    "* $f: \\mathbb{R}^d \\to (-\\infty, \\infty)$はプロパーである．（多分）少なくとも１つの$x$で$f(x) < + \\infty$かつすべての$x$で$f(x) > -\\infty$ならOKです．例えば$\\mathcal{C}$が確率単体であり，$x \\in \\mathcal{C}$ について$f(x)=\\langle x, Q\\rangle$はプロパーですね（RLでよく出てきます）．\n",
    "* $f$はclosedである．各$\\alpha$に対して，$\\{x \\in \\operatorname{dom} f \\mid f(x) \\leq \\alpha\\}$が閉集合ならOKです．まあ$\\operatorname{dom} f$を確率単体としておけば大丈夫かも？あとは$\\cup$みたいな形の関数では成立してます．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projectionについて\n",
    "\n",
    "**Second prox theorem**\n",
    "\n",
    "参考：\n",
    "* Beck本 Theorem 6.39\n",
    "\n",
    "作用素$u=\\operatorname{prox}_f(x)$について考えましょう．\n",
    "これは$u$が次のMinimizerと同じであることと等価だとします．\n",
    "$$\n",
    "\\min_{v} f(v) + \\frac{1}{2} \\|v - x\\|^2_2\n",
    "$$\n",
    "これは例えば$x^{t+1} = \\operatorname{proj}_{\\mathcal{X}} x^t - \\alpha \\xi^t$によるprojected gradient descentなどでよく出てきます:\n",
    "\n",
    "$$\n",
    "x^{t+1} \\in \\arg\\min_{x \\in \\mathcal{X}} \\langle \\xi^t, x - x^t\\rangle + \\frac{1}{2\\alpha} \\|x - x^t\\|_2^2\n",
    "$$\n",
    "\n",
    "このとき，任意の$x, u \\in \\mathcal{X}$について，次が等価です：\n",
    "1. $u = \\operatorname{Prox}_f(x)$\n",
    "2. $x - u \\in \\partial f(u)$\n",
    "3. $\\langle x - u, y - u \\rangle \\leq f(y) - f(u)$ for any $y \\in \\mathcal{X}$\n",
    "\n",
    "**証明**\n",
    "\n",
    "$u$は$\\min_{v} f(v) + \\frac{1}{2} \\|v - x\\|^2_2$を実現するので，\n",
    "$$\n",
    "0 \\in \\partial f(u) + u - x\n",
    "$$\n",
    "です．最後のやつは劣勾配の定義からすぐになりたちます．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projected gradient descentの収束\n",
    "\n",
    "$\\operatorname{dom}(f)$は凸であり，$f$は$\\operatorname{int}(\\operatorname{dom}(f))$上で$\\beta$-smoothである．\n",
    "\n",
    "また，最適値を$f(x^*)$と表記します．\n",
    "\n",
    "次のGradient mappingを定義します．\n",
    "\n",
    "$$G^\\eta(x) := \\frac{1}{\\eta} (x - P_C(x - \\eta \\nabla f(x)))$$\n",
    "\n",
    "ここで$P_C$は$C$上へのユークリッドノルムを最小化する写像です．\n",
    "また，$C=R^d$であれば$G^\\eta(x)=\\nabla f(x)$であることに注意しましょう．\n",
    "\n",
    "次が成立します：\n",
    "\n",
    "---\n",
    "\n",
    "[Beck本](https://epubs.siam.org/doi/10.1137/1.9781611974997)のTheorem 10.15\n",
    "\n",
    "$\\left\\{x_k\\right\\}_{k \\geq 0}$\n",
    "を学習率$\\eta=1/\\beta$の勾配降下アルゴリズムで生成された系列とします．\n",
    "\n",
    "このとき，\n",
    "1. $\\left\\{F\\left(x_t\\right)\\right\\}_{t \\geq 0}$はnon-increasing\n",
    "2. $G^\\eta(x_t) \\to 0$が$t\\to \\infty$で成立\n",
    "3. $\\min _{t=0,1, \\ldots, T-1}\\left\\|G^\\eta\\left(x_t\\right)\\right\\| \\leq \\frac{\\sqrt{2 \\beta f\\left(x_0\\right)-f\\left(x^*\\right)}}{\\sqrt{T}}$が成立します\n",
    "\n",
    "---\n",
    "\n",
    "[Accelerated Gradient Methods for Nonconvex Nonlinear and Stochastic Programming](https://arxiv.org/abs/1310.3787)のLemma 3\n",
    "\n",
    "$x^+$を$x-\\eta G^\\eta(x)$とします．このとき，\n",
    "$$\n",
    "\\nabla f\\left(x^{+}\\right) \\in N_C\\left(x^{+}\\right)+\\epsilon(\\eta \\beta+1) B_2,\n",
    "$$\n",
    "が成立します．\n",
    "\n",
    "ここで$B_2$はunit $\\ell_2$ ballであり，$N_C$は$C$上のnormal coneです．\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
