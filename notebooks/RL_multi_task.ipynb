{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# マルチタスク強化学習のサンプル効率\n",
    "\n",
    "参考：\n",
    "* [Sample Complexity of Multi-task Reinforcement Learning](https://arxiv.org/abs/1309.6821)：もっと良い論文ありそう．問題設定と解き方は参考になる．\n",
    "\n",
    "強化学習でマルチタスクを扱う場合，しばしば転移学習が用いられますが，なぜそれが良いのか？は自明ではありません．\n",
    "強化学習におけるサンプル効率の理論を見てみましょう．\n",
    "\n",
    "表記：\n",
    "* ホライゾン$H$\n",
    "* タスクの数$T$\n",
    "    * MDPの集合$\\mathcal{M}$．$|\\mathcal{M}|=C$とします．\n",
    "    * それぞれのタスクは$\\mathcal{M}$からサンプルされます\n",
    "    * タスクは状態行動空間は同じですが，報酬や遷移が異なります．[文脈付きMDP](RL_multi_task_contextual_MDP.ipynb)に似てます\n",
    "\n",
    "\n",
    "## マルチタスク強化学習アルゴリズム\n",
    "\n",
    "次の手順でマルチタスクMDPを解きます\n",
    "\n",
    "1. 入力：サンプルするタスクの数$T$と分類するMDPの数$\\bar{C}$\n",
    "2. for $t=1,2, \\ldots, T_1$ do\n",
    "    1. $M_t \\in \\mathcal{M}$をサンプル\n",
    "    2. $M_t$において，$H$ステップだけ$E^3$アルゴリズムを走らせ，$o\\left(s, a, s^{\\prime}, t\\right)$のカウントを得ます．$E^3$アルゴリズムについては[Near-Optimal Reinforcement Learning in Polynomial Time](https://www.cis.upenn.edu/~mkearns/papers/KearnsSinghE3.pdf)参照．\n",
    "3. 集めたカウントを利用して推定したそれぞれの遷移$\\hat{P}_t$について，$\\|\\hat{P}_{t_1} - \\hat{P}_{t_2}\\|_\\infty \\leq \\varepsilon$であるような$t_1$と$t_2$は同じ分類とします．\n",
    "4. for $t=T_1 + 1, \\ldots, T$ do\n",
    "    1. 未知の$M_t \\in \\mathcal{M}$を受け取る\n",
    "    2. この$M_t$が既知であるなら，...（この後がよくわからない．読みづらい...）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
