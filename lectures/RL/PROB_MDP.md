**伝えたいこと**

マルコフ決定過程は意思決定問題を定式化するフレームワーク．
様々な派生があり，効率的なアルゴリズムが存在する．

# マルコフ決定過程

**モチベーション**

前回の講義でお話ししたように、現実のサービスで最適な意思決定をするためには、現実の複雑な問題を数学的に扱いやすい形に「定式化」することが重要です。

そこで登場するのが，今回の「マルコフ決定過程」です。

マルコフ決定過程は、複雑な意思決定問題を、数学的にシンプルで扱いやすい形に抽象化したフレームワークです。
マルコフ決定過程には、効率よく最適な意思決定を求めるアルゴリズムがいくつか存在します。

では、具体的に「マルコフ決定過程」とは何なのか、説明していきましょう。

**定義**

マルコフ決定過程（MDP）は、次の5つの要素で構成されます。

* 状態（State）: 意思決定を行うときの「状況」です。
* 行動（Action）: その状況で「何をするか」という選択です。
* 遷移確率（Transition Probability）: ある状態で行動を選ぶと、次にどの状態に移るかの「確率」です。
* 報酬（Reward）: 各行動を選んだ結果として「どれくらい良いことがあったか」を示す値です。
* 割引率（Discount Factor）: 未来の報酬をどれくらい「今の価値」として考えるかを決めるパラメータです。

**例**

例えば、自動運転車を考えてみましょう。

状態は「今どの道路にいるか」や「周りにどんな車がいるか」など。
行動は「右に曲がる」「直進する」「停止する」など。
遷移確率は「直進したら、次の瞬間にはこの位置にいる確率は◯%」という具合です。
報酬は「目的地に近づいたら+10点、事故を起こしたら-100点」みたいなものです。
割引率は「10秒後の報酬は今より少し価値を下げて考えよう」といった具合です。
MDPのポイントは、「現在の状態が分かっていれば、次に何が起こるかは現在の状態と行動だけで決まる」という「マルコフ性」と呼ばれる性質です。過去の状態は関係ありません。

なぜMDPを使うのか？

MDPを使うと、将来得られる「報酬の合計」を最大化するために、今何をすべきかを計算的に求められます。

具体的には、ある状態で「最適な行動」を決める「最適方策（Optimal Policy）」を求めることが目標です。

MDPをうまく使うと、複雑な問題でも、理論的に保証された最適な意思決定ができるんです。

次回予告

次の講義では、このMDPの中で「最適な方策」を求めるための基本的な考え方やアルゴリズムについて学んでいきます。